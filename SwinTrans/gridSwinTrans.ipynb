{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "import os\n",
    "import json\n",
    "import random \n",
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "from PIL import Image\n",
    "from argparse import Namespace \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence # 压紧填充序列\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import ResNet101_Weights\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from typing import Optional\n",
    "from timm.models.layers import DropPath\n",
    "import torch.nn.functional as F\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "import os\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "from transformers import ViTModel, BertModel, BertConfig\n",
    "from torch.nn import TransformerDecoder, TransformerDecoderLayer\n",
    "import math\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset='deepfashion-multimodal'\n",
    "img_path = '/home/u2021213687/ImageCaptioning/ARCTIC/deepfashion-multimodal/images'\n",
    "train_json_path= '/home/u2021213687/ImageCaptioning/ARCTIC/deepfashion-multimodal/train_captions.json'\n",
    "test_json_path= '/home/u2021213687/ImageCaptioning/ARCTIC/deepfashion-multimodal/test_captions.json'\n",
    "vocab_path = '/home/u2021213687/ImageCaptioning/ARCTIC/deepfashion-multimodal/vocab.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idx_to_word(idx, vocab):\n",
    "    reverse_vocab = {v: k for k, v in vocab.items()}\n",
    "    return reverse_vocab.get(int(idx), '<unk>')\n",
    "\n",
    "def cap_to_wvec(vocab,cap):#将文本描述转换成向量\n",
    "    cap.replace(\",\",\"\")\n",
    "    cap.replace(\".\",\"\")\n",
    "    cap=cap.split()\n",
    "    res=[]\n",
    "    for word in cap:\n",
    "        if word in vocab.keys():\n",
    "            res.append(vocab[word])\n",
    "        else: #不在字典的词\n",
    "            res.append(vocab['<unk>'])\n",
    "    return res\n",
    "\n",
    "def create_dataset(dataset='deepfashion-multimodal',\n",
    "                   captions_per_image=5, \n",
    "                   min_word_count=5, \n",
    "                   max_len=30):\n",
    "\n",
    "    output_folder=f'data/{dataset}'\n",
    "    data_all={}\n",
    "    with open(train_json_path, 'r') as j:\n",
    "        data_train=json.load(j)\n",
    "        data_all.update(data_train)\n",
    "    with open(test_json_path, 'r') as j:\n",
    "        data_test=json.load(j)\n",
    "        data_all.update(data_test)\n",
    "\n",
    "\n",
    "    \n",
    "    image_paths = defaultdict(list) # 图片路径\n",
    "    image_captions = defaultdict(list) # 图片对应的文本描述\n",
    "    vocab = Counter() # 词频统计\n",
    "    print(f\"data_all.keys()_len:{len(data_all.keys())}\")\n",
    "    max_len=0\n",
    "    for img_path in data_all.keys(): #遍历所有图片统计词频\n",
    "        c=data_all[img_path]\n",
    "        c.replace(\",\",\"\")\n",
    "        c.replace(\".\",\"\")\n",
    "        c_ls=c.split() #分句\n",
    "        if len(c_ls)>max_len:\n",
    "            max_len=len(c_ls)\n",
    "        vocab.update(c_ls)\n",
    "    print(vocab)\n",
    "    print(f\"最大的描述长度|max_len:{max_len}\")\n",
    "    # 创建词典，增加占位标识符<pad>、未登录词标识符<unk>、句子首尾标识符<start>和<end>\n",
    "    words = [w for w in vocab.keys() if vocab[w] > min_word_count]\n",
    "    vocab = {k: v + 1 for v, k in enumerate(words)} #从1开始标号\n",
    "    vocab['<pad>'] = 0 #填充\n",
    "    vocab['<unk>'] = len(vocab) #未登录词\n",
    "    vocab['<start>'] = len(vocab) #句子首部\n",
    "    vocab['<end>'] = len(vocab) #句子尾部\n",
    "    print(vocab)\n",
    "    # 存储词典\n",
    "    with open(os.path.join(output_folder, 'vocab.json'), 'w') as fw:\n",
    "        json.dump(vocab, fw)\n",
    "def cap_to_wvec(vocab,cap):#将文本描述转换成向量\n",
    "    cap.replace(\",\",\"\")\n",
    "    cap.replace(\".\",\"\")\n",
    "    cap=cap.split()\n",
    "    res=[]\n",
    "    for word in cap:\n",
    "        if word in vocab.keys():\n",
    "            res.append(vocab[word])\n",
    "        else: #不在字典的词\n",
    "            res.append(vocab['<unk>'])\n",
    "    return res\n",
    "\n",
    "\n",
    "class ImageTextDataset(Dataset):\n",
    "    def __init__(self, dataset_path, vocab_path, split, captions_per_image=1, max_len=93, transform=None):\n",
    "\n",
    "        self.split = split\n",
    "        assert self.split in {'train', 'test'}\n",
    "        self.cpi = captions_per_image\n",
    "        self.max_len = max_len\n",
    "\n",
    "        # 载入数据集\n",
    "        with open(dataset_path, 'r') as f:\n",
    "            self.data = json.load(f) #key是图片名字 value是描述\n",
    "            self.data_img=list(self.data.keys())\n",
    "        # 载入词典\n",
    "        with open(vocab_path, 'r') as f:\n",
    "            self.vocab = json.load(f)\n",
    "\n",
    "        # PyTorch图像预处理流程\n",
    "        self.transform = transform\n",
    "\n",
    "        # Total number of datapoints\n",
    "        self.dataset_size = len(self.data_img)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        # 第i个文本描述对应第(i // captions_per_image)张图片\n",
    "        img = Image.open(img_path+\"/\"+self.data_img[i]).convert('RGB')\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        c_vec=cap_to_wvec(self.vocab,self.data[self.data_img[i]])\n",
    "        #加入起始和结束标志\n",
    "        c_vec = [self.vocab['<start>']] + c_vec + [self.vocab['<end>']]\n",
    "        caplen = len(c_vec)\n",
    "        caption = torch.LongTensor(c_vec+ [self.vocab['<pad>']] * (self.max_len + 2 - caplen))\n",
    "        \n",
    "        return img, caption, caplen\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.dataset_size\n",
    "    \n",
    "def mktrainval(data_dir, vocab_path, batch_size, workers=4):\n",
    "    train_tx = transforms.Compose([\n",
    "        transforms.Resize(256), # 重置图像分辨率\n",
    "        transforms.RandomCrop(224), # 随机裁剪\n",
    "        transforms.ToTensor(), # 转换成Tensor\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # 标准化--三个参数为三个通道的均值和标准差\n",
    "    ])\n",
    "    val_tx = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    train_set = ImageTextDataset(os.path.join(data_dir, 'train_captions.json'), vocab_path, 'train',  transform=train_tx)\n",
    "    test_set = ImageTextDataset(os.path.join(data_dir, 'test_captions.json'), vocab_path, 'test', transform=val_tx)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_set, batch_size=batch_size, shuffle=True, num_workers=workers, pin_memory=True)\n",
    "    \n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        test_set, batch_size=batch_size, shuffle=False, num_workers=workers, pin_memory=True, drop_last=False)\n",
    "\n",
    "    return train_loader, test_loader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def window_reverse(windows, window_size: int, H: int, W:int):\n",
    "    B = int(windows.shape[0] / (H * W / window_size / window_size))\n",
    "    x = windows.view(B, H // window_size, W // window_size, window_size, window_size, -1)\n",
    "    x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, H, W, -1)\n",
    "\n",
    "    return x\n",
    "def window_partition(x, window_size: int):\n",
    "    B, H, W, C = x.shape\n",
    "    x = x.view(B, H // window_size, window_size, W // window_size, window_size, C)\n",
    "    windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size, window_size, C)\n",
    "\n",
    "    return windows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, act=nn.GELU, drop=0.):\n",
    "        super(MLP, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act()\n",
    "        self.fc2 = nn.Linear(hidden_features, in_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "class WindowAttention(nn.Module):\n",
    "    # W-MSA SW-MSA\n",
    "    def __init__(self, dim, window_size, num_heads, qkv_bias=True, attn_drop=0., proj_drop=0.):\n",
    "        super(WindowAttention, self).__init__()\n",
    "\n",
    "        self.dim = dim\n",
    "        self.window_size = window_size\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = head_dim ** -0.5 # 根号d\n",
    "\n",
    "        self.relative_positive_bias_table = nn.Parameter(\n",
    "        \t# (2*Mh-1) * (2*Mw-1), num_heads\n",
    "            torch.zeros((2 * window_size[0] - 1) * (2 * window_size[1] - 1), num_heads) \n",
    "        )\n",
    "\n",
    "        # 生成relative_position_index\n",
    "        coords_h = torch.arange(self.window_size[0])\n",
    "        coords_w = torch.arange(self.window_size[1])\n",
    "        coords = torch.stack(torch.meshgrid([coords_h, coords_w],indexing='ij'))\n",
    "        coords_flatten = torch.flatten(coords, 1)\n",
    "        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]\n",
    "        relative_coords = relative_coords.permute(1, 2, 0).contiguous() \n",
    "        relative_coords[:, :, 0] += self.window_size[0] - 1\n",
    "        relative_coords[:, :, 1] += self.window_size[1] - 1\n",
    "        relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1\n",
    "        relative_position_index = relative_coords.sum(-1) \n",
    "        self.register_buffer('relative_position_index', relative_position_index)\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "        nn.init.trunc_normal_(self.relative_positive_bias_table, std=.02)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x, mask: Optional[torch.Tensor] = None):\n",
    "        B_, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B_, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        # batch_size * num_windows, num_heads, Mh*Mw, embed_dim_per_head\n",
    "        q, k, v = qkv.unbind(0)\n",
    "\n",
    "        q = q * self.scale\n",
    "        attn = (q @ k.transpose(-2, -1))\n",
    "        relative_position_bias = self.relative_positive_bias_table[self.relative_position_index.view(-1)].view(\n",
    "            self.window_size[0] * self.window_size[1], self.window_size[0] * self.window_size[1], -1)\n",
    "        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous() # [num_head, Mh*Mw, Mh*Mw]\n",
    "        # [batch_size * num_windows, num_heads, Mh * Mw, Mh * Mw]\n",
    "        attn = attn + relative_position_bias.unsqueeze(0)\n",
    "\n",
    "        if mask is not None:\n",
    "            num_window = mask.shape[0]\n",
    "            attn = attn.view(B_ // num_window, num_window, self.num_heads, N, N) + mask.unsqueeze(1).unsqueeze(0)\n",
    "            attn = attn.view(-1, self.num_heads, N, N)\n",
    "            attn = self.softmax(attn)\n",
    "        else:\n",
    "            attn = self.softmax(attn)\n",
    "\n",
    "        attn = self.attn_drop(attn)\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B_, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwinTransformerBlock(nn.Module):\n",
    "    def __init__(self, dim, num_heads, window_size=7, shift_size=0., mlp_ratio=4., qkv_bias=True, drop=0., attn_drop=0., drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
    "        super(SwinTransformerBlock, self).__init__()\n",
    "\n",
    "        self.dim = dim\n",
    "        self.num_heads = num_heads\n",
    "        self.window_size = window_size\n",
    "        self.shift_size = shift_size\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = self._create_attention_module(dim, num_heads, window_size, qkv_bias, attn_drop, drop)\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = MLP(in_features=dim, hidden_features=mlp_hidden_dim, act=act_layer, drop=drop)\n",
    "\n",
    "    def _create_attention_module(self, dim, num_heads, window_size, qkv_bias, attn_drop, drop):\n",
    "        return WindowAttention(\n",
    "            dim=dim, window_size=(window_size, window_size), num_heads=num_heads,\n",
    "            qkv_bias=qkv_bias, attn_drop=attn_drop, proj_drop=drop\n",
    "        )\n",
    "\n",
    "    def forward(self, x, attn_mask):\n",
    "        x, Hp, Wp = self._process_input(x)\n",
    "        shifted_x = self._shift_and_pad(x)\n",
    "        x_windows = self._prepare_windows(shifted_x)\n",
    "        attn_windows = self.attn(x_windows, mask=attn_mask)\n",
    "        shifted_x = self._reverse_windows(attn_windows, Hp, Wp)\n",
    "        x = self._restore_data(shifted_x, x.shape, Hp, Wp)\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "\n",
    "        return x\n",
    "\n",
    "    def _process_input(self, x):\n",
    "        H, W = self.H, self.W  # feature map\n",
    "        B, L, C = x.shape\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = x.view(B, H, W, C)\n",
    "        return x, H, W\n",
    "\n",
    "    def _shift_and_pad(self, x):\n",
    "        if self.shift_size > 0.:\n",
    "            shifted_x = torch.roll(x, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2))\n",
    "        else:\n",
    "            shifted_x = x\n",
    "        return shifted_x\n",
    "\n",
    "    def _prepare_windows(self, x):\n",
    "        x_windows = window_partition(x, self.window_size)\n",
    "        x_windows = x_windows.view(-1, self.window_size * self.window_size, x.shape[-1])\n",
    "        return x_windows\n",
    "\n",
    "    def _reverse_windows(self, attn_windows, Hp, Wp):\n",
    "        attn_windows = attn_windows.view(-1, self.window_size, self.window_size, attn_windows.shape[-1])\n",
    "        shifted_x = window_reverse(attn_windows, self.window_size, Hp, Wp)\n",
    "        return shifted_x\n",
    "\n",
    "    def _restore_data(self, shifted_x, original_shape, Hp, Wp):\n",
    "        if self.shift_size > 0:\n",
    "            x = torch.roll(shifted_x, shifts=(self.shift_size, self.shift_size), dims=(1, 2))\n",
    "        else:\n",
    "            x = shifted_x\n",
    "        x_r = (self.window_size - original_shape[2] % self.window_size) % self.window_size\n",
    "        x_d = (self.window_size - original_shape[1] % self.window_size) % self.window_size\n",
    "        if x_r > 0 or x_d > 0:\n",
    "            x = x[:, :original_shape[1], :original_shape[2], :].contiguous()\n",
    "        x = x.view(original_shape[0], original_shape[1] * original_shape[2], original_shape[3])\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicLayer(nn.Module):\n",
    "    def __init__(self, dim, depth, num_heads, window_size, mlp_ratio=4., qkv_bias=True, drop=0., attn_drop=0., drop_path=0., norm_layer=nn.LayerNorm, downsample=None):\n",
    "        super(BasicLayer, self).__init__()\n",
    "\n",
    "        self.dim = dim\n",
    "        self.depth = depth\n",
    "        self.window_size = window_size\n",
    "        self.shift_size = window_size // 2 \n",
    "\n",
    "        self.blocks = self._create_blocks(dim, num_heads, window_size, mlp_ratio, qkv_bias, drop, attn_drop, drop_path, norm_layer, depth)\n",
    "        self.downsample = downsample(dim=dim, norm_layer=norm_layer) if downsample is not None else None\n",
    "\n",
    "    def _create_blocks(self, dim, num_heads, window_size, mlp_ratio, qkv_bias, drop, attn_drop, drop_path, norm_layer, depth):\n",
    "        return nn.ModuleList([\n",
    "            self._create_block(dim, num_heads, window_size, mlp_ratio, qkv_bias, drop, attn_drop, drop_path[i] if isinstance(drop_path, list) else drop_path,\n",
    "                norm_layer, i) for i in range(depth)\n",
    "        ])\n",
    "\n",
    "    def _create_block(self, dim, num_heads, window_size, mlp_ratio, qkv_bias, drop, attn_drop, drop_path, norm_layer, index):\n",
    "        shift_size = 0 if (index % 2 == 0) else self.shift_size\n",
    "        return SwinTransformerBlock(\n",
    "            dim=dim, num_heads=num_heads, window_size=window_size, shift_size=shift_size,\n",
    "            mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, drop=drop, attn_drop=attn_drop, drop_path=drop_path,\n",
    "            norm_layer=norm_layer\n",
    "        )\n",
    "\n",
    "    def create_mask(self, x, H, W):\n",
    "        H_padding = int(np.ceil(H / self.window_size)) * self.window_size\n",
    "        W_padding = int(np.ceil(W / self.window_size)) * self.window_size\n",
    "        img_mask = self._create_image_mask(x, H_padding, W_padding)\n",
    "        mask_windows = self._window_partition(img_mask)\n",
    "        attn_mask = self._compute_attn_mask(mask_windows)\n",
    "        return attn_mask\n",
    "\n",
    "    def _create_image_mask(self, x, H_padding, W_padding):\n",
    "        img_mask = torch.zeros((1, H_padding, W_padding, 1), device=x.device)\n",
    "        h_slices = [slice(0, -self.window_size), slice(-self.window_size, -self.shift_size), slice(-self.shift_size, None)]\n",
    "        w_slices = [slice(0, -self.window_size), slice(-self.window_size, -self.window_size), slice(-self.window_size, None)]\n",
    "\n",
    "        cnt = 0\n",
    "        for h in h_slices:\n",
    "            for w in w_slices:\n",
    "                img_mask[:, h, w, :] = cnt\n",
    "                cnt += 1\n",
    "\n",
    "        return img_mask\n",
    "\n",
    "    def _window_partition(self, img_mask):\n",
    "        mask_windows = window_partition(img_mask, self.window_size)\n",
    "        return mask_windows.view(-1, self.window_size * self.window_size)\n",
    "\n",
    "    def _compute_attn_mask(self, mask_windows):\n",
    "        attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)\n",
    "        attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.)).masked_fill(attn_mask == 0, float(0.0))\n",
    "        return attn_mask\n",
    "\n",
    "    def forward(self, x, H, W):\n",
    "        attn_mask = self.create_mask(x, H, W)\n",
    "        for blk in self.blocks:\n",
    "            blk.H, blk.W = H, W\n",
    "            x = blk(x, attn_mask)\n",
    "        if self.downsample is not None:\n",
    "            x = self.downsample(x, H, W)\n",
    "            H, W = (H + 1) // 2, (W + 1) // 2\n",
    "\n",
    "        return x, H, W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class patchEmbed(nn.Module):\n",
    "    def __init__(self, patch_size=4, in_channels=3, embed_dim=96, norm_layer=None):\n",
    "        super(patchEmbed, self).__init__()\n",
    "\n",
    "        patch_size = (patch_size, patch_size)\n",
    "        self.patch_size = patch_size\n",
    "        self.in_channels = in_channels\n",
    "        self.embed_dim = embed_dim\n",
    "        self.proj = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        self.norm = norm_layer(embed_dim) if norm_layer else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, _, H, W = x.shape\n",
    "        pad_input = (H % self.patch_size[0] != 0) or (W % self.patch_size[1] != 0)\n",
    "\n",
    "        if pad_input:\n",
    "            x = F.pad(x, (0, self.patch_size[1] - W % self.patch_size[1],\n",
    "                          0, self.patch_size[0] - H % self.patch_size[0],\n",
    "                          0, 0))\n",
    "        x = self.proj(x)\n",
    "        _, _, H, W = x.shape\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "        x = self.norm(x)\n",
    "        return x, H, W\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class patchmerging(nn.Module):\n",
    "    # down-sample\n",
    "    def __init__(self, dim, norm_layer=nn.LayerNorm):\n",
    "        super(patchmerging, self).__init__()\n",
    "\n",
    "        self.dim = dim\n",
    "        self.reduction = nn.Linear(4 * dim, 2 * dim, bias=False)\n",
    "        self.norm = norm_layer(4 * dim)\n",
    "    def forward(self, x, H, W):\n",
    "        \"\"\" Forward function.\n",
    "\n",
    "        Args:\n",
    "            x: Input feature, tensor size (B, H*W, C).\n",
    "            H, W: Spatial resolution of the input feature.\n",
    "        \"\"\"\n",
    "        B, L, C = x.shape\n",
    "        assert L == H * W, \"input feature has wrong size\"\n",
    "\n",
    "        x = x.view(B, H, W, C)\n",
    "\n",
    "        # padding\n",
    "        pad_input = (H % 2 == 1) or (W % 2 == 1)\n",
    "        if pad_input:\n",
    "            x = F.pad(x, (0, 0, 0, W % 2, 0, H % 2))\n",
    "\n",
    "        x0 = x[:, 0::2, 0::2, :]  # B H/2 W/2 C 左上\n",
    "        x1 = x[:, 1::2, 0::2, :]  # B H/2 W/2 C 左下 \n",
    "        x2 = x[:, 0::2, 1::2, :]  # B H/2 W/2 C 右上\n",
    "        x3 = x[:, 1::2, 1::2, :]  # B H/2 W/2 C 右下\n",
    "        x = torch.cat([x0, x1, x2, x3], -1)  # B H/2 W/2 4*C\n",
    "        x = x.view(B, -1, 4 * C)  # B H/2*W/2 4*C\n",
    "\n",
    "        x = self.norm(x)\n",
    "        x = self.reduction(x)  # B H/2*W/2 2*C \n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwinTransformerFeatureExtractor(nn.Module):\n",
    "    def __init__(self, downsapmle_size=4, in_channels=3, embed_dim=96, depths=(2, 2, 6, 2), num_heads=(3, 6, 12, 24), window_size=7, mlp_ratio=4.,\n",
    "                 qkv_bias=True, drop_rate=0., attn_drop_rate=0., drop_path_rate=0.1, norm_layer=nn.LayerNorm, patch_norm=True, **kwargs):\n",
    "        super(SwinTransformerFeatureExtractor, self).__init__()\n",
    "\n",
    "        self.num_layers = len(depths)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.patch_norm = patch_norm\n",
    "        # stage4 输出的特征矩阵的Channel\n",
    "        self.num_features = int(embed_dim * 2 ** (self.num_layers - 1))\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "\n",
    "        self.patch_embed = patchEmbed(patch_size=downsapmle_size, in_channels=in_channels, embed_dim=embed_dim, norm_layer=norm_layer if self.patch_norm else None)\n",
    "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
    "\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i_layer in range(self.num_layers):\n",
    "            layers = BasicLayer(dim=int(embed_dim * 2 ** i_layer),\n",
    "                                depth=depths[i_layer],\n",
    "                                num_heads=num_heads[i_layer],\n",
    "                                window_size=window_size, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, drop=drop_rate, attn_drop=attn_drop_rate,\n",
    "                                drop_path=dpr[sum(depths[:i_layer]):sum(depths[:i_layer + 1])], norm_layer=norm_layer, downsample=patchmerging if (i_layer < self.num_layers - 1) else None)\n",
    "            self.layers.append(layers)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # [B, L, C]\n",
    "        x, H, W = self.patch_embed(x)\n",
    "        x = self.pos_drop(x)\n",
    "        hidden_states = []\n",
    "        for layer in self.layers:\n",
    "            x, H, W = layer(x, H, W)\n",
    "            \n",
    "            hidden_states.append(x.clone())\n",
    "\n",
    "        return x, hidden_states\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, num_layers=2, num_heads=8, hidden_dim=512, dropout=0.1):\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(output_dim, hidden_dim)\n",
    "        self.transformer_decoder = nn.TransformerDecoder(\n",
    "            nn.TransformerDecoderLayer(d_model=hidden_dim,\n",
    "                nhead=num_heads,\n",
    "                dim_feedforward=hidden_dim * 4,\n",
    "                dropout=dropout\n",
    "            ),\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "    def forward(self, decoder_input_ids, encoder_hidden_states):\n",
    "        \n",
    "        # Embedding layer\n",
    "        \n",
    "        x = self.embedding(decoder_input_ids)\n",
    "        x = x.permute(1, 0, 2).to(device)\n",
    "        attn_mask = torch.full_like(x, float('-inf'))\n",
    "        attn_mask = torch.triu(attn_mask, diagonal=1)\n",
    "\n",
    "        encoder_hidden_states = encoder_hidden_states.view(encoder_hidden_states.size(0), -1, encoder_hidden_states.size(-1))\n",
    "        encoder_hidden_states = encoder_hidden_states.permute(0, 2, 1)\n",
    "        new_shape = (-1, 1, 1, 512)\n",
    "        reshaped_tensor = encoder_hidden_states.view(new_shape)\n",
    "        reshaped_tensor=reshaped_tensor.squeeze(dim=1)\n",
    "        output = self.transformer_decoder(x,reshaped_tensor)\n",
    "        output = self.fc(output[-1])\n",
    "        \n",
    "        return output\n",
    "       \n",
    "          \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Img2TxtModel(nn.Module):\n",
    "    def __init__(self, transformer_config, vocab_size):\n",
    "        super(Img2TxtModel, self).__init__()\n",
    "        self.encoder  = SwinTransformerFeatureExtractor()\n",
    "\n",
    "        # Transformer解码器配置\n",
    "        transformer_config = BertConfig(vocab_size=vocab_size, num_hidden_layers=1, is_decoder=True,  add_cross_attention=True)\n",
    "        self.decoder = BertModel(transformer_config)\n",
    "\n",
    "        # 预测每个词的线性层\n",
    "        self.vocab_size = vocab_size\n",
    "        self.fc = nn.Linear(transformer_config.hidden_size, vocab_size)\n",
    "    \n",
    "    def forward(self, input_ids, decoder_input_ids, decoder_attention_mask):\n",
    "        # 编码器获取图像特征\n",
    "        encoder_outputs ,hidden_states= self.encoder(input_ids)\n",
    "\n",
    "        # 将图像特征作为解码器的输入\n",
    "        decoder_outputs = self.decoder(input_ids=decoder_input_ids, \n",
    "                                       attention_mask=decoder_attention_mask,\n",
    "                                       encoder_hidden_states=encoder_outputs).last_hidden_state\n",
    "\n",
    "        # 预测下一个词\n",
    "        prediction_scores = self.fc(decoder_outputs)\n",
    "        return prediction_scores\n",
    "\n",
    "    def generate_text(self, input_ids, max_length=95, start_token_id=154):\n",
    "        # 获取图像特征\n",
    "        encoder_outputs ,hidden_states= self.encoder(input_ids)#.last_hidden_state\n",
    "\n",
    "        # 初始化解码器输入为<start>标记\n",
    "        decoder_input_ids = torch.full((input_ids.size(0), 1), start_token_id).to(input_ids.device)\n",
    "        \n",
    "        # 存储所有时间步的logits\n",
    "        all_logits = []\n",
    "\n",
    "        for step in range(max_length):\n",
    "            # 获取解码器输出\n",
    "            decoder_outputs = self.decoder(\n",
    "                input_ids=decoder_input_ids, \n",
    "                encoder_hidden_states=encoder_outputs\n",
    "            ).last_hidden_state\n",
    "\n",
    "            # 预测下一个词\n",
    "            next_word_logits = self.fc(decoder_outputs[:, -1, :])\n",
    "            all_logits.append(next_word_logits.unsqueeze(1))\n",
    "            next_word_id = next_word_logits.argmax(dim=-1).unsqueeze(-1)\n",
    "            \n",
    "            # 将预测的词添加到解码器输入中\n",
    "            decoder_input_ids = torch.cat([decoder_input_ids, next_word_id], dim=-1)\n",
    "        \n",
    "        return decoder_input_ids ,torch.cat(all_logits, dim=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA GeForce RTX 3080\n",
      "Epoch [1/10], Batch [100/3385], Loss: 1.1622642278671265\n",
      "Epoch [1/10], Batch [200/3385], Loss: 0.9484333395957947\n",
      "Epoch [1/10], Batch [300/3385], Loss: 0.794165313243866\n",
      "Epoch [1/10], Batch [400/3385], Loss: 0.7194788455963135\n",
      "Epoch [1/10], Batch [500/3385], Loss: 0.6723021268844604\n",
      "Epoch [1/10], Batch [600/3385], Loss: 0.587674617767334\n",
      "Epoch [1/10], Batch [700/3385], Loss: 0.6576151847839355\n",
      "Epoch [1/10], Batch [800/3385], Loss: 0.6976811289787292\n",
      "Epoch [1/10], Batch [900/3385], Loss: 0.580855667591095\n",
      "Epoch [1/10], Batch [900/3385], Loss: 0.580855667591095, BLEU Score: 0.6756206887074717\n",
      "New best model saved to /home/u2021213687/ImageCaptioning/ARCTIC/model/best_model_epoch_1_batch_900.pth with BLEU score 0.6756206887074717\n",
      "Epoch [1/10], Batch [1000/3385], Loss: 0.6942274570465088\n",
      "Epoch [1/10], Batch [1100/3385], Loss: 0.5980542302131653\n",
      "Epoch [1/10], Batch [1200/3385], Loss: 0.6695214509963989\n",
      "Epoch [1/10], Batch [1300/3385], Loss: 0.6194022297859192\n",
      "Epoch [1/10], Batch [1400/3385], Loss: 0.6105453372001648\n",
      "Epoch [1/10], Batch [1500/3385], Loss: 0.4587896466255188\n",
      "Epoch [1/10], Batch [1600/3385], Loss: 0.5418819785118103\n",
      "Epoch [1/10], Batch [1700/3385], Loss: 0.5536412000656128\n",
      "Epoch [1/10], Batch [1800/3385], Loss: 0.6527004241943359\n",
      "Epoch [1/10], Batch [1800/3385], Loss: 0.6527004241943359, BLEU Score: 0.48057163200987085\n",
      "Epoch [1/10], Batch [1900/3385], Loss: 0.7389681339263916\n",
      "Epoch [1/10], Batch [2000/3385], Loss: 0.55706387758255\n",
      "Epoch [1/10], Batch [2100/3385], Loss: 0.5490767359733582\n",
      "Epoch [1/10], Batch [2200/3385], Loss: 0.6675091981887817\n",
      "Epoch [1/10], Batch [2300/3385], Loss: 0.5366799831390381\n",
      "Epoch [1/10], Batch [2400/3385], Loss: 0.6097325682640076\n",
      "Epoch [1/10], Batch [2500/3385], Loss: 0.48155269026756287\n",
      "Epoch [1/10], Batch [2600/3385], Loss: 0.5541577935218811\n",
      "Epoch [1/10], Batch [2700/3385], Loss: 0.514247715473175\n",
      "Epoch [1/10], Batch [2700/3385], Loss: 0.514247715473175, BLEU Score: 0.6535008575105448\n",
      "Epoch [1/10], Batch [2800/3385], Loss: 0.43529394268989563\n",
      "Epoch [1/10], Batch [2900/3385], Loss: 0.46906372904777527\n",
      "Epoch [1/10], Batch [3000/3385], Loss: 0.4627333879470825\n",
      "Epoch [1/10], Batch [3100/3385], Loss: 0.6529269814491272\n",
      "Epoch [1/10], Batch [3200/3385], Loss: 0.5369230508804321\n",
      "Epoch [1/10], Batch [3300/3385], Loss: 0.5638862252235413\n",
      "Epoch [2/10], Batch [100/3385], Loss: 0.5533261299133301\n",
      "Epoch [2/10], Batch [200/3385], Loss: 0.46928954124450684\n",
      "Epoch [2/10], Batch [300/3385], Loss: 0.47115764021873474\n",
      "Epoch [2/10], Batch [400/3385], Loss: 0.5360379815101624\n",
      "Epoch [2/10], Batch [500/3385], Loss: 0.5039961338043213\n",
      "Epoch [2/10], Batch [600/3385], Loss: 0.49417921900749207\n",
      "Epoch [2/10], Batch [700/3385], Loss: 0.6006736755371094\n",
      "Epoch [2/10], Batch [800/3385], Loss: 0.5970262885093689\n",
      "Epoch [2/10], Batch [900/3385], Loss: 0.4770062565803528\n",
      "Epoch [2/10], Batch [900/3385], Loss: 0.4770062565803528, BLEU Score: 0.5676160889173599\n",
      "Epoch [2/10], Batch [1000/3385], Loss: 0.5385525822639465\n",
      "Epoch [2/10], Batch [1100/3385], Loss: 0.39993128180503845\n",
      "Epoch [2/10], Batch [1200/3385], Loss: 0.4737723469734192\n",
      "Epoch [2/10], Batch [1300/3385], Loss: 0.4864092767238617\n",
      "Epoch [2/10], Batch [1400/3385], Loss: 0.5255646109580994\n",
      "Epoch [2/10], Batch [1500/3385], Loss: 0.4468797445297241\n",
      "Epoch [2/10], Batch [1600/3385], Loss: 0.4486602544784546\n",
      "Epoch [2/10], Batch [1700/3385], Loss: 0.4476377069950104\n",
      "Epoch [2/10], Batch [1800/3385], Loss: 0.4511355459690094\n",
      "Epoch [2/10], Batch [1800/3385], Loss: 0.4511355459690094, BLEU Score: 0.6362680497151638\n",
      "Epoch [2/10], Batch [1900/3385], Loss: 0.6040804386138916\n",
      "Epoch [2/10], Batch [2000/3385], Loss: 0.44357338547706604\n",
      "Epoch [2/10], Batch [2100/3385], Loss: 0.5891521573066711\n",
      "Epoch [2/10], Batch [2200/3385], Loss: 0.5060234665870667\n",
      "Epoch [2/10], Batch [2300/3385], Loss: 0.5462706089019775\n",
      "Epoch [2/10], Batch [2400/3385], Loss: 0.461823433637619\n",
      "Epoch [2/10], Batch [2500/3385], Loss: 0.4933989346027374\n",
      "Epoch [2/10], Batch [2600/3385], Loss: 0.5023946762084961\n",
      "Epoch [2/10], Batch [2700/3385], Loss: 0.4320172667503357\n",
      "Epoch [2/10], Batch [2700/3385], Loss: 0.4320172667503357, BLEU Score: 0.6054579694654114\n",
      "Epoch [2/10], Batch [2800/3385], Loss: 0.5090872049331665\n",
      "Epoch [2/10], Batch [2900/3385], Loss: 0.42105117440223694\n",
      "Epoch [2/10], Batch [3000/3385], Loss: 0.515643835067749\n",
      "Epoch [2/10], Batch [3100/3385], Loss: 0.48999345302581787\n",
      "Epoch [2/10], Batch [3200/3385], Loss: 0.4519733190536499\n",
      "Epoch [2/10], Batch [3300/3385], Loss: 0.4103909730911255\n",
      "Epoch [3/10], Batch [100/3385], Loss: 0.4990445375442505\n",
      "Epoch [3/10], Batch [200/3385], Loss: 0.49841904640197754\n",
      "Epoch [3/10], Batch [300/3385], Loss: 0.507885217666626\n",
      "Epoch [3/10], Batch [400/3385], Loss: 0.46631139516830444\n",
      "Epoch [3/10], Batch [500/3385], Loss: 0.5203529000282288\n",
      "Epoch [3/10], Batch [600/3385], Loss: 0.3993705213069916\n",
      "Epoch [3/10], Batch [700/3385], Loss: 0.48673465847969055\n",
      "Epoch [3/10], Batch [800/3385], Loss: 0.424417108297348\n",
      "Epoch [3/10], Batch [900/3385], Loss: 0.583488941192627\n",
      "Epoch [3/10], Batch [900/3385], Loss: 0.583488941192627, BLEU Score: 0.5719473613488195\n",
      "Epoch [3/10], Batch [1000/3385], Loss: 0.453644722700119\n",
      "Epoch [3/10], Batch [1100/3385], Loss: 0.41832679510116577\n",
      "Epoch [3/10], Batch [1200/3385], Loss: 0.48554298281669617\n",
      "Epoch [3/10], Batch [1300/3385], Loss: 0.4144153892993927\n",
      "Epoch [3/10], Batch [1400/3385], Loss: 0.4240263104438782\n",
      "Epoch [3/10], Batch [1500/3385], Loss: 0.5388942956924438\n",
      "Epoch [3/10], Batch [1600/3385], Loss: 0.5339193344116211\n",
      "Epoch [3/10], Batch [1700/3385], Loss: 0.40181806683540344\n",
      "Epoch [3/10], Batch [1800/3385], Loss: 0.46247419714927673\n",
      "Epoch [3/10], Batch [1800/3385], Loss: 0.46247419714927673, BLEU Score: 0.6204843940236752\n",
      "Epoch [3/10], Batch [1900/3385], Loss: 0.47653183341026306\n",
      "Epoch [3/10], Batch [2000/3385], Loss: 0.46227335929870605\n",
      "Epoch [3/10], Batch [2100/3385], Loss: 0.5195754766464233\n",
      "Epoch [3/10], Batch [2200/3385], Loss: 0.3912004232406616\n",
      "Epoch [3/10], Batch [2300/3385], Loss: 0.5265180468559265\n",
      "Epoch [3/10], Batch [2400/3385], Loss: 0.4441624879837036\n",
      "Epoch [3/10], Batch [2500/3385], Loss: 0.46272894740104675\n",
      "Epoch [3/10], Batch [2600/3385], Loss: 0.4495912194252014\n",
      "Epoch [3/10], Batch [2700/3385], Loss: 0.40168479084968567\n",
      "Epoch [3/10], Batch [2700/3385], Loss: 0.40168479084968567, BLEU Score: 0.57729668085302\n",
      "Epoch [3/10], Batch [2800/3385], Loss: 0.5060304999351501\n",
      "Epoch [3/10], Batch [2900/3385], Loss: 0.542866587638855\n",
      "Epoch [3/10], Batch [3000/3385], Loss: 0.4483906328678131\n",
      "Epoch [3/10], Batch [3100/3385], Loss: 0.562369704246521\n",
      "Epoch [3/10], Batch [3200/3385], Loss: 0.5321714282035828\n",
      "Epoch [3/10], Batch [3300/3385], Loss: 0.4676778018474579\n",
      "Epoch [4/10], Batch [100/3385], Loss: 0.5234858989715576\n",
      "Epoch [4/10], Batch [200/3385], Loss: 0.44342517852783203\n",
      "Epoch [4/10], Batch [300/3385], Loss: 0.520027756690979\n",
      "Epoch [4/10], Batch [400/3385], Loss: 0.5283620953559875\n",
      "Epoch [4/10], Batch [500/3385], Loss: 0.44060492515563965\n",
      "Epoch [4/10], Batch [600/3385], Loss: 0.6008850932121277\n",
      "Epoch [4/10], Batch [700/3385], Loss: 0.48241785168647766\n",
      "Epoch [4/10], Batch [800/3385], Loss: 0.326406329870224\n",
      "Epoch [4/10], Batch [900/3385], Loss: 0.5257508158683777\n",
      "Epoch [4/10], Batch [900/3385], Loss: 0.5257508158683777, BLEU Score: 0.64402369407205\n",
      "Epoch [4/10], Batch [1000/3385], Loss: 0.5039112567901611\n",
      "Epoch [4/10], Batch [1100/3385], Loss: 0.43356338143348694\n",
      "Epoch [4/10], Batch [1200/3385], Loss: 0.3799528479576111\n",
      "Epoch [4/10], Batch [1300/3385], Loss: 0.48491325974464417\n",
      "Epoch [4/10], Batch [1400/3385], Loss: 0.4557781219482422\n",
      "Epoch [4/10], Batch [1500/3385], Loss: 0.5142585635185242\n",
      "Epoch [4/10], Batch [1600/3385], Loss: 0.48900651931762695\n",
      "Epoch [4/10], Batch [1700/3385], Loss: 0.511745035648346\n",
      "Epoch [4/10], Batch [1800/3385], Loss: 0.458896279335022\n",
      "Epoch [4/10], Batch [1800/3385], Loss: 0.458896279335022, BLEU Score: 0.5978335011875666\n",
      "Epoch [4/10], Batch [1900/3385], Loss: 0.5410134792327881\n",
      "Epoch [4/10], Batch [2000/3385], Loss: 0.43945038318634033\n",
      "Epoch [4/10], Batch [2100/3385], Loss: 0.5591626763343811\n",
      "Epoch [4/10], Batch [2200/3385], Loss: 0.446851909160614\n",
      "Epoch [4/10], Batch [2300/3385], Loss: 0.4762606620788574\n",
      "Epoch [4/10], Batch [2400/3385], Loss: 0.40739938616752625\n",
      "Epoch [4/10], Batch [2500/3385], Loss: 0.4328993260860443\n",
      "Epoch [4/10], Batch [2600/3385], Loss: 0.46091729402542114\n",
      "Epoch [4/10], Batch [2700/3385], Loss: 0.42632541060447693\n",
      "Epoch [4/10], Batch [2700/3385], Loss: 0.42632541060447693, BLEU Score: 0.5973642187541907\n",
      "Epoch [4/10], Batch [2800/3385], Loss: 0.49016547203063965\n",
      "Epoch [4/10], Batch [2900/3385], Loss: 0.47704237699508667\n",
      "Epoch [4/10], Batch [3000/3385], Loss: 0.5123234987258911\n",
      "Epoch [4/10], Batch [3100/3385], Loss: 0.39873138070106506\n",
      "Epoch [4/10], Batch [3200/3385], Loss: 0.44941064715385437\n",
      "Epoch [4/10], Batch [3300/3385], Loss: 0.48853808641433716\n",
      "Epoch [5/10], Batch [100/3385], Loss: 0.3748058080673218\n",
      "Epoch [5/10], Batch [200/3385], Loss: 0.4510648846626282\n",
      "Epoch [5/10], Batch [300/3385], Loss: 0.4762944281101227\n",
      "Epoch [5/10], Batch [400/3385], Loss: 0.5451634526252747\n",
      "Epoch [5/10], Batch [500/3385], Loss: 0.477049320936203\n",
      "Epoch [5/10], Batch [600/3385], Loss: 0.3951447606086731\n",
      "Epoch [5/10], Batch [700/3385], Loss: 0.45421144366264343\n",
      "Epoch [5/10], Batch [800/3385], Loss: 0.4228250980377197\n",
      "Epoch [5/10], Batch [900/3385], Loss: 0.5014289021492004\n",
      "Epoch [5/10], Batch [900/3385], Loss: 0.5014289021492004, BLEU Score: 0.6210297096868636\n",
      "Epoch [5/10], Batch [1000/3385], Loss: 0.3887266218662262\n",
      "Epoch [5/10], Batch [1100/3385], Loss: 0.4869125485420227\n",
      "Epoch [5/10], Batch [1200/3385], Loss: 0.48141902685165405\n",
      "Epoch [5/10], Batch [1300/3385], Loss: 0.47915104031562805\n",
      "Epoch [5/10], Batch [1400/3385], Loss: 0.41996467113494873\n",
      "Epoch [5/10], Batch [1500/3385], Loss: 0.4438263773918152\n",
      "Epoch [5/10], Batch [1600/3385], Loss: 0.45371192693710327\n",
      "Epoch [5/10], Batch [1700/3385], Loss: 0.4728551506996155\n",
      "Epoch [5/10], Batch [1800/3385], Loss: 0.46682795882225037\n",
      "Epoch [5/10], Batch [1800/3385], Loss: 0.46682795882225037, BLEU Score: 0.5592221685750911\n",
      "Epoch [5/10], Batch [1900/3385], Loss: 0.43969112634658813\n",
      "Epoch [5/10], Batch [2000/3385], Loss: 0.450147420167923\n",
      "Epoch [5/10], Batch [2100/3385], Loss: 0.4243055284023285\n",
      "Epoch [5/10], Batch [2200/3385], Loss: 0.5837340950965881\n",
      "Epoch [5/10], Batch [2300/3385], Loss: 0.47797009348869324\n",
      "Epoch [5/10], Batch [2400/3385], Loss: 0.6294732689857483\n",
      "Epoch [5/10], Batch [2500/3385], Loss: 0.438723623752594\n",
      "Epoch [5/10], Batch [2600/3385], Loss: 0.4500543475151062\n",
      "Epoch [5/10], Batch [2700/3385], Loss: 0.49317073822021484\n",
      "Epoch [5/10], Batch [2700/3385], Loss: 0.49317073822021484, BLEU Score: 0.5776978082814043\n",
      "Epoch [5/10], Batch [2800/3385], Loss: 0.4650542736053467\n",
      "Epoch [5/10], Batch [2900/3385], Loss: 0.5605517029762268\n",
      "Epoch [5/10], Batch [3000/3385], Loss: 0.5151405930519104\n",
      "Epoch [5/10], Batch [3100/3385], Loss: 0.4479585886001587\n",
      "Epoch [5/10], Batch [3200/3385], Loss: 0.45301076769828796\n",
      "Epoch [5/10], Batch [3300/3385], Loss: 0.42256712913513184\n",
      "Epoch [6/10], Batch [100/3385], Loss: 0.4103618264198303\n",
      "Epoch [6/10], Batch [200/3385], Loss: 0.5123975276947021\n",
      "Epoch [6/10], Batch [300/3385], Loss: 0.5703345537185669\n",
      "Epoch [6/10], Batch [400/3385], Loss: 0.5040557384490967\n",
      "Epoch [6/10], Batch [500/3385], Loss: 0.4675014913082123\n",
      "Epoch [6/10], Batch [600/3385], Loss: 0.5346106290817261\n",
      "Epoch [6/10], Batch [700/3385], Loss: 0.4039643704891205\n",
      "Epoch [6/10], Batch [800/3385], Loss: 0.48243242502212524\n",
      "Epoch [6/10], Batch [900/3385], Loss: 0.4647281765937805\n",
      "Epoch [6/10], Batch [900/3385], Loss: 0.4647281765937805, BLEU Score: 0.5468606627855241\n",
      "Epoch [6/10], Batch [1000/3385], Loss: 0.43168938159942627\n",
      "Epoch [6/10], Batch [1100/3385], Loss: 0.4688193202018738\n",
      "Epoch [6/10], Batch [1200/3385], Loss: 0.559234082698822\n",
      "Epoch [6/10], Batch [1300/3385], Loss: 0.41780245304107666\n",
      "Epoch [6/10], Batch [1400/3385], Loss: 0.4594038426876068\n",
      "Epoch [6/10], Batch [1500/3385], Loss: 0.5161823034286499\n",
      "Epoch [6/10], Batch [1600/3385], Loss: 0.48843276500701904\n",
      "Epoch [6/10], Batch [1700/3385], Loss: 0.389083594083786\n",
      "Epoch [6/10], Batch [1800/3385], Loss: 0.43299829959869385\n",
      "Epoch [6/10], Batch [1800/3385], Loss: 0.43299829959869385, BLEU Score: 0.5928509122592276\n",
      "Epoch [6/10], Batch [1900/3385], Loss: 0.4140392243862152\n",
      "Epoch [6/10], Batch [2000/3385], Loss: 0.49803411960601807\n",
      "Epoch [6/10], Batch [2100/3385], Loss: 0.45513680577278137\n",
      "Epoch [6/10], Batch [2200/3385], Loss: 0.4872467517852783\n",
      "Epoch [6/10], Batch [2300/3385], Loss: 0.49528712034225464\n",
      "Epoch [6/10], Batch [2400/3385], Loss: 0.4263647198677063\n",
      "Epoch [6/10], Batch [2500/3385], Loss: 0.34864550828933716\n",
      "Epoch [6/10], Batch [2600/3385], Loss: 0.5345019698143005\n",
      "Epoch [6/10], Batch [2700/3385], Loss: 0.37819257378578186\n",
      "Epoch [6/10], Batch [2700/3385], Loss: 0.37819257378578186, BLEU Score: 0.5746916929604773\n",
      "Epoch [6/10], Batch [2800/3385], Loss: 0.42518261075019836\n",
      "Epoch [6/10], Batch [2900/3385], Loss: 0.48851799964904785\n",
      "Epoch [6/10], Batch [3000/3385], Loss: 0.432393342256546\n",
      "Epoch [6/10], Batch [3100/3385], Loss: 0.42756158113479614\n",
      "Epoch [6/10], Batch [3200/3385], Loss: 0.4097176790237427\n",
      "Epoch [6/10], Batch [3300/3385], Loss: 0.4087303876876831\n",
      "Epoch [7/10], Batch [100/3385], Loss: 0.46848827600479126\n",
      "Epoch [7/10], Batch [200/3385], Loss: 0.5192675590515137\n",
      "Epoch [7/10], Batch [300/3385], Loss: 0.39826300740242004\n",
      "Epoch [7/10], Batch [400/3385], Loss: 0.37436532974243164\n",
      "Epoch [7/10], Batch [500/3385], Loss: 0.49053245782852173\n",
      "Epoch [7/10], Batch [600/3385], Loss: 0.40783241391181946\n",
      "Epoch [7/10], Batch [700/3385], Loss: 0.41803979873657227\n",
      "Epoch [7/10], Batch [800/3385], Loss: 0.5242230892181396\n",
      "Epoch [7/10], Batch [900/3385], Loss: 0.46637415885925293\n",
      "Epoch [7/10], Batch [900/3385], Loss: 0.46637415885925293, BLEU Score: 0.6020287627248471\n",
      "Epoch [7/10], Batch [1000/3385], Loss: 0.4544418454170227\n",
      "Epoch [7/10], Batch [1100/3385], Loss: 0.4645463228225708\n",
      "Epoch [7/10], Batch [1200/3385], Loss: 0.45061591267585754\n",
      "Epoch [7/10], Batch [1300/3385], Loss: 0.44948622584342957\n",
      "Epoch [7/10], Batch [1400/3385], Loss: 0.4295600652694702\n",
      "Epoch [7/10], Batch [1500/3385], Loss: 0.41589853167533875\n",
      "Epoch [7/10], Batch [1600/3385], Loss: 0.5246374607086182\n",
      "Epoch [7/10], Batch [1700/3385], Loss: 0.5067019462585449\n",
      "Epoch [7/10], Batch [1800/3385], Loss: 0.5595409274101257\n",
      "Epoch [7/10], Batch [1800/3385], Loss: 0.5595409274101257, BLEU Score: 0.5845679496568482\n",
      "Epoch [7/10], Batch [1900/3385], Loss: 0.44941866397857666\n",
      "Epoch [7/10], Batch [2000/3385], Loss: 0.4469953775405884\n",
      "Epoch [7/10], Batch [2100/3385], Loss: 0.5134799480438232\n",
      "Epoch [7/10], Batch [2200/3385], Loss: 0.45455119013786316\n",
      "Epoch [7/10], Batch [2300/3385], Loss: 0.5060573816299438\n",
      "Epoch [7/10], Batch [2400/3385], Loss: 0.40012022852897644\n",
      "Epoch [7/10], Batch [2500/3385], Loss: 0.46678176522254944\n",
      "Epoch [7/10], Batch [2600/3385], Loss: 0.4792492389678955\n",
      "Epoch [7/10], Batch [2700/3385], Loss: 0.38847994804382324\n",
      "Epoch [7/10], Batch [2700/3385], Loss: 0.38847994804382324, BLEU Score: 0.5888306268657159\n",
      "Epoch [7/10], Batch [2800/3385], Loss: 0.46476665139198303\n",
      "Epoch [7/10], Batch [2900/3385], Loss: 0.4836307764053345\n",
      "Epoch [7/10], Batch [3000/3385], Loss: 0.39624717831611633\n",
      "Epoch [7/10], Batch [3100/3385], Loss: 0.43320199847221375\n",
      "Epoch [7/10], Batch [3200/3385], Loss: 0.44932326674461365\n",
      "Epoch [7/10], Batch [3300/3385], Loss: 0.3723844289779663\n",
      "Epoch [8/10], Batch [100/3385], Loss: 0.3481389582157135\n",
      "Epoch [8/10], Batch [200/3385], Loss: 0.45620444416999817\n",
      "Epoch [8/10], Batch [300/3385], Loss: 0.39793187379837036\n",
      "Epoch [8/10], Batch [400/3385], Loss: 0.49439510703086853\n",
      "Epoch [8/10], Batch [500/3385], Loss: 0.4366127848625183\n",
      "Epoch [8/10], Batch [600/3385], Loss: 0.4342566132545471\n",
      "Epoch [8/10], Batch [700/3385], Loss: 0.3708777129650116\n",
      "Epoch [8/10], Batch [800/3385], Loss: 0.47872838377952576\n",
      "Epoch [8/10], Batch [900/3385], Loss: 0.3954927325248718\n",
      "Epoch [8/10], Batch [900/3385], Loss: 0.3954927325248718, BLEU Score: 0.5941537151156882\n",
      "Epoch [8/10], Batch [1000/3385], Loss: 0.5594679713249207\n",
      "Epoch [8/10], Batch [1100/3385], Loss: 0.42039749026298523\n",
      "Epoch [8/10], Batch [1200/3385], Loss: 0.3597589135169983\n",
      "Epoch [8/10], Batch [1300/3385], Loss: 0.4236447215080261\n",
      "Epoch [8/10], Batch [1400/3385], Loss: 0.421511709690094\n",
      "Epoch [8/10], Batch [1500/3385], Loss: 0.46433025598526\n",
      "Epoch [8/10], Batch [1600/3385], Loss: 0.42888736724853516\n",
      "Epoch [8/10], Batch [1700/3385], Loss: 0.45256102085113525\n",
      "Epoch [8/10], Batch [1800/3385], Loss: 0.38545382022857666\n",
      "Epoch [8/10], Batch [1800/3385], Loss: 0.38545382022857666, BLEU Score: 0.6377169579188178\n",
      "Epoch [8/10], Batch [1900/3385], Loss: 0.3731049597263336\n",
      "Epoch [8/10], Batch [2000/3385], Loss: 0.37443581223487854\n",
      "Epoch [8/10], Batch [2100/3385], Loss: 0.46771788597106934\n",
      "Epoch [8/10], Batch [2200/3385], Loss: 0.42515304684638977\n",
      "Epoch [8/10], Batch [2300/3385], Loss: 0.3774884343147278\n",
      "Epoch [8/10], Batch [2400/3385], Loss: 0.5099074244499207\n",
      "Epoch [8/10], Batch [2500/3385], Loss: 0.47884994745254517\n",
      "Epoch [8/10], Batch [2600/3385], Loss: 0.4618517756462097\n",
      "Epoch [8/10], Batch [2700/3385], Loss: 0.4351719915866852\n",
      "Epoch [8/10], Batch [2700/3385], Loss: 0.4351719915866852, BLEU Score: 0.6037020170047647\n",
      "Epoch [8/10], Batch [2800/3385], Loss: 0.4120735228061676\n",
      "Epoch [8/10], Batch [2900/3385], Loss: 0.4774492681026459\n",
      "Epoch [8/10], Batch [3000/3385], Loss: 0.3688049912452698\n",
      "Epoch [8/10], Batch [3100/3385], Loss: 0.45926469564437866\n",
      "Epoch [8/10], Batch [3200/3385], Loss: 0.43501681089401245\n",
      "Epoch [8/10], Batch [3300/3385], Loss: 0.43942418694496155\n",
      "Epoch [9/10], Batch [100/3385], Loss: 0.41556602716445923\n",
      "Epoch [9/10], Batch [200/3385], Loss: 0.4853711426258087\n",
      "Epoch [9/10], Batch [300/3385], Loss: 0.5424379706382751\n",
      "Epoch [9/10], Batch [400/3385], Loss: 0.44168218970298767\n",
      "Epoch [9/10], Batch [500/3385], Loss: 0.5013605356216431\n",
      "Epoch [9/10], Batch [600/3385], Loss: 0.39477473497390747\n",
      "Epoch [9/10], Batch [700/3385], Loss: 0.49063581228256226\n",
      "Epoch [9/10], Batch [800/3385], Loss: 0.5335999131202698\n",
      "Epoch [9/10], Batch [900/3385], Loss: 0.4597897529602051\n",
      "Epoch [9/10], Batch [900/3385], Loss: 0.4597897529602051, BLEU Score: 0.5558482996525596\n",
      "Epoch [9/10], Batch [1000/3385], Loss: 0.40152230858802795\n",
      "Epoch [9/10], Batch [1100/3385], Loss: 0.5168380737304688\n",
      "Epoch [9/10], Batch [1200/3385], Loss: 0.5736192464828491\n",
      "Epoch [9/10], Batch [1300/3385], Loss: 0.45579993724823\n",
      "Epoch [9/10], Batch [1400/3385], Loss: 0.33923766016960144\n",
      "Epoch [9/10], Batch [1500/3385], Loss: 0.47240787744522095\n",
      "Epoch [9/10], Batch [1600/3385], Loss: 0.41163063049316406\n",
      "Epoch [9/10], Batch [1700/3385], Loss: 0.6011322736740112\n",
      "Epoch [9/10], Batch [1800/3385], Loss: 0.4643775224685669\n",
      "Epoch [9/10], Batch [1800/3385], Loss: 0.4643775224685669, BLEU Score: 0.6595749782956083\n",
      "Epoch [9/10], Batch [1900/3385], Loss: 0.4569464921951294\n",
      "Epoch [9/10], Batch [2000/3385], Loss: 0.4320577085018158\n",
      "Epoch [9/10], Batch [2100/3385], Loss: 0.3963547646999359\n",
      "Epoch [9/10], Batch [2200/3385], Loss: 0.42486366629600525\n",
      "Epoch [9/10], Batch [2300/3385], Loss: 0.44021159410476685\n",
      "Epoch [9/10], Batch [2400/3385], Loss: 0.3817131519317627\n",
      "Epoch [9/10], Batch [2500/3385], Loss: 0.43480178713798523\n",
      "Epoch [9/10], Batch [2600/3385], Loss: 0.4299853444099426\n",
      "Epoch [9/10], Batch [2700/3385], Loss: 0.4212871193885803\n",
      "Epoch [9/10], Batch [2700/3385], Loss: 0.4212871193885803, BLEU Score: 0.5575299022045547\n",
      "Epoch [9/10], Batch [2800/3385], Loss: 0.4076331853866577\n",
      "Epoch [9/10], Batch [2900/3385], Loss: 0.46511921286582947\n",
      "Epoch [9/10], Batch [3000/3385], Loss: 0.4243347644805908\n",
      "Epoch [9/10], Batch [3100/3385], Loss: 0.40208670496940613\n",
      "Epoch [9/10], Batch [3200/3385], Loss: 0.3230813443660736\n",
      "Epoch [9/10], Batch [3300/3385], Loss: 0.5585523247718811\n",
      "Epoch [10/10], Batch [100/3385], Loss: 0.45105281472206116\n",
      "Epoch [10/10], Batch [200/3385], Loss: 0.4550116956233978\n",
      "Epoch [10/10], Batch [300/3385], Loss: 0.4174891710281372\n",
      "Epoch [10/10], Batch [400/3385], Loss: 0.5194677114486694\n",
      "Epoch [10/10], Batch [500/3385], Loss: 0.39167118072509766\n",
      "Epoch [10/10], Batch [600/3385], Loss: 0.4502232074737549\n",
      "Epoch [10/10], Batch [700/3385], Loss: 0.42541638016700745\n",
      "Epoch [10/10], Batch [800/3385], Loss: 0.4721889793872833\n",
      "Epoch [10/10], Batch [900/3385], Loss: 0.42664310336112976\n",
      "Epoch [10/10], Batch [900/3385], Loss: 0.42664310336112976, BLEU Score: 0.5960003352482284\n",
      "Epoch [10/10], Batch [1000/3385], Loss: 0.4651057720184326\n",
      "Epoch [10/10], Batch [1100/3385], Loss: 0.5019055008888245\n",
      "Epoch [10/10], Batch [1200/3385], Loss: 0.42327386140823364\n",
      "Epoch [10/10], Batch [1300/3385], Loss: 0.4507986605167389\n",
      "Epoch [10/10], Batch [1400/3385], Loss: 0.47607898712158203\n",
      "Epoch [10/10], Batch [1500/3385], Loss: 0.3969777524471283\n",
      "Epoch [10/10], Batch [1600/3385], Loss: 0.41069313883781433\n",
      "Epoch [10/10], Batch [1700/3385], Loss: 0.4230535924434662\n",
      "Epoch [10/10], Batch [1800/3385], Loss: 0.4955095052719116\n",
      "Epoch [10/10], Batch [1800/3385], Loss: 0.4955095052719116, BLEU Score: 0.6558603156060071\n",
      "Epoch [10/10], Batch [1900/3385], Loss: 0.4714697301387787\n",
      "Epoch [10/10], Batch [2000/3385], Loss: 0.44930389523506165\n",
      "Epoch [10/10], Batch [2100/3385], Loss: 0.41401609778404236\n",
      "Epoch [10/10], Batch [2200/3385], Loss: 0.5546436309814453\n",
      "Epoch [10/10], Batch [2300/3385], Loss: 0.4436861276626587\n",
      "Epoch [10/10], Batch [2400/3385], Loss: 0.5022885203361511\n",
      "Epoch [10/10], Batch [2500/3385], Loss: 0.40128853917121887\n",
      "Epoch [10/10], Batch [2600/3385], Loss: 0.45368462800979614\n",
      "Epoch [10/10], Batch [2700/3385], Loss: 0.4582984447479248\n",
      "Epoch [10/10], Batch [2700/3385], Loss: 0.4582984447479248, BLEU Score: 0.61927067593469\n",
      "Epoch [10/10], Batch [2800/3385], Loss: 0.4534565508365631\n",
      "Epoch [10/10], Batch [2900/3385], Loss: 0.42754998803138733\n",
      "Epoch [10/10], Batch [3000/3385], Loss: 0.37170517444610596\n",
      "Epoch [10/10], Batch [3100/3385], Loss: 0.39805224537849426\n",
      "Epoch [10/10], Batch [3200/3385], Loss: 0.41399550437927246\n",
      "Epoch [10/10], Batch [3300/3385], Loss: 0.4118269979953766\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using GPU:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "def evaluate(test_loader, model):\n",
    "    model.eval()  # 将模型设置为评估模式\n",
    "    generated_captions = []\n",
    "    actual_captions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, captions, caplens in test_loader:\n",
    "            images = images.to(device)\n",
    "            input_ids = images\n",
    "            outputs,_ = model.generate_text(input_ids, max_length=95, start_token_id=test_dataset.vocab['<start>'])\n",
    "            for i in range(outputs.shape[0]):\n",
    "                # 生成字幕\n",
    "                gen_caption = [idx_to_word(idx, test_dataset.vocab) for idx in outputs[i]]\n",
    "                # print(gen_caption)\n",
    "                # 移除 <start> 和 <end>\n",
    "                if '<start>' in gen_caption:\n",
    "                    gen_caption = gen_caption[1:]  # 移除第一个元素 (<start>)\n",
    "                if '<end>' in gen_caption:\n",
    "                    gen_caption = gen_caption[:gen_caption.index('<end>')]  # 移除 <end> 及其后面的元素\n",
    "                \n",
    "                generated_captions.append(' '.join(gen_caption))\n",
    "\n",
    "                # 真实字幕\n",
    "                act_caption = [idx_to_word(idx, test_dataset.vocab) for idx in captions[i]]\n",
    "                #print(gen_caption)\n",
    "                # 移除 <start> 和 <end>\n",
    "                if '<start>' in act_caption:\n",
    "                    act_caption = act_caption[1:]  # 移除第一个元素 (<start>)\n",
    "                if '<end>' in act_caption:\n",
    "                    act_caption = act_caption[:act_caption.index('<end>')]  # 移除 <end> 及其后面的元素\n",
    "                \n",
    "                actual_captions.append([' '.join(act_caption)])\n",
    "\n",
    "        # BLEU分数\n",
    "        bleu4 = corpus_bleu(actual_captions, generated_captions, weights=(0.25,0.25,0.25,0.25))\n",
    "    model.train()\n",
    "    return bleu4\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)), \n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "dataset = ImageTextDataset(train_json_path, vocab_path, split='train', transform=transform)\n",
    "data_loader = DataLoader(dataset, batch_size=3, shuffle=True)\n",
    "\n",
    "vocab_size = len(dataset.vocab)\n",
    "transformer_config = BertConfig()\n",
    "\n",
    "# 初始化模型\n",
    "model = Img2TxtModel(transformer_config, vocab_size)\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=dataset.vocab['<pad>'])\n",
    "\n",
    "test_dataset = ImageTextDataset(test_json_path, vocab_path, split='test', transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=3, shuffle=True)\n",
    "# 设定训练周期\n",
    "num_epochs = 10\n",
    "best_bleu_score = 0.0  \n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, captions, caplens) in enumerate(data_loader):\n",
    "        # 假设接受标准化的图像张量作为输入\n",
    "        images = images.to(device)\n",
    "        captions = captions.to(device)\n",
    "        input_ids = images\n",
    "\n",
    "        # 准备解码器输入\n",
    "        decoder_input_ids = captions[:, :-1]  # 删除每个字幕的最后一个单词\n",
    "        decoder_attention_mask = (decoder_input_ids != dataset.vocab['<pad>']).type(torch.uint8)\n",
    "        \n",
    "        # 前向传播\n",
    "        outputs = model(input_ids, decoder_input_ids, decoder_attention_mask)\n",
    "        #print(outputs)\n",
    "        # 计算损失，outputs需要调整以适配损失函数的要求\n",
    "        loss = criterion(outputs.view(-1, outputs.size(-1)), captions[:, 1:].contiguous().view(-1))\n",
    "\n",
    "        # 反向传播和优化\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i+1) % 100 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Batch [{i+1}/{len(data_loader)}], Loss: {loss.item()}\")\n",
    "        \n",
    "        if (i + 1) % 900 == 0:\n",
    "            bleu4 = evaluate(test_loader, model)\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Batch [{i+1}/{len(data_loader)}], Loss: {loss.item()}, BLEU Score: {bleu4}\")\n",
    "\n",
    "            # 如果BLEU分数是新的最高分，则保存模型\n",
    "            if bleu4 > best_bleu_score:\n",
    "                best_bleu_score = bleu4\n",
    "                save_path = f\"model/best_model_epoch_{epoch+1}_batch_{i+1}.pth\"\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'batch': i,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'loss': loss,\n",
    "                    'bleu_score': bleu4,\n",
    "                }, save_path)\n",
    "                print(f\"New best model saved to {save_path} with BLEU score {bleu4}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_cut_useless_words(sent, filterd_words):\n",
    "    res=[]\n",
    "    for w in sent:\n",
    "        if w not in filterd_words:\n",
    "            res.append(w)\n",
    "        else:\n",
    "            if w==155:\n",
    "                return res\n",
    "            \n",
    "\n",
    "def get_BLEU_score(cands, refs): #获取BLEU分数\n",
    "    multiple_refs = []\n",
    "    for idx in range(len(refs)):\n",
    "        multiple_refs.append(refs[(idx//1)*1 : (idx//1)*1+1])#每个候选文本对应cpi==1条参考文本\n",
    "    bleu4 = corpus_bleu(multiple_refs, cands, weights=(0.25,0.25,0.25,0.25))\n",
    "    return bleu4\n",
    "\n",
    "def cider_d(reference_list, candidate_list, n=4):\n",
    "    def count_ngrams(tokens, n):\n",
    "        ngrams = []\n",
    "        for i in range(len(tokens) - n + 1):\n",
    "            ngram = tuple(tokens[i:i+n])\n",
    "            ngrams.append(ngram)\n",
    "        return ngrams\n",
    "\n",
    "    def compute_cider_d(reference_list, candidate_list, n):\n",
    "        cider_d_scores = []\n",
    "        for refs, cand in zip(reference_list, candidate_list):\n",
    "            cider_d_score = 0.0\n",
    "            for i in range(1, n + 1):\n",
    "                cand_ngrams = count_ngrams(cand, i)\n",
    "                ref_ngrams_list = [count_ngrams(ref, i) for ref in refs]\n",
    "\n",
    "                total_ref_ngrams = [ngram for ref_ngrams in ref_ngrams_list for ngram in ref_ngrams]\n",
    "\n",
    "                count_cand = 0\n",
    "                count_clip = 0\n",
    "\n",
    "                for ngram in cand_ngrams:\n",
    "                    count_cand += 1\n",
    "                    if ngram in total_ref_ngrams:\n",
    "                        count_clip += 1\n",
    "\n",
    "                precision = count_clip / count_cand if count_cand > 0 else 0.0\n",
    "                recall = count_clip / len(total_ref_ngrams) if len(total_ref_ngrams) > 0 else 0.0\n",
    "\n",
    "                beta = 1.0\n",
    "                f_score = (1 + beta**2) * precision * recall / (beta**2 * precision + recall) if precision + recall > 0 else 0.0\n",
    "\n",
    "                cider_d_score += f_score\n",
    "\n",
    "            cider_d_score /= n\n",
    "            cider_d_scores.append(cider_d_score)\n",
    "\n",
    "        return cider_d_scores\n",
    "\n",
    "    reference_tokens_list = reference_list\n",
    "    candidate_tokens_list = candidate_list\n",
    "\n",
    "    scores = compute_cider_d(reference_tokens_list, candidate_tokens_list, n)\n",
    "\n",
    "    return np.mean(scores)\n",
    "\n",
    "def spice(reference_list, candidate_list, idf=None, beta=3):\n",
    "    def tokenize(sentence):\n",
    "        return sentence.lower().split()\n",
    "\n",
    "    def count_ngrams(tokens, n):\n",
    "        ngrams = []\n",
    "        for i in range(len(tokens) - n + 1):\n",
    "            ngram = tuple(tokens[i:i+n])\n",
    "            ngrams.append(ngram)\n",
    "        return ngrams\n",
    "\n",
    "    def compute_spice_score(reference, candidate, idf, beta):\n",
    "        reference_tokens = reference\n",
    "        candidate_tokens = candidate\n",
    "\n",
    "        reference_ngrams = [count_ngrams(reference_tokens, i) for i in range(1, beta + 1)]\n",
    "        candidate_ngrams = [count_ngrams(candidate_tokens, i) for i in range(1, beta + 1)]\n",
    "\n",
    "        precision_scores = []\n",
    "        recall_scores = []\n",
    "\n",
    "        for i in range(beta):\n",
    "            common_ngrams = set(candidate_ngrams[i]) & set(reference_ngrams[i])\n",
    "\n",
    "            precision = len(common_ngrams) / len(candidate_ngrams[i]) if len(candidate_ngrams[i]) > 0 else 0.0\n",
    "            recall = len(common_ngrams) / len(reference_ngrams[i]) if len(reference_ngrams[i]) > 0 else 0.0\n",
    "\n",
    "            precision_scores.append(precision)\n",
    "            recall_scores.append(recall)\n",
    "\n",
    "        precision_avg = np.mean(precision_scores)\n",
    "        recall_avg = np.mean(recall_scores)\n",
    "\n",
    "        spice_score = (precision_avg * recall_avg) / (precision_avg + recall_avg) if precision_avg + recall_avg > 0 else 0.0\n",
    "\n",
    "        if idf:\n",
    "            spice_score *= np.exp(np.sum([idf[token] for token in common_ngrams]) / len(candidate_tokens))\n",
    "\n",
    "        return spice_score\n",
    "\n",
    "    if idf is None:\n",
    "        idf = {}\n",
    "\n",
    "    spice_scores = []\n",
    "\n",
    "    for reference, candidate in zip(reference_list, candidate_list):\n",
    "        spice_score = compute_spice_score(reference, candidate, idf, beta)\n",
    "        spice_scores.append(spice_score)\n",
    "\n",
    "    return np.mean(spice_scores)\n",
    "\n",
    "def wvec_to_capls(vocab,wvec):#将向量转换成文本描述\n",
    "    res=[]\n",
    "    for word in wvec:\n",
    "        for key,value in vocab.items():\n",
    "            if value==word and key not in ['<start>','<end>','<pad>','<unk>']:\n",
    "                res.append(key)\n",
    "    return res\n",
    "\n",
    "def wvec_to_cap(vocab,wvec):#将向量转换成文本描述\n",
    "    res=[]\n",
    "    for word in wvec:\n",
    "        for key,value in vocab.items():\n",
    "            if value==word and key not in ['<start>','<end>','<pad>','<unk>']:\n",
    "                res.append(key)\n",
    "    res=\" \".join(res)\n",
    "    return res\n",
    "\n",
    "def get_CIDER_D_score(vocab,cands, refs): #获得CIDER-D分数\n",
    "    refs_ = [wvec_to_capls(vocab,ref) for ref in refs]\n",
    "    cands_ = [wvec_to_capls(vocab,cand) for cand in cands]\n",
    "    return cider_d(refs_, cands_)\n",
    "\n",
    "def get_SPICE_score(vocab,cands, refs): #获得SPICE分数\n",
    "    refs_ = [wvec_to_cap(vocab,ref) for ref in refs]\n",
    "    cands_ = [wvec_to_cap(vocab,cand) for cand in cands]\n",
    "    return spice(refs_, cands_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bleu4 :0.6756206887074717\n",
      "bleu4_token :0.30755556179642474\n",
      "cider_d_score :0.004411394091438619\n",
      "spice_score :0.13556899884580814\n"
     ]
    }
   ],
   "source": [
    "test_dataset = ImageTextDataset(test_json_path, vocab_path, split='test', transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=True)\n",
    "vocab_size = len(test_dataset.vocab)\n",
    "transformer_config = BertConfig()\n",
    "model = Img2TxtModel( transformer_config, vocab_size)\n",
    "# 加载模型状态字典\n",
    "checkpoint = torch.load('/home/u2021213687/ImageCaptioning/ARCTIC/model/best_model_epoch_1_batch_900.pth')\n",
    "\n",
    "# 将状态字典应用到模型实例中\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model = model.to(device)\n",
    "\n",
    "model.eval()  # 将模型设置为评估模式\n",
    "\n",
    "generated_captions = []\n",
    "actual_captions = []\n",
    "cands = []\n",
    "refs = []\n",
    "filterd_words = set({test_dataset.vocab['<start>'], test_dataset.vocab['<end>'], test_dataset.vocab['<pad>']})\n",
    "with torch.no_grad():\n",
    "    for images, captions, caplens in test_loader:\n",
    "        images = images.to(device)\n",
    "        input_ids = images\n",
    "        outputs,_ = model.generate_text(input_ids, max_length=95, start_token_id=test_dataset.vocab['<start>'])\n",
    "        for i in range(outputs.shape[0]):\n",
    "            gen_caption = [idx_to_word(idx, test_dataset.vocab) for idx in outputs[i]]\n",
    "            if '<start>' in gen_caption:\n",
    "                gen_caption = gen_caption[1:]  # 移除第一个元素 (<start>)\n",
    "            if '<end>' in gen_caption:\n",
    "                gen_caption = gen_caption[:gen_caption.index('<end>')]  # 移除 <end> 及其后面的元素\n",
    "            generated_captions.append(' '.join(gen_caption))\n",
    "            act_caption = [idx_to_word(idx, test_dataset.vocab) for idx in captions[i]]\n",
    "            # 移除 <start> 和 <end>\n",
    "            if '<start>' in act_caption:\n",
    "                act_caption = act_caption[1:]  # 移除第一个元素 (<start>)\n",
    "            if '<end>' in act_caption:\n",
    "                act_caption = act_caption[:act_caption.index('<end>')]  # 移除 <end> 及其后面的元素\n",
    "            \n",
    "            actual_captions.append([' '.join(act_caption)])\n",
    "        texts=outputs\n",
    "        cands.extend([filter_cut_useless_words(text, filterd_words) for text in texts.tolist()])\n",
    "            # 参考文本\n",
    "        refs.extend([filter_cut_useless_words(cap, filterd_words) for cap in captions.tolist()])\n",
    "    \n",
    "    bleu4 = corpus_bleu(actual_captions, generated_captions, weights=(0.25,0.25,0.25,0.25))\n",
    "    \n",
    "    bleu4_token=get_BLEU_score(cands, refs)\n",
    "\n",
    "    cider_d_score=get_CIDER_D_score(test_dataset.vocab,refs, cands)\n",
    "    \n",
    "    spice_score=get_SPICE_score(test_dataset.vocab,refs, cands)\n",
    "    \n",
    "\n",
    "    print(f\"bleu4 :{bleu4}\")\n",
    "    print(f\"bleu4_token :{bleu4_token}\")\n",
    "    print(f\"cider_d_score :{cider_d_score}\")\n",
    "    print(f\"spice_score :{spice_score}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
