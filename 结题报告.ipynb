{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NNDL课设报告\n",
    "小组成员及对应工作量：苏柏侨(0.3)    李恒屹(0.4)    石基宽(0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 任务说明\n",
    "服饰图像描述,训练一个模型,对输入的服饰图片,输出描述信息，我们实现的模型有以下三个实现：\n",
    "- ARCTIC，一个典型的基于注意力的编解码模型\n",
    "- 视觉Transformer (ViT) + Transformer解码器\n",
    "- 网格/区域表示、Transformer编码器+Transformer解码器\n",
    "  \n",
    "同时也实现三种测评方法进行测评：\n",
    "- BLEU (Bilingual Evaluation Understudy)\n",
    "- SPICE (Semantic Propositional Image Caption Evaluation): \n",
    "- CIDEr-D (Consensus-based Image Description Evaluation)\n",
    "\n",
    "以及实现了附加任务：\n",
    "- 利用训练的服饰图像描述模型和多模态大语言模型，为真实背景的服饰图像数据集增加服饰描述和背景描述，构建全新的服饰图像描述数据集\n",
    "  - 在新数据集上重新训练服饰图像描述模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 实验数据\n",
    "数据集使用的是 DeepFashion-MultiModal (https://github.com/yumingj/DeepFashion-MultiModal), 仅用到image和textual descriptions ，数据集划分为10k+行数据的训练集和2k+行数据的测试集，`train_captions.json`和`test_captions.json`分别对应训练集和测试集的图片与描述信息的键值对应"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = f'../data/deepfashion-multimodal/images'\n",
    "def cap_to_wvec(vocab,cap):#将文本描述转换成向量\n",
    "    cap.replace(\",\",\"\")\n",
    "    cap.replace(\".\",\"\")\n",
    "    cap=cap.split()\n",
    "    res=[]\n",
    "    for word in cap:\n",
    "        if word in vocab.keys():\n",
    "            res.append(vocab[word])\n",
    "        else: #不在字典的词\n",
    "            res.append(vocab['<unk>'])\n",
    "    return res\n",
    "def wvec_to_cap(vocab,wvec):#将向量转换成文本描述\n",
    "    res=[]\n",
    "    for word in wvec:\n",
    "        for key,value in vocab.items():\n",
    "            if value==word and key not in ['<start>','<end>','<pad>','<unk>']:\n",
    "                res.append(key)\n",
    "    res=\" \".join(res)\n",
    "    return res\n",
    "def wvec_to_capls(vocab,wvec):#将向量转换成文本描述\n",
    "    res=[]\n",
    "    for word in wvec:\n",
    "        for key,value in vocab.items():\n",
    "            if value==word and key not in ['<start>','<end>','<pad>','<unk>']:\n",
    "                res.append(key)\n",
    "    return res\n",
    "class ImageTextDataset(Dataset):\n",
    "    def __init__(self, dataset_path, vocab_path, split, captions_per_image=1, max_len=93, transform=None):\n",
    "\n",
    "        self.split = split\n",
    "        assert self.split in {'train', 'test'}\n",
    "        self.cpi = captions_per_image\n",
    "        self.max_len = max_len\n",
    "\n",
    "        # 载入数据集\n",
    "        with open(dataset_path, 'r') as f:\n",
    "            self.data = json.load(f) #key是图片名字 value是描述\n",
    "            self.data_img=list(self.data.keys())\n",
    "        # 载入词典\n",
    "        with open(vocab_path, 'r') as f:\n",
    "            self.vocab = json.load(f)\n",
    "\n",
    "        # PyTorch图像预处理流程\n",
    "        self.transform = transform\n",
    "\n",
    "        # Total number of datapoints\n",
    "        self.dataset_size = len(self.data_img)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        # 第i个文本描述对应第(i // captions_per_image)张图片\n",
    "        img = Image.open(img_path+\"/\"+self.data_img[i]).convert('RGB')\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        c_vec=cap_to_wvec(self.vocab,self.data[self.data_img[i]])\n",
    "        #加入起始和结束标志\n",
    "        c_vec = [self.vocab['<start>']] + c_vec + [self.vocab['<end>']]\n",
    "        caplen = len(c_vec)\n",
    "        caption = torch.LongTensor(c_vec+ [self.vocab['<pad>']] * (self.max_len + 2 - caplen))\n",
    "        \n",
    "        return img, caption, caplen\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.dataset_size\n",
    "def mktrainval(data_dir, vocab_path, batch_size, workers=1):\n",
    "    train_tx = transforms.Compose([\n",
    "        transforms.Resize(256), # 重置图像分辨率\n",
    "        transforms.RandomCrop(224), # 随机裁剪\n",
    "        transforms.ToTensor(), # 转换成Tensor\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # 标准化--三个参数为三个通道的均值和标准差\n",
    "    ])\n",
    "    val_tx = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    train_set = ImageTextDataset(os.path.join(data_dir, 'train_captions.json'), vocab_path, 'train',  transform=train_tx)\n",
    "    test_set = ImageTextDataset(os.path.join(data_dir, 'test_captions.json'), vocab_path, 'test', transform=val_tx)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_set, batch_size=batch_size, shuffle=True, num_workers=workers, pin_memory=True)\n",
    "    \n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        test_set, batch_size=batch_size, shuffle=False, num_workers=workers, pin_memory=True, drop_last=False)\n",
    "\n",
    "    return train_loader, test_loader    \n",
    "train_loader,test_loader=mktrainval(data_dir='../data/deepfashion-multimodal',\\\n",
    "                                        vocab_path='../data/deepfashion-multimodal/vocab.json',\\\n",
    "                                        batch_size=3,workers=0) \n",
    "#workers=0 是因为ipynb不支持多线程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 关于字典的处理\n",
    "    我们采用了一个非常传统的方式，加载所有的训练集和测试集，进行词频统计，我们默认阈值为5 到达阈值的词将会加入到词典中\n",
    "\n",
    "    之后我们额外添加了 < pad > < start > < end > < unk >四个词，分别代表填充词，句首标记，句尾标记，未知词\n",
    "    \n",
    "    最终写入到vocab.json文件中\n",
    "- 关于数据集类的处理\n",
    "我们使用Pytroch的Dataset来构建数据集类，在此之外封装了返回测试集和训练集的函数，可以进行自定义的批量预处理，我们在训练和推理过程中进行了如下的处理\n",
    "    - resize 图像大小为256*256\n",
    "    - 随机裁剪 为224*224\n",
    "    - 转换为Torch Tensor\n",
    "    - normalize 归一化为\n",
    "        - mean=[0.485, 0.456, 0.406]\n",
    "        - std=[0.229, 0.224, 0.225]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 实验环境\n",
    "- Python  3.9.16\n",
    "- 主要依赖库\n",
    "  - torch \n",
    "  - torchvision\n",
    "  - nltk \n",
    "实际使用py库情况如下\n",
    "\n",
    "项目开发中除数据集和模型外代码使用git进行版本控制，需要说明的是，由于课设由多个人多个设备下完成，而训练模型和数据集有所不同，我们维持同步的仅仅是Image2TextEvaluation这一项目，所以不同设备下由细微不同之处，如此测试，需要更换实际环境替换对应代码中的路径，直接进行测试可能会出现一些问题————这里提前声明这一点————文件路径大致如下：\n",
    "```\n",
    "├── Image2TextEvaluation  \n",
    "│   ├── ARCTIC\n",
    "│   │   ├── ARCTIC_dataloader.py\n",
    "│   │   ├── ARCTIC_model.py\n",
    "│   │   └── train.py\n",
    "│   ├── Vit\n",
    "│   │   ├── Vit.ipynb\n",
    "│   │   ├── config.json\n",
    "│   │   ├── generate.ipynb\n",
    "│   ├── SwinTrans\n",
    "│   │   ├── ...//同上\n",
    "│   │   ├── evaluate.ipynb\n",
    "│   │   └── gridSwinTrans.ipynb\n",
    "│   │\n",
    "│   ├── Tools\n",
    "│   │   ├── test_blip.py\n",
    "│   │   ├── ...\n",
    "│   │\n",
    "│   ├── new_dataset //Blip+多模态构建新数据集\n",
    "│   │   ├── QianFan-agent.py \n",
    "│   │   ├── combined_input.json\n",
    "│   │   ├── statement.txt\n",
    "│   │   ├── merge_json.py\n",
    "│   │   ├── ...\n",
    "│   │\n",
    "│   ├── evaluate.py\n",
    "│   ├── README.md\n",
    "│   └── 结题报告.ipynb\n",
    "├── model\n",
    "│   ├── best_arctic.ckpt\n",
    "│   ├── last_arctic.ckpt\n",
    "│   └── ...\n",
    "├── data\n",
    "│   ├── deepfashion-multimodal\n",
    "│   │   ├── images\n",
    "│   │   │   ├── 001.jpg\n",
    "│   │   │   ├── ...\n",
    "│   │   ├── train_captions.json\n",
    "│   │   ├── test_captions.json\n",
    "│   │   ├── vocab.json\n",
    "│   │   └── ...\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence # 压紧填充序列\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import ResNet101_Weights\n",
    "from nltk.translate.bleu_score import corpus_bleu # BLEU评价指标\n",
    "import numpy as np\n",
    "import json\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "from PIL import Image\n",
    "from collections import Counter,defaultdict\n",
    "from argparse import Namespace \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 所用的方法或模型\n",
    "\n",
    "## 评估方法\n",
    "\n",
    "### BLEU (BiLingual Evaluation Understudy)\n",
    "- BLUE是比较常用的评估指标之一，也是我们默认指标，需要注意的是，再调用计算BLEU值之前，要先将文本中人工添加的文本开始符、结束符和占位符去掉，其公式如下， 实际代码中我们借助nltk库进行实现\n",
    "$$BLEU = \\sum_{n=1}^k w_n \\frac{ngram_{sys}(n)}{ngram_{ref}(n)}$$\n",
    "其中：\n",
    "  - n 是 n-gram 的阶数，取值范围为 1 到 4。\n",
    "  - wn 是 n-gram 的权重，通常取均匀权重。\n",
    "  - ngramsys(n) 是机器翻译结果中的 n-gram 数量。\n",
    "  - ngramref(n) 是参考翻译中的 n-gram 数量。\n",
    "  BLEU 的得分范围为 0 到 1。得分越高，表示机器翻译结果与参考翻译越相似。\n",
    "  - 优点：容易计算\n",
    "  - 缺点:\n",
    "    - 没有考虑n-gram的顺序\n",
    "    - 平等对待所有的n-gram\n",
    "    - 衡量的是句子之间的流畅性而非语义相似度\n",
    "### CIDEr-D (Consensus-based Image Description Evaluation)\n",
    "- 是CIDEr的改进，对于动词原形和名词匹配成功的问题，CIDEr-D不再取词根\n",
    "其用了一种折扣函数来降低长句子对评分的影响，增加了惩罚生成句子和参考句子的长度差别的权重，并且通过对n-gram计数的截断操作不再计算生成句子中出现次数超过参考句子的n-gram,\n",
    "从而减少了重复单词对评分的影响，其实也是计算1到4 gram的结果的平均值，其公式如下\n",
    "$$C I D E r - D _ { n } ( c _ { i } , S _ { i } ) = \\frac { 1 0 } { m } \\sum _ { j } e ^ { - \\frac { -( i ( c _ { i } ) - l ( s _ { i j } ) ) ^ { 2 } } { 2 \\sigma ^ { 2 } } } \\times \\frac { \\min ( g ^ { n } ( c _ { i } ) , g ^ { n } ( s _ { i j } ) ) \\cdot g ^ { n } ( s _ { i j } ) } {| | g ^ { n } ( c _ { i } ) | | | g ^ { n } ( s _ { i j } ) || } $$\n",
    "- 优点：\n",
    "  - CIDEr引入了TF-IDF为n-gram进行加权，这样就避免评价候选句子时因为一些常见却不够有信息量的n-gram打上高分\n",
    "- 缺点：\n",
    "  - CIDEr取词根的操作会让一些动词的原型和名词匹配成功\n",
    "  - 高置信度的词重复出现的长句的CIDEr得分也很高\n",
    "### SPICE (Semantic Propositional Image Caption Evaluation): \n",
    "- 是以名词为中心的度量，是以图的语义表示来编码图像描述中的对象、属性和关系\n",
    "首先要将候选句子和参考句子集转化为场景图\n",
    "然后比较候选句子和参考句子集中元组的precision、recall，最终计算出F1 score\n",
    "公式如下\n",
    "$$SPICE = \\sum_{i=1}^m \\frac{1}{|S_i|} \\sum_{j=1}^n \\frac{s_{ij}}{|R_i|}\n",
    "$$\n",
    "  - m 是图像描述的数量。\n",
    "  - n 是图像描述中的对象、属性和关系的数量。\n",
    "  - Si 是图像描述 i 中的对象、属性和关系。\n",
    "  - Ri 是参考图像描述 i 中的对象、属性和关系。\n",
    "  - sij 是图像描述 i 中的对象、属性和关系 j 与参考图像描述 i 中的对象、属性和关系 j 的相似度\n",
    "- 优点：\n",
    "  - 在语义而非n-gram层级度量\n",
    "  - 每个句子映射到场景图后可以从中提取出模型关于某些关系或者属性的识别能力\n",
    "- 缺点\n",
    "  - 缺少n-gram来度量句子的流畅性\n",
    "  - 度量的准确性受到场景图解析器的制约\n",
    "\n",
    "使用代码如下，在evaluate的时候调用,接受cands, refs返回对应评估分数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cider_d(reference_list, candidate_list, n=4):\n",
    "    def count_ngrams(tokens, n):\n",
    "        ngrams = []\n",
    "        for i in range(len(tokens) - n + 1):\n",
    "            ngram = tuple(tokens[i:i+n])\n",
    "            ngrams.append(ngram)\n",
    "        return ngrams\n",
    "\n",
    "    def compute_cider_d(reference_list, candidate_list, n):\n",
    "        cider_d_scores = []\n",
    "        for refs, cand in zip(reference_list, candidate_list):\n",
    "            cider_d_score = 0.0\n",
    "            for i in range(1, n + 1):\n",
    "                cand_ngrams = count_ngrams(cand, i)\n",
    "                ref_ngrams_list = [count_ngrams(ref, i) for ref in refs]\n",
    "\n",
    "                total_ref_ngrams = [ngram for ref_ngrams in ref_ngrams_list for ngram in ref_ngrams]\n",
    "\n",
    "                count_cand = 0\n",
    "                count_clip = 0\n",
    "\n",
    "                for ngram in cand_ngrams:\n",
    "                    count_cand += 1\n",
    "                    if ngram in total_ref_ngrams:\n",
    "                        count_clip += 1\n",
    "\n",
    "                precision = count_clip / count_cand if count_cand > 0 else 0.0\n",
    "                recall = count_clip / len(total_ref_ngrams) if len(total_ref_ngrams) > 0 else 0.0\n",
    "\n",
    "                beta = 1.0\n",
    "                f_score = (1 + beta**2) * precision * recall / (beta**2 * precision + recall) if precision + recall > 0 else 0.0\n",
    "\n",
    "                cider_d_score += f_score\n",
    "\n",
    "            cider_d_score /= n\n",
    "            cider_d_scores.append(cider_d_score)\n",
    "\n",
    "        return cider_d_scores\n",
    "\n",
    "    reference_tokens_list = reference_list\n",
    "    candidate_tokens_list = candidate_list\n",
    "\n",
    "    scores = compute_cider_d(reference_tokens_list, candidate_tokens_list, n)\n",
    "\n",
    "    return np.mean(scores)\n",
    "def spice(reference_list, candidate_list, idf=None, beta=3):\n",
    "    def count_ngrams(tokens, n):\n",
    "        ngrams = []\n",
    "        for i in range(len(tokens) - n + 1):\n",
    "            ngram = tuple(tokens[i:i+n])\n",
    "            ngrams.append(ngram)\n",
    "        return ngrams\n",
    "\n",
    "    def compute_spice_score(reference, candidate, idf, beta):\n",
    "        reference_tokens = reference\n",
    "        candidate_tokens = candidate\n",
    "\n",
    "        reference_ngrams = [count_ngrams(reference_tokens, i) for i in range(1, beta + 1)]\n",
    "        candidate_ngrams = [count_ngrams(candidate_tokens, i) for i in range(1, beta + 1)]\n",
    "\n",
    "        precision_scores = []\n",
    "        recall_scores = []\n",
    "\n",
    "        for i in range(beta):\n",
    "            common_ngrams = set(candidate_ngrams[i]) & set(reference_ngrams[i])\n",
    "\n",
    "            precision = len(common_ngrams) / len(candidate_ngrams[i]) if len(candidate_ngrams[i]) > 0 else 0.0\n",
    "            recall = len(common_ngrams) / len(reference_ngrams[i]) if len(reference_ngrams[i]) > 0 else 0.0\n",
    "\n",
    "            precision_scores.append(precision)\n",
    "            recall_scores.append(recall)\n",
    "\n",
    "        precision_avg = np.mean(precision_scores)\n",
    "        recall_avg = np.mean(recall_scores)\n",
    "\n",
    "        spice_score = (precision_avg * recall_avg) / (precision_avg + recall_avg) if precision_avg + recall_avg > 0 else 0.0\n",
    "\n",
    "        if idf:\n",
    "            spice_score *= np.exp(np.sum([idf[token] for token in common_ngrams]) / len(candidate_tokens))\n",
    "\n",
    "        return spice_score\n",
    "\n",
    "    if idf is None:\n",
    "        idf = {}\n",
    "\n",
    "    spice_scores = []\n",
    "\n",
    "    for reference, candidate in zip(reference_list, candidate_list):\n",
    "        spice_score = compute_spice_score(reference, candidate, idf, beta)\n",
    "        spice_scores.append(spice_score)\n",
    "\n",
    "    return np.mean(spice_scores)\n",
    "def get_BLEU_score(cands, refs): #获取BLEU分数\n",
    "    multiple_refs = []\n",
    "    for idx in range(len(refs)):\n",
    "        multiple_refs.append(refs[(idx//1)*1 : (idx//1)*1+1])#每个候选文本对应cpi==1条参考文本\n",
    "    bleu4 = corpus_bleu(multiple_refs, cands, weights=(0.25,0.25,0.25,0.25))\n",
    "    return bleu4\n",
    "def get_CIDER_D_score(cands, refs): #获得CIDER-D分数\n",
    "    refs_ = [wvec_to_capls(model.vocab,ref) for ref in refs]\n",
    "    cands_ = [wvec_to_capls(model.vocab,cand) for cand in cands]\n",
    "    return cider_d(refs_, cands_)\n",
    "def get_SPICE_score(cands, refs): #获得SPICE分数\n",
    "    refs_ = [wvec_to_cap(model.vocab,ref) for ref in refs]\n",
    "    cands_ = [wvec_to_cap(model.vocab,cand) for cand in cands]\n",
    "    return spice(refs_, cands_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型定义\n",
    "### ARCTIC，一个典型的基于注意力的编解码模型\n",
    "\n",
    "模型架构: ARCTIC 是一个基于注意力的编码-解码模型，使用了图像编码器和注意力解码器。\n",
    "#### 编码器部分\n",
    "图像编码器使用了 ResNet-101 网络进行特征提取。\n",
    "\n",
    "我们使用了预训练模型的权重，所以后续的训练中实际上编码器不参与训练过程，其参数是冻结的\n",
    "\n",
    "\n",
    "\n",
    "我们并将其最后一个非全连接层作为网格表示提取层。\n",
    "#### 解码器部分\n",
    "解码器采用 GRU，利用注意力上下文向量和当前时刻的词嵌入来生成预测结果。该模型支持束搜索来生成更准确的描述。\n",
    "\n",
    "解码器实质是一个rnn，其是有一层加性注意力机制，它接受查询（Q）和键值对（K，V），计算注意力分数，最后输出上下文向量。\n",
    "\n",
    "- **加性注意力机制**：我们在AdditiveAttention 类上实现了加性注意力机制，用于在解码过程中关注图像中不同部分的信息。\n",
    "\n",
    "- **使用GRU**: rnn具体使用的是 GRU ：是一种门控循环单元（gated recurrent unit）的缩写，它是一种用于处理序列数据的循环神经网络（RNN）\n",
    "- \n",
    "    我个人认为选择GRU 而不是 LSTM 是因为相比LSTM，使用GRU能够达到相当的效果，并且相比之下更容易进行训练，能够很大程度上提高训练效率，可以很出现可观的效果。\n",
    "\n",
    "- **forward过程：** 在单步forward过程中，我们将上下文向量和当前时刻的词表示拼接，然后和我们的rnn贴合在一起\n",
    "\n",
    "    作为 GRU 的输入，进入全连接层对GRU 输出进行线性变换，得到单步forward；而实际生成句子的时候就是重复这个过程，选择概率最大的词作为下一个词，直到遇到结束符或者到达最大的长度\n",
    "\n",
    "    当然，这种推理方法不是最好的，实质上这是一种 贪心算法\n",
    "    我们知道，**贪心算法的缺点就是它无法保证全局最优**，我们要的是所有预测的的概率相乘最大。\n",
    "\n",
    "- **使用束搜索**：所以我们还实现了另一种方法，即使用束搜索来生成更准确的描述。\n",
    "\n",
    "    束搜索就是在每一步的时候，计算到这一步为止的预测y序列的概率最大的前k条，k叫集束宽。\n",
    "\n",
    "    当然束搜索的缺点也很明显，推理所需时间实际上是倍增的，但是效果上是有增益的。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ARCTIC_config = Namespace(\n",
    "        max_len = 93,\n",
    "        captions_per_image = 1,\n",
    "        batch_size = 32,\n",
    "        image_code_dim = 2048,\n",
    "        word_dim = 512,\n",
    "        hidden_size = 512,\n",
    "        attention_dim = 512,\n",
    "        num_layers = 1,\n",
    "        encoder_learning_rate = 0.0001,\n",
    "        decoder_learning_rate = 0.0005,\n",
    "        num_epochs = 10,\n",
    "        grad_clip = 5.0,\n",
    "        alpha_weight = 1.0,\n",
    "        evaluate_step = 900, # 每隔多少步在验证集上测试一次\n",
    "        checkpoint = None, # 如果不为None，则利用该变量路径的模型继续训练\n",
    "        best_checkpoint = 'model/ARCTIC/best_ARCTIC.ckpt', # 验证集上表现最优的模型的路径\n",
    "        last_checkpoint = 'model/ARCTIC/last_ARCTIC.ckpt', # 训练完成时的模型的路径\n",
    "        beam_k = 5 #束搜索的束宽\n",
    "    )\n",
    "class ImageEncoder(nn.Module):\n",
    "    def __init__(self, finetuned=True):\n",
    "        super(ImageEncoder, self).__init__()\n",
    "        model = torchvision.models.resnet101(weights=ResNet101_Weights.DEFAULT)\n",
    "        # ResNet-101网格表示提取器\n",
    "        self.grid_rep_extractor = nn.Sequential(*(list(model.children())[:-2])) #去掉最后两层 \n",
    "        for param in self.grid_rep_extractor.parameters(): #冻结参数--不参与训练\n",
    "            param.requires_grad = finetuned #是否微调\n",
    "    def forward(self, images):\n",
    "        out = self.grid_rep_extractor(images) \n",
    "        return out\n",
    "class AdditiveAttention(nn.Module): #加性注意力\n",
    "    def  __init__(self, query_dim, key_dim, attn_dim):\n",
    "        \"\"\"\n",
    "            query_dim: 查询Q的维度\n",
    "            key_dim: 键K的维度\n",
    "            attn_dim: 注意力函数隐藏层表示的维度\n",
    "        \"\"\"\n",
    "        \n",
    "        super(AdditiveAttention, self).__init__()\n",
    "        self.attn_w_1_q = nn.Linear(query_dim, attn_dim) #Q的线性变换\n",
    "        self.attn_w_1_k = nn.Linear(key_dim, attn_dim) #K的线性变换\n",
    "        self.attn_w_2 = nn.Linear(attn_dim, 1) #注意力函数隐藏层到输出层的线性变换\n",
    "        self.tanh = nn.Tanh() #激活函数\n",
    "        self.softmax = nn.Softmax(dim=1) #归一化函数\n",
    "\n",
    "    def forward(self, query, key_value):\n",
    "        \"\"\"\n",
    "        Q K V：Q和K算出相关性得分，作为V的权重，K=V\n",
    "        参数：\n",
    "            query: 查询 (batch_size, q_dim)\n",
    "            key_value: 键和值，(batch_size, n_kv, kv_dim)\n",
    "        \"\"\"\n",
    "        # （2）计算query和key的相关性，实现注意力评分函数\n",
    "        # -> (batch_size, 1, attn_dim)\n",
    "        queries = self.attn_w_1_q(query).unsqueeze(1) \n",
    "        # -> (batch_size, n_kv, attn_dim)\n",
    "        keys = self.attn_w_1_k(key_value) #\n",
    "        # -> (batch_size, n_kv)\n",
    "        attn = self.attn_w_2(self.tanh(queries+keys)).squeeze(2)  #注意力评分函数\n",
    "        # （3）归一化相关性分数\n",
    "        # -> (batch_size, n_kv)\n",
    "        attn = self.softmax(attn)  #归一化\n",
    "        # （4）计算输出\n",
    "        # (batch_size x 1 x n_kv)(batch_size x n_kv x kv_dim)\n",
    "        # -> (batch_size, 1, kv_dim)\n",
    "        output = torch.bmm(attn.unsqueeze(1), key_value).squeeze(1)\n",
    "        return output, attn\n",
    "class AttentionDecoder(nn.Module):\n",
    "    def __init__(self, image_code_dim, vocab_size, word_dim, attention_dim, hidden_size, num_layers, dropout=0.5):\n",
    "        super(AttentionDecoder, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, word_dim) #词嵌入 \n",
    "        self.attention = AdditiveAttention(hidden_size, image_code_dim, attention_dim) #注意力机制\n",
    "        self.init_state = nn.Linear(image_code_dim, num_layers*hidden_size) #初始化隐状态\n",
    "        self.rnn = nn.GRU(word_dim + image_code_dim, hidden_size, num_layers) #GRU 门控循环\n",
    "        self.dropout = nn.Dropout(p=dropout) #dropout\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size) #全连接层\n",
    "        # RNN默认已初始化\n",
    "        self.init_weights() #初始化权重\n",
    "        \n",
    "    def init_weights(self): #初始化权重\n",
    "        self.embed.weight.data.uniform_(-0.1, 0.1) #词嵌入\n",
    "        self.fc.bias.data.fill_(0) #全连接层\n",
    "        self.fc.weight.data.uniform_(-0.1, 0.1) #全连接层\n",
    "    \n",
    "    def init_hidden_state(self, image_code, captions, cap_lens):\n",
    "        \"\"\"\n",
    "        参数：\n",
    "            image_code：图像编码器输出的图像表示 \n",
    "                        (batch_size, image_code_dim, grid_height, grid_width)\n",
    "        \"\"\"\n",
    "        # 将图像网格表示转换为序列表示形式 \n",
    "        batch_size, image_code_dim = image_code.size(0), image_code.size(1)\n",
    "        # -> (batch_size, grid_height, grid_width, image_code_dim) \n",
    "        image_code = image_code.permute(0, 2, 3, 1)  \n",
    "        # -> (batch_size, grid_height * grid_width, image_code_dim)\n",
    "        image_code = image_code.view(batch_size, -1, image_code_dim)\n",
    "        # （1）按照caption的长短排序\n",
    "        sorted_cap_lens, sorted_cap_indices = torch.sort(cap_lens, 0, True)\n",
    "        captions = captions[sorted_cap_indices]\n",
    "        image_code = image_code[sorted_cap_indices]\n",
    "         #（2）初始化隐状态\n",
    "        hidden_state = self.init_state(image_code.mean(axis=1))\n",
    "        hidden_state = hidden_state.view(\n",
    "                            batch_size, \n",
    "                            self.rnn.num_layers, \n",
    "                            self.rnn.hidden_size).permute(1, 0, 2)\n",
    "        return image_code, captions, sorted_cap_lens, sorted_cap_indices, hidden_state\n",
    "\n",
    "    def forward_step(self, image_code, curr_cap_embed, hidden_state):\n",
    "        #（3.2）利用注意力机制获得上下文向量\n",
    "        # query：hidden_state[-1]，即最后一个隐藏层输出 (batch_size, hidden_size)\n",
    "        # context: (batch_size, hidden_size)\n",
    "        context, alpha = self.attention(hidden_state[-1], image_code)\n",
    "        #（3.3）以上下文向量和当前时刻词表示为输入，获得GRU输出\n",
    "        x = torch.cat((context, curr_cap_embed), dim=-1).unsqueeze(0)\n",
    "        # x: (1, real_batch_size, hidden_size+word_dim)\n",
    "        # out: (1, real_batch_size, hidden_size)\n",
    "        out, hidden_state = self.rnn(x, hidden_state)\n",
    "        #（3.4）获取该时刻的预测结果\n",
    "        # (real_batch_size, vocab_size)\n",
    "        preds = self.fc(self.dropout(out.squeeze(0)))\n",
    "        return preds, alpha, hidden_state\n",
    "        \n",
    "    def forward(self, image_code, captions, cap_lens):\n",
    "        \"\"\"\n",
    "        参数：\n",
    "            hidden_state: (num_layers, batch_size, hidden_size)\n",
    "            image_code:  (batch_size, feature_channel, feature_size)\n",
    "            captions: (batch_size, )\n",
    "        \"\"\"\n",
    "        # （1）将图文数据按照文本的实际长度从长到短排序\n",
    "        # （2）获得GRU的初始隐状态\n",
    "        image_code, captions, sorted_cap_lens, sorted_cap_indices, hidden_state \\\n",
    "            = self.init_hidden_state(image_code, captions, cap_lens)\n",
    "        batch_size = image_code.size(0)\n",
    "        # 输入序列长度减1，因为最后一个时刻不需要预测下一个词\n",
    "        lengths = sorted_cap_lens.cpu().numpy() - 1\n",
    "        # 初始化变量：模型的预测结果和注意力分数\n",
    "        predictions = torch.zeros(batch_size, lengths[0], self.fc.out_features).to(captions.device)\n",
    "        alphas = torch.zeros(batch_size, lengths[0], image_code.shape[1]).to(captions.device)\n",
    "        # 获取文本嵌入表示 cap_embeds: (batch_size, num_steps, word_dim)\n",
    "        cap_embeds = self.embed(captions)\n",
    "        # Teacher-Forcing模式\n",
    "        for step in range(lengths[0]):\n",
    "            #（3）解码\n",
    "            #（3.1）模拟pack_padded_sequence函数的原理，获取该时刻的非<pad>输入\n",
    "            real_batch_size = np.where(lengths>step)[0].shape[0]\n",
    "            preds, alpha, hidden_state = self.forward_step(\n",
    "                            image_code[:real_batch_size], \n",
    "                            cap_embeds[:real_batch_size, step, :],\n",
    "                            hidden_state[:, :real_batch_size, :].contiguous())            \n",
    "            # 记录结果\n",
    "            predictions[:real_batch_size, step, :] = preds\n",
    "            alphas[:real_batch_size, step, :] = alpha\n",
    "        return predictions, alphas, captions, lengths, sorted_cap_indices\n",
    "    \n",
    "class ARCTIC(nn.Module): #模型主体部分\n",
    "    def __init__(self, image_code_dim, vocab, word_dim, attention_dim, hidden_size, num_layers):\n",
    "        super(ARCTIC, self).__init__()\n",
    "        self.vocab = vocab\n",
    "        self.encoder = ImageEncoder()\n",
    "        self.decoder = AttentionDecoder(image_code_dim, len(vocab),\n",
    "                                        word_dim, attention_dim, hidden_size, num_layers)\n",
    "        print(\"test\")\n",
    "    def forward(self, images, captions, cap_lens):\n",
    "        image_code = self.encoder(images)\n",
    "        return self.decoder(image_code, captions, cap_lens)\n",
    "    def generate_by_beamsearch(self, images, beam_k, max_len): # beam_k束搜索\n",
    "        vocab_size = len(self.vocab)\n",
    "        image_codes = self.encoder(images)\n",
    "        texts = []\n",
    "        device = images.device\n",
    "        # 对每个图像样本执行束搜索\n",
    "        for image_code in image_codes:\n",
    "            # 将图像表示复制k份\n",
    "            image_code = image_code.unsqueeze(0).repeat(beam_k,1,1,1)\n",
    "            # 生成k个候选句子，初始时，仅包含开始符号<start>\n",
    "            cur_sents = torch.full((beam_k, 1), self.vocab['<start>'], dtype=torch.long).to(device)\n",
    "            cur_sent_embed = self.decoder.embed(cur_sents)[:,0,:]\n",
    "            sent_lens = torch.LongTensor([1]*beam_k).to(device)\n",
    "            # 获得GRU的初始隐状态\n",
    "            image_code, cur_sent_embed, _, _, hidden_state = \\\n",
    "                self.decoder.init_hidden_state(image_code, cur_sent_embed, sent_lens)\n",
    "            # 存储已生成完整的句子（以句子结束符<end>结尾的句子）\n",
    "            end_sents = []\n",
    "            # 存储已生成完整的句子的概率\n",
    "            end_probs = []\n",
    "            # 存储未完整生成的句子的概率\n",
    "            probs = torch.zeros(beam_k, 1).to(device)\n",
    "            k = beam_k\n",
    "            while True:\n",
    "                preds, _, hidden_state = self.decoder.forward_step(image_code[:k], cur_sent_embed, hidden_state.contiguous())\n",
    "                # -> (k, vocab_size)\n",
    "                preds = nn.functional.log_softmax(preds, dim=1)\n",
    "                # 对每个候选句子采样概率值最大的前k个单词生成k个新的候选句子，并计算概率\n",
    "                # -> (k, vocab_size)\n",
    "                probs = probs.repeat(1,preds.size(1)) + preds\n",
    "                if cur_sents.size(1) == 1:\n",
    "                    # 第一步时，所有句子都只包含开始标识符，因此，仅利用其中一个句子计算topk\n",
    "                    values, indices = probs[0].topk(k, 0, True, True)\n",
    "                else:\n",
    "                    # probs: (k, vocab_size) 是二维张量\n",
    "                    # topk函数直接应用于二维张量会按照指定维度取最大值，这里需要在全局取最大值\n",
    "                    # 因此，将probs转换为一维张量，再使用topk函数获取最大的k个值\n",
    "                    values, indices = probs.view(-1).topk(k, 0, True, True)\n",
    "                # 计算最大的k个值对应的句子索引和词索引\n",
    "                sent_indices = torch.div(indices, vocab_size, rounding_mode='trunc') \n",
    "                word_indices = indices % vocab_size \n",
    "                # 将词拼接在前一轮的句子后，获得此轮的句子\n",
    "                cur_sents = torch.cat([cur_sents[sent_indices], word_indices.unsqueeze(1)], dim=1)\n",
    "                # 查找此轮生成句子结束符<end>的句子\n",
    "                end_indices = [idx for idx, word in enumerate(word_indices) if word == self.vocab['<end>']]\n",
    "                if len(end_indices) > 0:\n",
    "                    end_probs.extend(values[end_indices])\n",
    "                    end_sents.extend(cur_sents[end_indices].tolist())\n",
    "                    # 如果所有的句子都包含结束符，则停止生成\n",
    "                    k -= len(end_indices)\n",
    "                    if k == 0:\n",
    "                        break\n",
    "                # 查找还需要继续生成词的句子\n",
    "                cur_indices = [idx for idx, word in enumerate(word_indices) \n",
    "                               if word != self.vocab['<end>']]\n",
    "                if len(cur_indices) > 0:\n",
    "                    cur_sent_indices = sent_indices[cur_indices]\n",
    "                    cur_word_indices = word_indices[cur_indices]\n",
    "                    # 仅保留还需要继续生成的句子、句子概率、隐状态、词嵌入\n",
    "                    cur_sents = cur_sents[cur_indices]\n",
    "                    probs = values[cur_indices].view(-1,1)\n",
    "                    hidden_state = hidden_state[:,cur_sent_indices,:]\n",
    "                    cur_sent_embed = self.decoder.embed(\n",
    "                        cur_word_indices.view(-1,1))[:,0,:]\n",
    "                # 句子太长，停止生成\n",
    "                if cur_sents.size(1) >= max_len:\n",
    "                    break\n",
    "            if len(end_sents) == 0:\n",
    "                # 如果没有包含结束符的句子，则选取第一个句子作为生成句子\n",
    "                gen_sent = cur_sents[0].tolist()\n",
    "            else: \n",
    "                # 否则选取包含结束符的句子中概率最大的句子\n",
    "                gen_sent = end_sents[end_probs.index(max(end_probs))]\n",
    "            texts.append(gen_sent)\n",
    "        return texts\n",
    "mode_arctic=\"../best_arctic.ckpt\"\n",
    "model=torch.load(mode_arctic)[\"model\"] #加载模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 视觉Transformer (ViT) + Transformer解码器\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### ViT部分：\n",
    "ViT是一种用于处理视觉数据的转换器架构。传统的卷积神经网络（CNN）在图像处理任务中表现出色，但ViT引入了自注意力机制，使得它可以处理可变大小的图像。ViT将输入图片分为多个patch（16x16），再将每个patch投影为固定长度的向量送入Transformer，后续encoder的操作和原始Transformer中完全相同。但是因为对图片分类，因此在输入序列中加入一个特殊的token，该token对应的输出即为最后的类别预测\n",
    "一个ViT block可以分为以下几个步骤\n",
    "\n",
    "- **patch embedding**\n",
    "  例如输入图片大小为224x224，将图片分为固定大小的patch，patch大小为16x16，则每张图像会生成224x224/16x16=196个patch，即输入序列长度为196，每个patch维度16x16x3=768，线性投射层的维度为768xN (N=768)，因此输入通过线性投射层之后的维度依然为196x768，即一共有196个token，每个token的维度是768。这里还需要加上一个特殊字符cls，因此最终的维度是197x768。到目前为止，已经通过patch embedding将一个视觉问题转化为了一个seq2seq问题\n",
    "\n",
    "- **positional encoding（standard learnable 1D position embeddings）**\n",
    "  加入位置编码。加入位置编码信息之后，维度依然是197x768。位置编码的方式有**1-D位置编码**和**2-D 位置编码**，无论是哪种方式精度都很接近，甚至不适用位置编码性能损失也没有大到无法接受的地步。猜测因为ViT作用在image patch上，对网络来说这些patch之间的相对位置信息很容易理解，所以位置编码的方式与是否采用的影响都不大\n",
    "\n",
    "- **LN/multi-head attention/LN**\n",
    "  \n",
    "  LN输出维度依然是197x768。将输入映射到q，k，v从而进行MHA操作，如果只有一个head，qkv的维度都是197x768，如果有12个头（768/12=64），则qkv的维度是197x64，一共有12组qkv，最后再将12组qkv的输出拼接起来，输出维度是197x768，然后在过一层LN，维度依然是197x768\n",
    "\n",
    "- **MLP**\n",
    "  \n",
    "  将维度放大再缩小回去，197x768放大为197x3072，再缩小变为197x768一个block之后维度依然和输入相同，都是197x768，因此可以堆叠多个block。最后会将特殊字符cls对应的输出 作为encoder的最终输出 ，代表最终的image presentation（另一种做法是不加cls字符，对所有的tokens的输出做一个平均），后面接一个MLP进行图片分类\n",
    "\n",
    "### Transformer解码器：\n",
    "我们首先采用BERT来对提取的特征进行编码，再用Transformer解码器对编码内容进行处理后输出。要注意的是，这里我们仅采用了BERT的结构，并没有调用预训练模型。\n",
    "BERT（Bidirectional Encoder Representations from Transformers）是一种自然语言处理中常用的预训练模型。我们将BERT用作图像特征的编码器。通过将ViT提取的图像特征输入BERT，模型能够学习图像中的上下文信息，捕捉图像内部的复杂关系。这使得模型能够更好地理解图像内容，从而更好地生成相关的图像描述。\n",
    "- BERT的结构是标准transformer结构的encoder部分， 一个transformer的encoder单元由一个multi-head-Attention + Layer Normalization + feedforword + Layer Normalization 叠加产生，BERT的每一层由一个这样的encoder单元构成\n",
    "- 这种transformer的结构可以使用上下文来预测mask的token，从而捕捉双向关系\n",
    "\n",
    "之后，我们采用经典的Transformer解码器结构，Transformer解码器在NLP领域中被广泛用于生成序列数据，如文本。在这里，我们使用Transformer解码器来生成图像描述。解码器接收来自BERT编码器的图像特征作为输入，然后通过自注意力机制和前馈神经网络层逐步生成与图像内容相关的描述序列。这个描述序列最终形成图像的摘要或说明。\n",
    "\n",
    "### 网格/区域表示、Transformer编码器+Transformer解码器\n",
    "\n",
    "### 网格表示：\n",
    "我们借鉴了 **SwinTransformer** 的结构来做网格特征提取。SwinTransformer是一个基于注意力机制的深度学习模型，专门设计用于图像处理任务。它采用了分层的注意力机制，允许模型有效地处理大尺寸的图像。在图像摘要生成任务中，SwinTransformer作为网格特征提取器，从输入图像中提取有用的特征。\n",
    "- **Hierarchical Patch-based Attention**\n",
    "  \n",
    "  Swin Transformer引入了层次化的基于Patch的注意力机制。传统的Transformer模型是基于全连接的，对于大尺寸的图像，计算复杂度可能会非常高。为了解决这个问题，Swin Transformer将图像划分为一系列的非重叠图像块（patches），并在这些块上应用自注意力机制。这种分块处理允许Swin Transformer更好地扩展到大规模图像。\n",
    "- **Shifted Windows**\n",
    "  \n",
    "  Swin Transformer采用了一种称为\"shifted windows\"的策略，通过改变注意力机制中的窗口偏移，提高了模型的局部感知能力。这对于捕捉图像中不同区域的特征非常有帮助，尤其是在需要考虑对象之间相对位置关系时。\n",
    "- **Tokenization and Positional Embeddings**\n",
    "  \n",
    "  Swin Transformer将图像块转换为序列，每个序列元素对应一个图像块。为了使模型能够处理序列数据，需要引入类似自然语言处理中的tokenization和positional embeddings机制。这样，Swin Transformer可以对图像块进行有效的注意力计算，并理解它们之间的语义关系。\n",
    "\n",
    "- **PatchMergin**\n",
    "  \n",
    "  类似于池化操作\n",
    "\n",
    "- 借鉴了许多CNN中的trick，每经过一个stage后size就会缩小为原来的二分之一，channel扩大为两倍，与CNN相似\n",
    "  - Patch Merging 模块将 尺寸为 H×WH×W 的 Patch 块首先进行拼接并在 channel 维度上进行 concatenate 构成了 H/2×W/2×4C的特征图，然后再进行 Layer Normalization 操作进行正则化，通过一个 Linear 层后形成了一个 H/2×W/2×2C得到特征图，完成了特征图的下采样过程。size 缩小为原来的 1/2，channel 扩大为原来的 2 倍。\n",
    "- 分割成多个固定的窗口，每个窗口内的像素只能内部进行内积，虽然减少了计算开销，但是也因为这个操作，各个窗口无法进行信息交互，也即是感受野减小，难以整体获得信息，因此需要进行平移图像，改变分割方式，但这会使得计算量增加，因此重新滑动分割后的窗口，同时加入mask机制，防止不同位置的元素进行了自注意力的计算，最后再将信息数据平移回到原来的位置。通过 SW-MSA 机制完成了偏移窗口的像素点的 MSA 计算并实现了不同窗口间像素点的信息交流，从而间接扩大了网络的“感受野”，提高了信息的利用效率。\n",
    "\n",
    "\n",
    "### Transformer编码器+Transformer解码器\n",
    "\n",
    "同上，我们首先采用BERT来对提取的特征进行编码，再用Transformer解码器对编码内容进行处理后输出。要注意的是，这里我们仅采用了BERT的结构，并没有调用预训练模型。\n",
    "BERT（Bidirectional Encoder Representations from Transformers）是一种自然语言处理中常用的预训练模型。我们将BERT用作图像特征的编码器。通过将SwinTransformer提取特征信息输入BERT，模型能够学习图像中的上下文信息，捕捉图像内部的复杂关系。这使得模型能够更好地理解图像内容，从而更好地生成相关的图像描述。\n",
    "- BERT的结构是标准transformer结构的encoder部分， 一个transformer的encoder单元由一个multi-head-Attention + Layer Normalization + feedforword + Layer Normalization 叠加产生，BERT的每一层由一个这样的encoder单元构成\n",
    "- 这种transformer的结构可以使用上下文来预测mask的token，从而捕捉双向关系\n",
    "\n",
    "之后，我们采用经典的Transformer解码器结构，Transformer解码器在NLP领域中被广泛用于生成序列数据，如文本。在这里，我们使用Transformer解码器来生成图像描述。解码器接收来自BERT编码器的图像特征作为输入，然后通过自注意力机制和前馈神经网络层逐步生成与图像内容相关的描述序列。这个描述序列最终形成图像的摘要或说明。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ViT编码器+Transformer解码器\n",
    "class Img2TxtModel(nn.Module):\n",
    "    def __init__(self, vit_model_name, transformer_config, vocab_size):\n",
    "        super(Img2TxtModel, self).__init__()\n",
    "        # ViT模型作为编码器\n",
    "        self.encoder = ViTModel.from_pretrained(vit_model_name)\n",
    "\n",
    "        # Transformer解码器配置\n",
    "        transformer_config = BertConfig(vocab_size=vocab_size, num_hidden_layers=1, is_decoder=True,  add_cross_attention=True)\n",
    "        self.decoder = BertModel(transformer_config)\n",
    "\n",
    "        # 预测每个词的线性层\n",
    "        self.vocab_size = vocab_size\n",
    "        self.fc = nn.Linear(transformer_config.hidden_size, vocab_size)\n",
    "    \n",
    "    def forward(self, input_ids, decoder_input_ids, decoder_attention_mask):\n",
    "        # 通过ViT编码器获取图像特征\n",
    "        encoder_outputs = self.encoder(pixel_values=input_ids).last_hidden_state\n",
    "\n",
    "        # 将图像特征作为解码器的输入\n",
    "        decoder_outputs = self.decoder(input_ids=decoder_input_ids, \n",
    "                                       attention_mask=decoder_attention_mask,\n",
    "                                       encoder_hidden_states=encoder_outputs).last_hidden_state\n",
    "\n",
    "        # 预测下一个词\n",
    "        prediction_scores = self.fc(decoder_outputs)\n",
    "        return prediction_scores\n",
    "\n",
    "    def generate_text(self, input_ids, max_length=95, start_token_id=154):\n",
    "        # 获取图像特征\n",
    "        encoder_outputs = self.encoder(pixel_values=input_ids).last_hidden_state\n",
    "\n",
    "        # 初始化解码器输入为<start>标记\n",
    "        decoder_input_ids = torch.full((input_ids.size(0), 1), start_token_id).to(input_ids.device)\n",
    "        \n",
    "        # 存储所有时间步的logits\n",
    "        all_logits = []\n",
    "\n",
    "        for step in range(max_length):\n",
    "            # 获取解码器输出\n",
    "            decoder_outputs = self.decoder(\n",
    "                input_ids=decoder_input_ids, \n",
    "                encoder_hidden_states=encoder_outputs\n",
    "            ).last_hidden_state\n",
    "\n",
    "            # 预测下一个词\n",
    "            next_word_logits = self.fc(decoder_outputs[:, -1, :])\n",
    "            all_logits.append(next_word_logits.unsqueeze(1))\n",
    "            next_word_id = next_word_logits.argmax(dim=-1).unsqueeze(-1)\n",
    "            \n",
    "            # 将预测的词添加到解码器输入中\n",
    "            decoder_input_ids = torch.cat([decoder_input_ids, next_word_id], dim=-1)\n",
    "        \n",
    "        return decoder_input_ids ,torch.cat(all_logits, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SwinTransformerBlock ，SwinTransformer经过多个这样的block\n",
    "class SwinTransformerBlock(nn.Module):\n",
    "    def __init__(self, dim, num_heads, window_size=7, shift_size=0., mlp_ratio=4., qkv_bias=True, drop=0., attn_drop=0., drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
    "        super(SwinTransformerBlock, self).__init__()\n",
    "\n",
    "        self.dim = dim\n",
    "        self.num_heads = num_heads\n",
    "        self.window_size = window_size\n",
    "        self.shift_size = shift_size\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = self._create_attention_module(dim, num_heads, window_size, qkv_bias, attn_drop, drop)\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = MLP(in_features=dim, hidden_features=mlp_hidden_dim, act=act_layer, drop=drop)\n",
    "\n",
    "    def _create_attention_module(self, dim, num_heads, window_size, qkv_bias, attn_drop, drop):\n",
    "        return WindowAttention(\n",
    "            dim=dim, window_size=(window_size, window_size), num_heads=num_heads,\n",
    "            qkv_bias=qkv_bias, attn_drop=attn_drop, proj_drop=drop\n",
    "        )\n",
    "\n",
    "    def forward(self, x, attn_mask):\n",
    "        x, Hp, Wp = self._process_input(x)\n",
    "        shifted_x = self._shift_and_pad(x)\n",
    "        x_windows = self._prepare_windows(shifted_x)\n",
    "        attn_windows = self.attn(x_windows, mask=attn_mask)\n",
    "        shifted_x = self._reverse_windows(attn_windows, Hp, Wp)\n",
    "        x = self._restore_data(shifted_x, x.shape, Hp, Wp)\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "\n",
    "        return x\n",
    "\n",
    "    def _process_input(self, x):\n",
    "        H, W = self.H, self.W  # feature map\n",
    "        B, L, C = x.shape\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = x.view(B, H, W, C)\n",
    "        return x, H, W\n",
    "\n",
    "    def _shift_and_pad(self, x):\n",
    "        if self.shift_size > 0.:\n",
    "            shifted_x = torch.roll(x, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2))\n",
    "        else:\n",
    "            shifted_x = x\n",
    "        return shifted_x\n",
    "\n",
    "    def _prepare_windows(self, x):\n",
    "        x_windows = window_partition(x, self.window_size)\n",
    "        x_windows = x_windows.view(-1, self.window_size * self.window_size, x.shape[-1])\n",
    "        return x_windows\n",
    "\n",
    "    def _reverse_windows(self, attn_windows, Hp, Wp):\n",
    "        attn_windows = attn_windows.view(-1, self.window_size, self.window_size, attn_windows.shape[-1])\n",
    "        shifted_x = window_reverse(attn_windows, self.window_size, Hp, Wp)\n",
    "        return shifted_x\n",
    "\n",
    "    def _restore_data(self, shifted_x, original_shape, Hp, Wp):\n",
    "        if self.shift_size > 0:\n",
    "            x = torch.roll(shifted_x, shifts=(self.shift_size, self.shift_size), dims=(1, 2))\n",
    "        else:\n",
    "            x = shifted_x\n",
    "        x_r = (self.window_size - original_shape[2] % self.window_size) % self.window_size\n",
    "        x_d = (self.window_size - original_shape[1] % self.window_size) % self.window_size\n",
    "        if x_r > 0 or x_d > 0:\n",
    "            x = x[:, :original_shape[1], :original_shape[2], :].contiguous()\n",
    "        x = x.view(original_shape[0], original_shape[1] * original_shape[2], original_shape[3])\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#特征提取器的总体结构\n",
    "class SwinTransformerFeatureExtractor(nn.Module):\n",
    "    def __init__(self, downsapmle_size=4, in_channels=3, embed_dim=96, depths=(2, 2, 6, 2), num_heads=(3, 6, 12, 24), window_size=7, mlp_ratio=4.,\n",
    "                 qkv_bias=True, drop_rate=0., attn_drop_rate=0., drop_path_rate=0.1, norm_layer=nn.LayerNorm, patch_norm=True, **kwargs):\n",
    "        super(SwinTransformerFeatureExtractor, self).__init__()\n",
    "\n",
    "        self.num_layers = len(depths)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.patch_norm = patch_norm\n",
    "        # stage4 输出的特征矩阵的Channel\n",
    "        self.num_features = int(embed_dim * 2 ** (self.num_layers - 1))\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "\n",
    "        self.patch_embed = patchEmbed(patch_size=downsapmle_size, in_channels=in_channels, embed_dim=embed_dim, norm_layer=norm_layer if self.patch_norm else None)\n",
    "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
    "\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i_layer in range(self.num_layers):\n",
    "            layers = BasicLayer(dim=int(embed_dim * 2 ** i_layer),\n",
    "                                depth=depths[i_layer],\n",
    "                                num_heads=num_heads[i_layer],\n",
    "                                window_size=window_size, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, drop=drop_rate, attn_drop=attn_drop_rate,\n",
    "                                drop_path=dpr[sum(depths[:i_layer]):sum(depths[:i_layer + 1])], norm_layer=norm_layer, downsample=patchmerging if (i_layer < self.num_layers - 1) else None)\n",
    "            self.layers.append(layers)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # [B, L, C]\n",
    "        x, H, W = self.patch_embed(x)\n",
    "        x = self.pos_drop(x)\n",
    "        hidden_states = []\n",
    "        for layer in self.layers:\n",
    "            x, H, W = layer(x, H, W)\n",
    "            \n",
    "            hidden_states.append(x.clone())\n",
    "\n",
    "        return x, hidden_states\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### blip + 多模态构建 新数据集\n",
    "\n",
    "为了得到带有背景描述的数据集，我们利用训练的服饰图像描述模型和多模态大语言模型，为真实背景的服饰图像数据集增加服饰描述和背景描述，构建全新的服饰图像描述数据集。\n",
    "我的使用的新数据集是选用 DeepFasion开源的12w数据集，仅使用图片，选用其中背景较丰富的5000张图像区间,具体范围可以看new_dataset/combined_input.json部分的起点和终点key。\n",
    "\n",
    "由于没有条件调用GPT-4 V的API，我们没有直接使用多模态的大模型，而是采用img2txt再txt2txt的模式，先让图像描述模型（Blip）生成图像的文字描述，再用大语言模型（文心一言）接受这些文字描述，按照提示性的prompt生成更有深度的背景信息。\n",
    "\n",
    "下面我封装了一个模块，用以快速调用Blip 进行批量处理，生成关于图片的简单文字表述，结果保存在new_dataset/res_new.json中，包括一个背景的粗略描述和一个整体的图像描述。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class blip_model():\n",
    "    def __init__(self) -> None:\n",
    "        self.processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "        self.model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\").to(\"cuda\")\n",
    "    def gen_res(self,img_path):\n",
    "        raw_image = Image.open(img_path).convert('RGB')\n",
    "\n",
    "        text = \"a people in front of \"\n",
    "        input_1 = self.processor(raw_image, text, return_tensors=\"pt\").to(\"cuda\")\n",
    "        out_1 = self.model.generate(**input_1,max_length=100)\n",
    "        res_1=self.processor.decode(out_1[0], skip_special_tokens=True)\n",
    "\n",
    "        input_2 = self.processor(raw_image, return_tensors=\"pt\").to(\"cuda\")\n",
    "        out_2 = self.model.generate(**input_2,max_length=100)\n",
    "        res_2=self.processor.decode(out_2[0], skip_special_tokens=True)\n",
    "        return res_1+\". \"+res_2\n",
    "def gen_json(img_path,n):\n",
    "    model=blip_model()\n",
    "    #img_path=\"D:/NNDL/data/deepfashion-multimodal/images\"\n",
    "    #获取该目录下所有文件，存入列表中\n",
    "    imgs=os.listdir(img_path)\n",
    "    res={}\n",
    "    start=31000\n",
    "\n",
    "    for img in range(start,len(imgs)):\n",
    "        img_k=imgs[img]\n",
    "        img_path_=img_path+\"/\"+img_k\n",
    "        res[img_k]=model.gen_res(img_path_)\n",
    "        if len(res)>=n:\n",
    "            break\n",
    "    #保存为json文件\n",
    "    with open('res.json', 'w') as f:\n",
    "        json.dump(res, f,indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM生成更有深度的图像描述\n",
    "将Blip生成的文字输入给文心一言，返回更有深度的背景图像描述。由于文心一言未能按照预期仅仅输出背景相关的描述，这里取描述的第一句话存于new_dataset/res_add.json中作为数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "API_KEY = 'jIHlGsMYHp4j1MrMZGNmYbCL'  \n",
    "SECRET_KEY = 'GzG1o4HC6G0qDVxPKcn9Zl4pv20j7CGA' #估计看到的时候已经过了有效期了所以无所谓了\n",
    "\n",
    "headers = {\n",
    "    'Content-Type': 'application/json',\n",
    "    'Accept': 'application/json'\n",
    "}\n",
    "\n",
    "def get_access_token():\n",
    "    \"\"\"\n",
    "    使用 AK，SK 生成鉴权签名（Access Token）\n",
    "    :return: access_token，或是None(如果错误)\n",
    "    \"\"\"\n",
    "    url = \"https://aip.baidubce.com/oauth/2.0/token\"\n",
    "    params = {\"grant_type\": \"client_credentials\", \"client_id\": API_KEY, \"client_secret\": SECRET_KEY}\n",
    "    return str(requests.post(url, params=params).json().get(\"access_token\"))\n",
    "\n",
    "def read_json(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return json.load(file)\n",
    "\n",
    "def QianFan(url, inputs):\n",
    "    responses = {}  # 创建一个字典来存储输入和相应的响应\n",
    "\n",
    "    for key, user_input in inputs.items():  # inputs是一个字典\n",
    "        request = {\n",
    "            \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"I will give you a sentence, where the first part is a summary of the background and the second part is information about a person's outfit. Please focus on the background information from the first part and provide an overall background description. \"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": \"Of course, please provide the sentence, and I will only output a sentence like: 'The background is'. \"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"a people in front of a bed. a pair of jeans\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": \"The backgroud is a homely bedroom.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"a people in front of a mirror. a man in a blue shorts and a white shirt\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": \"The backgroud is a mirror.\"\n",
    "            },\n",
    "            ]\n",
    "        }#添加一些前置词，以获得更贴合且标准化的回答。\n",
    "        # 添加数据\n",
    "        request[\"messages\"].append({\"role\": \"user\", \"content\": f\"Describe the setting of the following scene, focusing solely on the background without including any details about the person:{user_input}\"})\n",
    "\n",
    "\n",
    "        try:\n",
    "            response = requests.request(\"POST\", url, headers=headers, data=json.dumps(request))\n",
    "            text = response.text\n",
    "            data = json.loads(text)\n",
    "            model_response = data['result']\n",
    "            print(\"\\n回答：\\n\", model_response, '\\n')\n",
    "            # 根据句号分割文本\n",
    "            sentences = model_response.split(\". \")\n",
    "\n",
    "            # 获取第一个句子\n",
    "            first_sentence = sentences[0] + \".\"\n",
    "            responses[key] = first_sentence  # 将响应存储在字典中\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"QianFan 接口调用出错: {e}\")\n",
    "\n",
    "    # 保存或处理responses字典\n",
    "    return responses\n",
    "\n",
    "def main():\n",
    "    if not os.path.exists('./Amadeus/history'):\n",
    "        os.makedirs('./Amadeus/history')\n",
    "\n",
    "    url = \"https://aip.baidubce.com/rpc/2.0/ai_custom/v1/wenxinworkshop/chat/eb-instant?access_token=\" + get_access_token()\n",
    "\n",
    "    inputs = read_json('res_new.json') \n",
    "    responses = QianFan(url, inputs)\n",
    "\n",
    "    # 可以选择保存responses字典\n",
    "    with open('./Amadeus/history/responses.json', 'w', encoding='utf-8') as file:\n",
    "        json.dump(responses, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 生成基本的服饰描述\n",
    "使用之前训练好的ViT模型，生成新数据集上的图像描述，结果储存在new_dataset/res.json中。\n",
    "下面是用于调用之前训练的模型生成服装描述的脚本代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "import json\n",
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import ViTFeatureExtractor, BertTokenizer ,ViTModel, BertModel, BertConfig\n",
    "from collections import defaultdict, Counter\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "dataset='deepfashion-multimodal'\n",
    "img_path = f'data/{dataset}/img-001/img'\n",
    "vocab_path = f'data/{dataset}/vocab.json'\n",
    "\n",
    "def idx_to_word(idx, vocab):#将向量转化为文本描述\n",
    "    reverse_vocab = {v: k for k, v in vocab.items()}\n",
    "    return reverse_vocab.get(int(idx), '<unk>')\n",
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, img_folder, transform=None):\n",
    "        self.img_folder = img_folder\n",
    "        self.img_names = [img for img in os.listdir(img_folder) if img.endswith(('.png', '.jpg', '.jpeg'))]\n",
    "        print(len(self.img_names))\n",
    "        self.img_names = self.img_names[31000:36000]\n",
    "        print(self.img_names[0])\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_folder, self.img_names[idx])\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, self.img_names[idx]\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using GPU:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "# 图像预处理\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    # 根据需要添加更多的转换\n",
    "])\n",
    "\n",
    "# 创建 Dataset 实例\n",
    "dataset = CustomImageDataset(img_folder=img_path, transform=transform)\n",
    "\n",
    "# 创建 DataLoader\n",
    "data_loader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "with open(vocab_path, 'r') as f:\n",
    "    vocab = json.load(f)\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "vit_model_name = 'google/vit-base-patch16-224-in21k'\n",
    "transformer_config = BertConfig()\n",
    "\n",
    "model = Img2TxtModel(vit_model_name, transformer_config, vocab_size)\n",
    "# 加载模型状态字典\n",
    "checkpoint = torch.load('./model/best_model_epoch_10_batch_2700.pth')\n",
    "\n",
    "\n",
    "# 将状态字典应用到模型实例中\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model = model.to(device)\n",
    "\n",
    "model.eval()  # 将模型设置为评估模式\n",
    "\n",
    "generated_captions_dict = {}\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, name in data_loader:\n",
    "        images = images.to(device)\n",
    "        input_ids = images\n",
    "        outputs,_ = model.generate_text(input_ids, max_length=95, start_token_id=vocab['<start>'])\n",
    "        for i in range(outputs.shape[0]):\n",
    "            gen_caption = [idx_to_word(idx, vocab) for idx in outputs[i]]\n",
    "            if '<start>' in gen_caption:\n",
    "                gen_caption = gen_caption[1:]  # 移除第一个元素 (<start>)\n",
    "            if '<end>' in gen_caption:\n",
    "                gen_caption = gen_caption[:gen_caption.index('<end>')]  # 移除 <end> 及其后面的元素\n",
    "\n",
    "            caption_text = ' '.join(gen_caption)\n",
    "            generated_captions_dict[name[0]] = caption_text\n",
    "with open('res.json', 'w') as f:\n",
    "    json.dump(generated_captions_dict, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 组合数据\n",
    "使用脚本将服饰描述和背景描述拼接在一起，得到完整的数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# 读取JSON文件的函数\n",
    "def read_json(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return json.load(file)\n",
    "\n",
    "# 读取两个JSON文件\n",
    "json1 = read_json('res.json')\n",
    "json2 = read_json('res_add.json')\n",
    "\n",
    "# 存储组合后的结果\n",
    "combined_values = {}\n",
    "\n",
    "# 遍历第一个JSON文件的键\n",
    "for key in json1:\n",
    "    if key in json2:\n",
    "        # 将两个文件中相同键的值组合在一起\n",
    "        combined_values[key] = json1[key] + json2[key]\n",
    "\n",
    "# 保存组合后的结果到新的JSON文件\n",
    "with open('combined_input.json', 'w', encoding='utf-8') as file:\n",
    "    json.dump(combined_values, file, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"Combined JSON saved as combined_output.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 再次训练模型\n",
    "按照之前的流程重新在新数据集上训练一个模型，得到新的模型，这里不再放训练的代码。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 实验结果\n",
    "由于除了BLEU-4以外，其他指标的最大值其实和数据有关，所以为此我们先进行一个实验：得到如下指标的最大结果，然后将其作为一种相对的标准，结果如下：\n",
    "BLEU-MAX:1.0|CIDEr-D-MAX:0.0043918896512309125|SPICE-MAX:0.18703616844830037，在评估过程中将会同时输出实际值和相对值（被压缩到0-1之间），"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-MAX:1.0|CIDEr-D-MAX:0.0043918896512309125|SPICE-MAX:0.18703616844830037\n"
     ]
    }
   ],
   "source": [
    "max_cider=0.0043918896512309125\n",
    "max_spice=0.18703616844830037\n",
    "def evaluate_(data_loader) :#用来测试剩下两个指标的最大值\n",
    "    cands = []# 存储参考文本\n",
    "    refs = []# 需要过滤的词\n",
    "    filterd_words = set({model.vocab['<start>'], model.vocab['<end>'], model.vocab['<pad>']})\n",
    "    for i, (imgs, caps, caplens) in enumerate(data_loader):\n",
    "        cands.extend([filter_useless_words(cap, filterd_words) for cap in caps.tolist()])# 参考文本\n",
    "        refs.extend([filter_useless_words(cap, filterd_words) for cap in caps.tolist()]) #候选文本\n",
    "    bleu4_score = get_BLEU_score(cands, refs)\n",
    "    cider_d_score = get_CIDER_D_score(cands, refs)\n",
    "    spice_score= get_SPICE_score(cands, refs)\n",
    "    print(f\"BLEU-MAX:{bleu4_score}|CIDEr-D-MAX:{cider_d_score}|SPICE-MAX:{spice_score}\")\n",
    "    max_cider=cider_d_score\n",
    "    max_spice=spice_score\n",
    "evaluate_(test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里以 ARCTIC 的测评代码为例子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@@@实际值 BLEU:0.30466660274770024|CIDEr-D:0.0017447527516659645|SPICE:0.1336081641272871\n",
      "@@@相对值（0-1） BLEU:0.30466660274770024|CIDEr-D:0.3972669830575009|SPICE:0.7143439968629298\n"
     ]
    }
   ],
   "source": [
    "def filter_useless_words(sent, filterd_words):\n",
    "    # 去除句子中不参与BLEU值计算的符号\n",
    "    return [w for w in sent if w not in filterd_words]\n",
    "cider_d_score=0\n",
    "spice_score=0\n",
    "def evaluate(data_loader, model, config):\n",
    "    model.eval()\n",
    "    # 存储候选文本\n",
    "    cands = []\n",
    "    # 存储参考文本\n",
    "    refs = []\n",
    "    # 需要过滤的词\n",
    "    filterd_words = set({model.vocab['<start>'], model.vocab['<end>'], model.vocab['<pad>']})\n",
    "    cpi = config.captions_per_image\n",
    "    device = next(model.parameters()).device\n",
    "    for i, (imgs, caps, caplens) in enumerate(data_loader):\n",
    "        with torch.no_grad():\n",
    "            # 通过束搜索，生成候选文本\n",
    "            texts = model.generate_by_beamsearch(imgs.to(device), config.beam_k, config.max_len+2)\n",
    "            # 候选文本\n",
    "            cands.extend([filter_useless_words(text, filterd_words) for text in texts])\n",
    "            # 参考文本\n",
    "            refs.extend([filter_useless_words(cap, filterd_words) for cap in caps.tolist()])\n",
    "    \n",
    "    \n",
    "    bleu4_score = get_BLEU_score(cands, refs)\n",
    "    cider_d_score = get_CIDER_D_score(cands, refs)\n",
    "    spice_score= get_SPICE_score(cands, refs)\n",
    "    print(f\"@@@实际值 BLEU:{bleu4_score}|CIDEr-D:{cider_d_score}|SPICE:{spice_score}\")\n",
    "    print(f\"@@@相对值（0-1） BLEU:{bleu4_score}|CIDEr-D:{cider_d_score/max_cider}|SPICE:{spice_score/max_spice}\")\n",
    "    #model.train()\n",
    "evaluate(test_loader, model, ARCTIC_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同时我们以测试集的第一行数据来进行演示：以生成的描述文本和参考文本进行比较，直观评估ARCTIC的性能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@生成数据： [[154, 21, 47, 16, 31, 32, 39, 39, 52, 28, 10, 11, 12, 1, 39, 52, 16, 28, 7, 8, 9, 72, 13, 16, 84, 1, 49, 50, 28, 7, 8, 9, 10, 11, 12, 57, 16, 25, 59, 34, 35, 60, 155], [154, 21, 47, 16, 31, 32, 39, 39, 52, 28, 43, 12, 1, 39, 52, 16, 28, 7, 8, 9, 72, 13, 16, 84, 1, 49, 50, 28, 7, 8, 9, 43, 12, 57, 16, 25, 59, 34, 35, 60, 155], [154, 38, 40, 4, 5, 6, 7, 8, 9, 43, 12, 1, 13, 14, 15, 16, 81, 1, 18, 3, 16, 14, 5, 19, 1, 8, 16, 7, 9, 15, 4, 43, 12, 1, 26, 3, 16, 28, 7, 8, 9, 43, 12, 1, 26, 3, 16, 28, 7, 8, 9, 43, 12, 57, 16, 25, 59, 34, 35, 60, 155]]\n",
      "@生成的文本： This person is wearing a tank tank top with solid color patterns. The tank top is with cotton fabric and its neckline is round. The pants are with cotton fabric and solid color patterns. There is an accessory on her wrist.\n",
      "@实际的文本： This woman is wearing a tank tank shirt with graphic patterns and a three-point shorts. The tank shirt is with cotton fabric and its neckline is crew. The shorts are with cotton fabric and graphic patterns.\n"
     ]
    }
   ],
   "source": [
    "def batch_eva(data_loader, model, config): #这里使用实验数据的第一个batch来进行演示\n",
    "    model.eval()\n",
    "    for i, (imgs, caps, caplens) in  enumerate(test_loader):\n",
    "        cands = [] # 存储候选文本 \n",
    "        refs = [] # 存储参考文本\n",
    "        filterd_words = set({model.vocab['<start>'], model.vocab['<end>'], model.vocab['<pad>']}) #过滤词\n",
    "        cpi = config.captions_per_image\n",
    "        texts = model.generate_by_beamsearch(imgs.to(\"cuda\"), config.beam_k, config.max_len+2)\n",
    "        print(\"@生成数据：\",texts)\n",
    "        print(\"@生成的文本：\",wvec_to_cap(model.vocab,texts[0])) #抽出一个batch的第一个文本\n",
    "        print(\"@实际的文本：\",wvec_to_cap(model.vocab,caps[0].tolist()))\n",
    "        break\n",
    "batch_eva(test_loader, model, ARCTIC_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "之后的两个模型的forward形式同上， 我们整理了最大值的统计情况,如下表格\n",
    "\n",
    "\n",
    "| 模型名称 | BLEU-4 | CIDEr-D | SPICE |\n",
    "|---------|------|------|------|\n",
    "| ARCTIC   |  0.30466660274770024    |   (相对值：0.39726) 0.0017447527516659645  |   (相对值：0.71434)0.1336081641272871   |\n",
    "| VIT   |    **0.3074786561430062**  |   (相对值：0.93790) 0.004119164250591897   |   (相对值：0.70628)0.1321016861247424   |\n",
    "| SwinTrans   |   0.25770958160979623   |    (相对值：0.91071) 0.003999756354414652  |  (相对值：0.60681)0.11349623153205401    |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实验结果分析\n",
    "\n",
    "根据实验结果，分析模型的优缺点\n",
    "- BLEU-4 分析下\n",
    "  - ARCTIC 模型 性能评价:\n",
    "    ARCTIC 模型在 BLEU-4 上表现良好，得分为（0.30466660274770024）。\n",
    "    语法和词汇的准确性得到了有效提升,而且经过实验贪心算法下的得分要低于束搜索算法下的得分，所以束搜索算法在生成文本时，能够生成更流畅的文本。\n",
    "  - 视觉Transformer (ViT) + Transformer解码器 性能评价:\n",
    "      ViT + Transformer 模型在 BLEU-4 上的得分为（0.3074786561430062）。模型在服饰描述任务中取得了良好的结果，生成文本在语和词汇方面表现出色。**在这个指标下 其语义流畅度最好**\n",
    "  - 网格/区域表示、Transformer编码器+Transformer解码器 性能评价:  \n",
    "      网格/区域表示 + Transformer 模型在 BLEU-4 上的得分为（0.25770958160979623）。\n",
    "      模型在服饰描述任务中呈现出略低的性能，表现缺乏一定语句流畅度\n",
    "- CIDEr-D 分析下\n",
    "  - ARCTIC 模型 性能评价:\n",
    "    ARCTIC 模型在 CIDEr-D 上取得了(相对值：0.39726) 0.0017447527516659645 \n",
    "    CIDEr-D 分数显示模型在文本多样性和丰富性方面有一定的成功，但是不如其他模型。\n",
    "  - 视觉Transformer (ViT) + Transformer解码器 性能评价:\n",
    "    ViT + Transformer 模型在 CIDEr-D 上的得分为 (相对值：0.93790) 0.004119164250591897。\n",
    "    模型在服饰描述任务中具有较高的文本多样性和丰富性，**是表现最好的模型**\n",
    "  - 网格/区域表示、Transformer编码器+Transformer解码器 性能评价:\n",
    "    网格/区域表示 + Transformer 模型在 CIDEr-D 上的得分为(相对值：0.91071) 0.003999756354414652。\n",
    "    模型对服饰描述的多样性和丰富性取得了一定的成功。可以看出虽然流畅度不如其他模型，但是多样性还是不错的。\n",
    "- SPICE 分析下\n",
    "  - ARCTIC 模型 性能评价:\n",
    "    ARCTIC 模型在 SPICE 上取得了 (相对值：0.71434)0.1336081641272871 。\n",
    "    SPICE 分数反映了模型生成文本与图像内容相关性的程度，在语义层面上有一个很好的效果\n",
    "  - 视觉Transformer (ViT) + Transformer解码器 性能评价:\n",
    "    ViT + Transformer 模型在 SPICE 上的得分为 (相对值：0.70628)0.1321016861247424 。\n",
    "    模型在服饰描述任务中表现出色，成功捕捉图像语义信息。\n",
    "  - 网格/区域表示、Transformer编码器+Transformer解码器 性能评价:\n",
    "    网格/区域表示 + Transformer 模型在 SPICE 上的得分为 (相对值：0.60681)0.11349623153205401。\n",
    "    模型在 SPICE 上的表现显示其在描述图像内容方面的良好性能，但是效果相对较低\n",
    "\n",
    "当然 ARCTIC、视觉Transformer (ViT) + Transformer解码器和网格/区域表示、Transformer编码器+Transformer解码器这三个模型有不同的优劣势\n",
    "\n",
    "- ARCTIC 模型：\n",
    "  - 优势：\n",
    "    结合了注意力机制和编解码模型，有助于捕捉输入图像和生成描述之间的语义关系。\n",
    "    注意力机制使得模型在生成描述时能够更加关注与服饰相关的区域，提高了描述的准确性。\n",
    "  - 劣势：\n",
    "    可能需要更多的计算资源和训练时间，因为结合了多个模型组件。\n",
    "    在处理大规模数据集时，训练和推理速度可能较慢。\n",
    "    推理过程中尝试了不同的生成方式，发现加入 beam search 策略效果最佳。\n",
    "\n",
    "\n",
    "- 视觉 Transformer (ViT) + Transformer 解码器：\n",
    "  - 优势：\n",
    "    ViT 模型将输入图像转换为序列数据，直接应用 Transformer 解码器进行描述生成。\n",
    "    Transformer 解码器在自然语言处理任务中表现出色，生成准确且流畅的描述。\n",
    "  - 劣势：\n",
    "    ViT 模型可能对输入图像的分辨率和细节要求较高，对于复杂的服饰图像可能需要更多的训练数据和计算资源。\n",
    "    在处理长序列数据时，Transformer 解码器可能面临较长的训练和推理时间。\n",
    "\n",
    "\n",
    "- 网格/区域表示、Transformer 编码器+Transformer 解码器：\n",
    "  - 优势：\n",
    "    网格/区域表示将图像划分为网格或区域，有助于捕捉局部特征。\n",
    "    Transformer 编码器和解码器在处理序列数据时具有较强的建模能力，能够生成准确的描述。\n",
    "  - 劣势：\n",
    "    网格/区域表示可能需要额外的预处理步骤来划分图像，并可能导致信息损失。\n",
    "    Transformer 编码器和解码器的训练和推理时间可能较长，特别是在处理大规模数据集时\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 新数据集分析\n",
    "1.\"img_00035891.jpg\": \"a people in front of a red wall. a young man wearing a blue shirt and khaki pants\" --->\"img_00035891.jpg\": \"The scene takes place in front of a red wall.\"\n",
    "2.\"img_00035892.jpg\": \"a people in front of a mirror. a woman wearing a t - shirt with a picture of a man\"--->\"img_00035892.jpg\": \"The setting is a mirror in a room.\"\n",
    "3.\"img_00035899.jpg\": \"a people in front of a computer. a young boy sitting on a desk with a laptop\"--->\"img_00035899.jpg\": \"The setting is a room filled with a modern office desks and computers.\"\n",
    "4.\"img_00035904.jpg\": \"a people in front of a flower. a little girl with a flower in her hand\"--->\"img_00035904.jpg\": \"The setting is a garden or park with flowers blooming.\"\n",
    "在上述几个例子中，LLM接受blip生成的文字，确实可以输出更加有深度的背景信息，并且不会明显影响原本的信息，如1中的red wall。而且也能获得更深层的信息，如从mirror推出room，还有从laptop推出office，从flower推出花园\n",
    "但是可惜的是，blip的功能比较弱，提取图片中央信息就已经不准确了，背景更是经常出错，如1中其实真实的背景是一个红色集装箱，而2中背景是窗帘和玻璃。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 存在的问题 \n",
    "\n",
    "- 模型训练部分\n",
    "  - **资源不足**：不同模型对计算资源的需求不同，,作为学生的算力水平非常有限, 要训练出以该模型架构下的最优模型是一件困难的问题\n",
    "  - **数据集限制**：以训练数据构建的词典其实是一个非常小的词典，其实限制了模型的上限，离开数据集，在真实数据下的推理可能还是会有不完备的地方\n",
    "  - **图像裁剪**：数据集进行批量处理时的正方形裁剪可能会导致原始数据丢失部分信息，尤其是头部和脚步，还有白边导致的背景错误。\n",
    " \n",
    "- 数据生成部分\n",
    "  - **Blip性能不足**：我们使用的较小的模型，性能不足以提取所有背景，而且错误率出奇的高，导致后面的各项工作收到严重影响\n",
    "    比如对于一个图像，经过blip生成的文本是：a people in front of a brick wall. a girl wearing a denim dress and white **shoes** ；但是原图没有鞋子的信息。在没有鞋子信息的情况下还是生成了鞋子信息\n",
    "  - **LLM性能不足**：文心一言相比ChatGPT，理解能力还是较弱，而且除了第一句总结性的背景概括，后续便开始进行一些离奇的没有根据的幻想，比如下面这个例子中即使在我反复强调的情况下，LLM还是违背我对文本内容和长度的限制，生成如下长篇大论：\n",
    "    The scene takes place in a room with a green mat on the floor. The background is a wall, which is painted in a neutral color and has no decorations or features. The room is dimly lit, with only a small amount of natural light coming from the window. There is a door leading out of the room, but it is closed. The man standing on the green mat is wearing a blue shorts and a white shirt, and he is facing the wall. He appears to be in a meditative or contemplative state, as he is standing still and not interacting with anything else in the room.\n",
    "    但是，实际上我们需要的是相对较短的描述\n",
    "   - **真实多模态能力的缺失**：由于现在的结构是先img2txt再txt2txt，对于LLM来说会损失很多图像信息，真正的多模态还是需要直接进行img、txt2txt，有一个badcase：img2txt模型将楼上的玻璃识别成了镜子，导致LLM的输出也出错，误认为其在一个房间里，而真实的图片是在一个街道上。\n",
    "  这些因素导致数据集目前是一个不可用的状态。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 总结\n",
    "\n",
    "我们基本完成了作业要求，并额外进行了数据集的构建。\n",
    "\n",
    "完成了数个模型的构建和训练。\n",
    "\n",
    "利用训练的服饰图像描述模型和多模态大语言模型，为真实背景的服饰图像数据集增加服饰描述和背景描述，构建了一套可以生成经过了增强的全新数据集，并且在新数据集上重新训练服饰图像描述模型。\n",
    "\n",
    "尽管在课设完成的过程中我们面临了很多挑战和问题，例如有限的算力水平，这可能影响了构建模型的性能和实验的严谨性，但是我们依然尽我们所能完成了实验任务。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
