{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 任务说明\n",
    "服饰图像描述,训练一个模型,对输入的服饰图片,输出描述信息，我们实现的模型有以下三个实现：\n",
    "- ARCTIC，一个典型的基于注意力的编解码模型\n",
    "- 视觉Transformer (ViT) + Transformer解码器\n",
    "- 网格/区域表示、Transformer编码器+Transformer解码器\n",
    "同时也实现三种测评方法进行测评：\n",
    "- BLEU (Bilingual Evaluation Understudy)\n",
    "- SPICE (Semantic Propositional Image Caption Evaluation): \n",
    "- CIDEr-D (Consensus-based Image Description Evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 实验数据\n",
    "数据集使用的是 DeepFashion-MultiModal (https://github.com/yumingj/DeepFashion-MultiModal), 仅用到image和textual descriptions ，数据集划分为40k+行数据的训练集和2k+行数据的测试集，`train_captions.json`和`test_captions.json`分别对应训练集和测试集的图片与描述信息的键值对应"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = f'data/deepfashion-multimodal/images'\n",
    "def cap_to_wvec(vocab,cap):#将文本描述转换成向量\n",
    "    cap.replace(\",\",\"\")\n",
    "    cap.replace(\".\",\"\")\n",
    "    cap=cap.split()\n",
    "    res=[]\n",
    "    for word in cap:\n",
    "        if word in vocab.keys():\n",
    "            res.append(vocab[word])\n",
    "        else: #不在字典的词\n",
    "            res.append(vocab['<unk>'])\n",
    "    return res\n",
    "def wvec_to_cap(vocab,wvec):#将向量转换成文本描述\n",
    "    res=[]\n",
    "    for word in wvec:\n",
    "        for key,value in vocab.items():\n",
    "            if value==word and key not in ['<start>','<end>','<pad>','<unk>']:\n",
    "                res.append(key)\n",
    "    res=\" \".join(res)\n",
    "    return res\n",
    "def wvec_to_capls(vocab,wvec):#将向量转换成文本描述\n",
    "    res=[]\n",
    "    for word in wvec:\n",
    "        for key,value in vocab.items():\n",
    "            if value==word and key not in ['<start>','<end>','<pad>','<unk>']:\n",
    "                res.append(key)\n",
    "    return res\n",
    "class ImageTextDataset(Dataset):\n",
    "    def __init__(self, dataset_path, vocab_path, split, captions_per_image=1, max_len=93, transform=None):\n",
    "\n",
    "        self.split = split\n",
    "        assert self.split in {'train', 'test'}\n",
    "        self.cpi = captions_per_image\n",
    "        self.max_len = max_len\n",
    "\n",
    "        # 载入数据集\n",
    "        with open(dataset_path, 'r') as f:\n",
    "            self.data = json.load(f) #key是图片名字 value是描述\n",
    "            self.data_img=list(self.data.keys())\n",
    "        # 载入词典\n",
    "        with open(vocab_path, 'r') as f:\n",
    "            self.vocab = json.load(f)\n",
    "\n",
    "        # PyTorch图像预处理流程\n",
    "        self.transform = transform\n",
    "\n",
    "        # Total number of datapoints\n",
    "        self.dataset_size = len(self.data_img)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        # 第i个文本描述对应第(i // captions_per_image)张图片\n",
    "        img = Image.open(img_path+\"/\"+self.data_img[i]).convert('RGB')\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        c_vec=cap_to_wvec(self.vocab,self.data[self.data_img[i]])\n",
    "        #加入起始和结束标志\n",
    "        c_vec = [self.vocab['<start>']] + c_vec + [self.vocab['<end>']]\n",
    "        caplen = len(c_vec)\n",
    "        caption = torch.LongTensor(c_vec+ [self.vocab['<pad>']] * (self.max_len + 2 - caplen))\n",
    "        \n",
    "        return img, caption, caplen\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.dataset_size\n",
    "def mktrainval(data_dir, vocab_path, batch_size, workers=1):\n",
    "    train_tx = transforms.Compose([\n",
    "        transforms.Resize(256), # 重置图像分辨率\n",
    "        transforms.RandomCrop(224), # 随机裁剪\n",
    "        transforms.ToTensor(), # 转换成Tensor\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # 标准化--三个参数为三个通道的均值和标准差\n",
    "    ])\n",
    "    val_tx = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    train_set = ImageTextDataset(os.path.join(data_dir, 'train_captions.json'), vocab_path, 'train',  transform=train_tx)\n",
    "    test_set = ImageTextDataset(os.path.join(data_dir, 'test_captions.json'), vocab_path, 'test', transform=val_tx)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_set, batch_size=batch_size, shuffle=True, num_workers=workers, pin_memory=True)\n",
    "    \n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        test_set, batch_size=batch_size, shuffle=False, num_workers=workers, pin_memory=True, drop_last=False)\n",
    "\n",
    "    return train_loader, test_loader    \n",
    "train_loader,test_loader=mktrainval(data_dir='data/deepfashion-multimodal',\\\n",
    "                                        vocab_path='data/deepfashion-multimodal/vocab.json',\\\n",
    "                                        batch_size=3,workers=0) \n",
    "#workers=0 是因为ipynb不支持多线程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 实验环境\n",
    "- Python  3.9.16\n",
    "- 主要依赖库\n",
    "  - torch \n",
    "  - torchvision\n",
    "  - nltk \n",
    "实际使用py库情况如下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence # 压紧填充序列\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import ResNet101_Weights\n",
    "from nltk.translate.bleu_score import corpus_bleu # BLEU评价指标\n",
    "import numpy as np\n",
    "import json\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "from PIL import Image\n",
    "from collections import Counter,defaultdict\n",
    "from argparse import Namespace \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 所用的方法或模型\n",
    "\n",
    "## 评估方法\n",
    "\n",
    "### BLEU (BiLingual Evaluation Understudy)\n",
    "- BLUE是比较常用的评估指标之一，也是我们默认指标，需要注意的是，再调用计算BLEU值之前，要先将文本中人工添加的文本开始符、结束符和占位符去掉，其公式如下， 实际代码中我们借助nltk库进行实现\n",
    "$$BLEU = \\sum_{n=1}^k w_n \\frac{ngram_{sys}(n)}{ngram_{ref}(n)}$$\n",
    "其中：\n",
    "  - n 是 n-gram 的阶数，取值范围为 1 到 4。\n",
    "  - wn 是 n-gram 的权重，通常取均匀权重。\n",
    "  - ngramsys(n) 是机器翻译结果中的 n-gram 数量。\n",
    "  - ngramref(n) 是参考翻译中的 n-gram 数量。\n",
    "  BLEU 的得分范围为 0 到 1。得分越高，表示机器翻译结果与参考翻译越相似。\n",
    "  - 优点：容易计算\n",
    "  - 缺点:\n",
    "    - 没有考虑n-gram的顺序\n",
    "    - 平等对待所有的n-gram\n",
    "    - 衡量的是句子之间的流畅性而非语义相似度\n",
    "### CIDEr-D (Consensus-based Image Description Evaluation)\n",
    "- 是CIDEr的改进，对于动词原形和名词匹配成功的问题，CIDEr-D不再取词根\n",
    "其用了一种折扣函数来降低长句子对评分的影响，增加了惩罚生成句子和参考句子的长度差别的权重，并且通过对n-gram计数的截断操作不再计算生成句子中出现次数超过参考句子的n-gram,\n",
    "从而减少了重复单词对评分的影响，其实也是计算1到4 gram的结果的平均值，其公式如下\n",
    "$$C I D E r - D _ { n } ( c _ { i } , S _ { i } ) = \\frac { 1 0 } { m } \\sum _ { j } e ^ { - \\frac { -( i ( c _ { i } ) - l ( s _ { i j } ) ) ^ { 2 } } { 2 \\sigma ^ { 2 } } } \\times \\frac { \\min ( g ^ { n } ( c _ { i } ) , g ^ { n } ( s _ { i j } ) ) \\cdot g ^ { n } ( s _ { i j } ) } {| | g ^ { n } ( c _ { i } ) | | | g ^ { n } ( s _ { i j } ) || } $$\n",
    "- 优点：\n",
    "  - CIDEr引入了TF-IDF为n-gram进行加权，这样就避免评价候选句子时因为一些常见却不够有信息量的n-gram打上高分\n",
    "- 缺点：\n",
    "  - CIDEr取词根的操作会让一些动词的原型和名词匹配成功\n",
    "  - 高置信度的词重复出现的长句的CIDEr得分也很高\n",
    "### SPICE (Semantic Propositional Image Caption Evaluation): \n",
    "- 是以名词为中心的度量，是以图的语义表示来编码图像描述中的对象、属性和关系\n",
    "首先要将候选句子和参考句子集转化为场景图\n",
    "然后比较候选句子和参考句子集中元组的precision、recall，最终计算出F1 score\n",
    "公式如下\n",
    "$$SPICE = \\sum_{i=1}^m \\frac{1}{|S_i|} \\sum_{j=1}^n \\frac{s_{ij}}{|R_i|}\n",
    "$$\n",
    "  - m 是图像描述的数量。\n",
    "  - n 是图像描述中的对象、属性和关系的数量。\n",
    "  - Si 是图像描述 i 中的对象、属性和关系。\n",
    "  - Ri 是参考图像描述 i 中的对象、属性和关系。\n",
    "  - sij 是图像描述 i 中的对象、属性和关系 j 与参考图像描述 i 中的对象、属性和关系 j 的相似度\n",
    "- 优点：\n",
    "  - 在语义而非n-gram层级度量\n",
    "  - 每个句子映射到场景图后可以从中提取出模型关于某些关系或者属性的识别能力\n",
    "- 缺点\n",
    "  - 缺少n-gram来度量句子的流畅性\n",
    "  - 度量的准确性受到场景图解析器的制约\n",
    "\n",
    "使用代码如下，在evaluate的时候调用,接受cands, refs返回对应评估分数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cider_d(reference_list, candidate_list, n=4):\n",
    "    def count_ngrams(tokens, n):\n",
    "        ngrams = []\n",
    "        for i in range(len(tokens) - n + 1):\n",
    "            ngram = tuple(tokens[i:i+n])\n",
    "            ngrams.append(ngram)\n",
    "        return ngrams\n",
    "\n",
    "    def compute_cider_d(reference_list, candidate_list, n):\n",
    "        cider_d_scores = []\n",
    "        for refs, cand in zip(reference_list, candidate_list):\n",
    "            cider_d_score = 0.0\n",
    "            for i in range(1, n + 1):\n",
    "                cand_ngrams = count_ngrams(cand, i)\n",
    "                ref_ngrams_list = [count_ngrams(ref, i) for ref in refs]\n",
    "\n",
    "                total_ref_ngrams = [ngram for ref_ngrams in ref_ngrams_list for ngram in ref_ngrams]\n",
    "\n",
    "                count_cand = 0\n",
    "                count_clip = 0\n",
    "\n",
    "                for ngram in cand_ngrams:\n",
    "                    count_cand += 1\n",
    "                    if ngram in total_ref_ngrams:\n",
    "                        count_clip += 1\n",
    "\n",
    "                precision = count_clip / count_cand if count_cand > 0 else 0.0\n",
    "                recall = count_clip / len(total_ref_ngrams) if len(total_ref_ngrams) > 0 else 0.0\n",
    "\n",
    "                beta = 1.0\n",
    "                f_score = (1 + beta**2) * precision * recall / (beta**2 * precision + recall) if precision + recall > 0 else 0.0\n",
    "\n",
    "                cider_d_score += f_score\n",
    "\n",
    "            cider_d_score /= n\n",
    "            cider_d_scores.append(cider_d_score)\n",
    "\n",
    "        return cider_d_scores\n",
    "\n",
    "    reference_tokens_list = reference_list\n",
    "    candidate_tokens_list = candidate_list\n",
    "\n",
    "    scores = compute_cider_d(reference_tokens_list, candidate_tokens_list, n)\n",
    "\n",
    "    return np.mean(scores)\n",
    "def spice(reference_list, candidate_list, idf=None, beta=3):\n",
    "    def tokenize(sentence):\n",
    "        return sentence.lower().split()\n",
    "\n",
    "    def count_ngrams(tokens, n):\n",
    "        ngrams = []\n",
    "        for i in range(len(tokens) - n + 1):\n",
    "            ngram = tuple(tokens[i:i+n])\n",
    "            ngrams.append(ngram)\n",
    "        return ngrams\n",
    "\n",
    "    def compute_spice_score(reference, candidate, idf, beta):\n",
    "        reference_tokens = reference\n",
    "        candidate_tokens = candidate\n",
    "\n",
    "        reference_ngrams = [count_ngrams(reference_tokens, i) for i in range(1, beta + 1)]\n",
    "        candidate_ngrams = [count_ngrams(candidate_tokens, i) for i in range(1, beta + 1)]\n",
    "\n",
    "        precision_scores = []\n",
    "        recall_scores = []\n",
    "\n",
    "        for i in range(beta):\n",
    "            common_ngrams = set(candidate_ngrams[i]) & set(reference_ngrams[i])\n",
    "\n",
    "            precision = len(common_ngrams) / len(candidate_ngrams[i]) if len(candidate_ngrams[i]) > 0 else 0.0\n",
    "            recall = len(common_ngrams) / len(reference_ngrams[i]) if len(reference_ngrams[i]) > 0 else 0.0\n",
    "\n",
    "            precision_scores.append(precision)\n",
    "            recall_scores.append(recall)\n",
    "\n",
    "        precision_avg = np.mean(precision_scores)\n",
    "        recall_avg = np.mean(recall_scores)\n",
    "\n",
    "        spice_score = (precision_avg * recall_avg) / (precision_avg + recall_avg) if precision_avg + recall_avg > 0 else 0.0\n",
    "\n",
    "        if idf:\n",
    "            spice_score *= np.exp(np.sum([idf[token] for token in common_ngrams]) / len(candidate_tokens))\n",
    "\n",
    "        return spice_score\n",
    "\n",
    "    if idf is None:\n",
    "        idf = {}\n",
    "\n",
    "    spice_scores = []\n",
    "\n",
    "    for reference, candidate in zip(reference_list, candidate_list):\n",
    "        spice_score = compute_spice_score(reference, candidate, idf, beta)\n",
    "        spice_scores.append(spice_score)\n",
    "\n",
    "    return np.mean(spice_scores)\n",
    "def get_BLEU_score(cands, refs): #获取BLEU分数\n",
    "    pasmultiple_refs = []\n",
    "    for idx in range(len(refs)):\n",
    "        multiple_refs.append(refs[(idx//1)*1 : (idx//1)*1+1])#每个候选文本对应cpi==1条参考文本\n",
    "    bleu4 = corpus_bleu(multiple_refs, cands, weights=(0.25,0.25,0.25,0.25))\n",
    "    return bleu4\n",
    "def get_CIDER_D_score(cands, refs): #获得CIDER-D分数\n",
    "    refs_ = [wvec_to_capls(model.vocab,ref) for ref in refs]\n",
    "    cands_ = [wvec_to_capls(model.vocab,cand) for cand in cands]\n",
    "    return cider_d(refs_, cands_)\n",
    "def get_SPICE_score(cands, refs): #获得SPICE分数\n",
    "    refs_ = [wvec_to_cap(model.vocab,ref) for ref in refs]\n",
    "    cands_ = [wvec_to_cap(model.vocab,cand) for cand in cands]\n",
    "    return spice(refs_, cands_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型定义\n",
    "### ARCTIC，一个典型的基于注意力的编解码模型\n",
    "\n",
    "### 视觉Transformer (ViT) + Transformer解码器\n",
    "\n",
    "### 网格/区域表示、Transformer编码器+Transformer解码器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 实验结果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 实验结果分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 总结"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
