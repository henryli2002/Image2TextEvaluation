{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 任务说明\n",
    "服饰图像描述,训练一个模型,对输入的服饰图片,输出描述信息，我们实现的模型有以下三个实现：\n",
    "- ARCTIC，一个典型的基于注意力的编解码模型\n",
    "- 视觉Transformer (ViT) + Transformer解码器\n",
    "- 网格/区域表示、Transformer编码器+Transformer解码器\n",
    "  \n",
    "同时也实现三种测评方法进行测评：\n",
    "- BLEU (Bilingual Evaluation Understudy)\n",
    "- SPICE (Semantic Propositional Image Caption Evaluation): \n",
    "- CIDEr-D (Consensus-based Image Description Evaluation)\n",
    "\n",
    "以及实现了附加任务：\n",
    "- 利用训练的服饰图像描述模型和多模态大语言模型，为真实背景的服饰图像数据集增加服饰描述和背景描述，构建全新的服饰图像描述数据集\n",
    "  - 在新数据集上重新训练服饰图像描述模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 实验数据\n",
    "数据集使用的是 DeepFashion-MultiModal (https://github.com/yumingj/DeepFashion-MultiModal), 仅用到image和textual descriptions ，数据集划分为10k+行数据的训练集和2k+行数据的测试集，`train_captions.json`和`test_captions.json`分别对应训练集和测试集的图片与描述信息的键值对应"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = f'../data/deepfashion-multimodal/images'\n",
    "def cap_to_wvec(vocab,cap):#将文本描述转换成向量\n",
    "    cap.replace(\",\",\"\")\n",
    "    cap.replace(\".\",\"\")\n",
    "    cap=cap.split()\n",
    "    res=[]\n",
    "    for word in cap:\n",
    "        if word in vocab.keys():\n",
    "            res.append(vocab[word])\n",
    "        else: #不在字典的词\n",
    "            res.append(vocab['<unk>'])\n",
    "    return res\n",
    "def wvec_to_cap(vocab,wvec):#将向量转换成文本描述\n",
    "    res=[]\n",
    "    for word in wvec:\n",
    "        for key,value in vocab.items():\n",
    "            if value==word and key not in ['<start>','<end>','<pad>','<unk>']:\n",
    "                res.append(key)\n",
    "    res=\" \".join(res)\n",
    "    return res\n",
    "def wvec_to_capls(vocab,wvec):#将向量转换成文本描述\n",
    "    res=[]\n",
    "    for word in wvec:\n",
    "        for key,value in vocab.items():\n",
    "            if value==word and key not in ['<start>','<end>','<pad>','<unk>']:\n",
    "                res.append(key)\n",
    "    return res\n",
    "class ImageTextDataset(Dataset):\n",
    "    def __init__(self, dataset_path, vocab_path, split, captions_per_image=1, max_len=93, transform=None):\n",
    "\n",
    "        self.split = split\n",
    "        assert self.split in {'train', 'test'}\n",
    "        self.cpi = captions_per_image\n",
    "        self.max_len = max_len\n",
    "\n",
    "        # 载入数据集\n",
    "        with open(dataset_path, 'r') as f:\n",
    "            self.data = json.load(f) #key是图片名字 value是描述\n",
    "            self.data_img=list(self.data.keys())\n",
    "        # 载入词典\n",
    "        with open(vocab_path, 'r') as f:\n",
    "            self.vocab = json.load(f)\n",
    "\n",
    "        # PyTorch图像预处理流程\n",
    "        self.transform = transform\n",
    "\n",
    "        # Total number of datapoints\n",
    "        self.dataset_size = len(self.data_img)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        # 第i个文本描述对应第(i // captions_per_image)张图片\n",
    "        img = Image.open(img_path+\"/\"+self.data_img[i]).convert('RGB')\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        c_vec=cap_to_wvec(self.vocab,self.data[self.data_img[i]])\n",
    "        #加入起始和结束标志\n",
    "        c_vec = [self.vocab['<start>']] + c_vec + [self.vocab['<end>']]\n",
    "        caplen = len(c_vec)\n",
    "        caption = torch.LongTensor(c_vec+ [self.vocab['<pad>']] * (self.max_len + 2 - caplen))\n",
    "        \n",
    "        return img, caption, caplen\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.dataset_size\n",
    "def mktrainval(data_dir, vocab_path, batch_size, workers=1):\n",
    "    train_tx = transforms.Compose([\n",
    "        transforms.Resize(256), # 重置图像分辨率\n",
    "        transforms.RandomCrop(224), # 随机裁剪\n",
    "        transforms.ToTensor(), # 转换成Tensor\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # 标准化--三个参数为三个通道的均值和标准差\n",
    "    ])\n",
    "    val_tx = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    train_set = ImageTextDataset(os.path.join(data_dir, 'train_captions.json'), vocab_path, 'train',  transform=train_tx)\n",
    "    test_set = ImageTextDataset(os.path.join(data_dir, 'test_captions.json'), vocab_path, 'test', transform=val_tx)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_set, batch_size=batch_size, shuffle=True, num_workers=workers, pin_memory=True)\n",
    "    \n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        test_set, batch_size=batch_size, shuffle=False, num_workers=workers, pin_memory=True, drop_last=False)\n",
    "\n",
    "    return train_loader, test_loader    \n",
    "train_loader,test_loader=mktrainval(data_dir='../data/deepfashion-multimodal',\\\n",
    "                                        vocab_path='../data/deepfashion-multimodal/vocab.json',\\\n",
    "                                        batch_size=3,workers=0) \n",
    "#workers=0 是因为ipynb不支持多线程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 关于字典的处理\n",
    "    我们采用了一个非常传统的方式，加载所有的训练集和测试集，进行词频统计，我们默认阈值为5 到达阈值的词将会加入到词典中\n",
    "\n",
    "    之后我们额外添加了 < pad > < start > < end > < unk >四个词，分别代表填充词，句首标记，句尾标记，未知词\n",
    "    \n",
    "    最终写入到vocab.json文件中\n",
    "- 关于数据集类的处理\n",
    "我们使用Pytroch的Dataset来构建数据集类，在此之外封装了返回测试集和训练集的函数，可以进行自定义的批量预处理，我们在训练和推理过程中进行了如下的处理\n",
    "    - resize 图像大小为256*256\n",
    "    - 随机裁剪 为224*224\n",
    "    - 转换为Torch Tensor\n",
    "    - normalize 归一化为\n",
    "        - mean=[0.485, 0.456, 0.406]\n",
    "        - std=[0.229, 0.224, 0.225]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 实验环境\n",
    "- Python  3.9.16\n",
    "- 主要依赖库\n",
    "  - torch \n",
    "  - torchvision\n",
    "  - nltk \n",
    "实际使用py库情况如下\n",
    "\n",
    "项目开发中除数据集和模型外代码使用git进行版本控制，需要说明的是，由于课设由多个人多个设备下完成，而训练模型和数据集有所不同，我们维持同步的仅仅是Image2TextEvaluation这一项目，所以不同设备下由细微不同之处，如此测试，需要更换实际环境替换对应代码中的路径，直接进行测试可能会出现一些问题————这里提前声明这一点————文件路径大致如下：\n",
    "```\n",
    "├── Image2TextEvaluation  \n",
    "│   ├── ARCTIC\n",
    "│   │   ├── ARCTIC_dataloader.py\n",
    "│   │   ├── ARCTIC_model.py\n",
    "│   │   └── train.py\n",
    "│   ├── Vit\n",
    "│   │   ├── ... //同上的模型和训练文件\n",
    "│   │   ├── QianFan-agent.py //多模态大模型+blip 构建数据集\n",
    "│   │   ├── merge_json.py\n",
    "│   │   ├── generate.ipynb\n",
    "│   │   └── ....//生成的一些数据文件\n",
    "│   ├── SwinTrans\n",
    "│   │   ├── ...//同上\n",
    "│   │\n",
    "│   ├── Tools\n",
    "│   │   ├── test_blip.py\n",
    "│   │   ├── ...\n",
    "│   │\n",
    "│   ├── evaluate.py\n",
    "│   ├── README.md\n",
    "│   └── 结题报告.ipynb\n",
    "├── model\n",
    "│   ├── best_arctic.ckpt\n",
    "│   ├── last_arctic.ckpt\n",
    "│   └── ...\n",
    "├── data\n",
    "│   ├── deepfashion-multimodal\n",
    "│   │   ├── images\n",
    "│   │   │   ├── 001.jpg\n",
    "│   │   │   ├── ...\n",
    "│   │   ├── train_captions.json\n",
    "│   │   ├── test_captions.json\n",
    "│   │   ├── vocab.json\n",
    "│   │   └── ...\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence # 压紧填充序列\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import ResNet101_Weights\n",
    "from nltk.translate.bleu_score import corpus_bleu # BLEU评价指标\n",
    "import numpy as np\n",
    "import json\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "from PIL import Image\n",
    "from collections import Counter,defaultdict\n",
    "from argparse import Namespace \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 所用的方法或模型\n",
    "\n",
    "## 评估方法\n",
    "\n",
    "### BLEU (BiLingual Evaluation Understudy)\n",
    "- BLUE是比较常用的评估指标之一，也是我们默认指标，需要注意的是，再调用计算BLEU值之前，要先将文本中人工添加的文本开始符、结束符和占位符去掉，其公式如下， 实际代码中我们借助nltk库进行实现\n",
    "$$BLEU = \\sum_{n=1}^k w_n \\frac{ngram_{sys}(n)}{ngram_{ref}(n)}$$\n",
    "其中：\n",
    "  - n 是 n-gram 的阶数，取值范围为 1 到 4。\n",
    "  - wn 是 n-gram 的权重，通常取均匀权重。\n",
    "  - ngramsys(n) 是机器翻译结果中的 n-gram 数量。\n",
    "  - ngramref(n) 是参考翻译中的 n-gram 数量。\n",
    "  BLEU 的得分范围为 0 到 1。得分越高，表示机器翻译结果与参考翻译越相似。\n",
    "  - 优点：容易计算\n",
    "  - 缺点:\n",
    "    - 没有考虑n-gram的顺序\n",
    "    - 平等对待所有的n-gram\n",
    "    - 衡量的是句子之间的流畅性而非语义相似度\n",
    "### CIDEr-D (Consensus-based Image Description Evaluation)\n",
    "- 是CIDEr的改进，对于动词原形和名词匹配成功的问题，CIDEr-D不再取词根\n",
    "其用了一种折扣函数来降低长句子对评分的影响，增加了惩罚生成句子和参考句子的长度差别的权重，并且通过对n-gram计数的截断操作不再计算生成句子中出现次数超过参考句子的n-gram,\n",
    "从而减少了重复单词对评分的影响，其实也是计算1到4 gram的结果的平均值，其公式如下\n",
    "$$C I D E r - D _ { n } ( c _ { i } , S _ { i } ) = \\frac { 1 0 } { m } \\sum _ { j } e ^ { - \\frac { -( i ( c _ { i } ) - l ( s _ { i j } ) ) ^ { 2 } } { 2 \\sigma ^ { 2 } } } \\times \\frac { \\min ( g ^ { n } ( c _ { i } ) , g ^ { n } ( s _ { i j } ) ) \\cdot g ^ { n } ( s _ { i j } ) } {| | g ^ { n } ( c _ { i } ) | | | g ^ { n } ( s _ { i j } ) || } $$\n",
    "- 优点：\n",
    "  - CIDEr引入了TF-IDF为n-gram进行加权，这样就避免评价候选句子时因为一些常见却不够有信息量的n-gram打上高分\n",
    "- 缺点：\n",
    "  - CIDEr取词根的操作会让一些动词的原型和名词匹配成功\n",
    "  - 高置信度的词重复出现的长句的CIDEr得分也很高\n",
    "### SPICE (Semantic Propositional Image Caption Evaluation): \n",
    "- 是以名词为中心的度量，是以图的语义表示来编码图像描述中的对象、属性和关系\n",
    "首先要将候选句子和参考句子集转化为场景图\n",
    "然后比较候选句子和参考句子集中元组的precision、recall，最终计算出F1 score\n",
    "公式如下\n",
    "$$SPICE = \\sum_{i=1}^m \\frac{1}{|S_i|} \\sum_{j=1}^n \\frac{s_{ij}}{|R_i|}\n",
    "$$\n",
    "  - m 是图像描述的数量。\n",
    "  - n 是图像描述中的对象、属性和关系的数量。\n",
    "  - Si 是图像描述 i 中的对象、属性和关系。\n",
    "  - Ri 是参考图像描述 i 中的对象、属性和关系。\n",
    "  - sij 是图像描述 i 中的对象、属性和关系 j 与参考图像描述 i 中的对象、属性和关系 j 的相似度\n",
    "- 优点：\n",
    "  - 在语义而非n-gram层级度量\n",
    "  - 每个句子映射到场景图后可以从中提取出模型关于某些关系或者属性的识别能力\n",
    "- 缺点\n",
    "  - 缺少n-gram来度量句子的流畅性\n",
    "  - 度量的准确性受到场景图解析器的制约\n",
    "\n",
    "使用代码如下，在evaluate的时候调用,接受cands, refs返回对应评估分数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cider_d(reference_list, candidate_list, n=4):\n",
    "    def count_ngrams(tokens, n):\n",
    "        ngrams = []\n",
    "        for i in range(len(tokens) - n + 1):\n",
    "            ngram = tuple(tokens[i:i+n])\n",
    "            ngrams.append(ngram)\n",
    "        return ngrams\n",
    "\n",
    "    def compute_cider_d(reference_list, candidate_list, n):\n",
    "        cider_d_scores = []\n",
    "        for refs, cand in zip(reference_list, candidate_list):\n",
    "            cider_d_score = 0.0\n",
    "            for i in range(1, n + 1):\n",
    "                cand_ngrams = count_ngrams(cand, i)\n",
    "                ref_ngrams_list = [count_ngrams(ref, i) for ref in refs]\n",
    "\n",
    "                total_ref_ngrams = [ngram for ref_ngrams in ref_ngrams_list for ngram in ref_ngrams]\n",
    "\n",
    "                count_cand = 0\n",
    "                count_clip = 0\n",
    "\n",
    "                for ngram in cand_ngrams:\n",
    "                    count_cand += 1\n",
    "                    if ngram in total_ref_ngrams:\n",
    "                        count_clip += 1\n",
    "\n",
    "                precision = count_clip / count_cand if count_cand > 0 else 0.0\n",
    "                recall = count_clip / len(total_ref_ngrams) if len(total_ref_ngrams) > 0 else 0.0\n",
    "\n",
    "                beta = 1.0\n",
    "                f_score = (1 + beta**2) * precision * recall / (beta**2 * precision + recall) if precision + recall > 0 else 0.0\n",
    "\n",
    "                cider_d_score += f_score\n",
    "\n",
    "            cider_d_score /= n\n",
    "            cider_d_scores.append(cider_d_score)\n",
    "\n",
    "        return cider_d_scores\n",
    "\n",
    "    reference_tokens_list = reference_list\n",
    "    candidate_tokens_list = candidate_list\n",
    "\n",
    "    scores = compute_cider_d(reference_tokens_list, candidate_tokens_list, n)\n",
    "\n",
    "    return np.mean(scores)\n",
    "def spice(reference_list, candidate_list, idf=None, beta=3):\n",
    "    def count_ngrams(tokens, n):\n",
    "        ngrams = []\n",
    "        for i in range(len(tokens) - n + 1):\n",
    "            ngram = tuple(tokens[i:i+n])\n",
    "            ngrams.append(ngram)\n",
    "        return ngrams\n",
    "\n",
    "    def compute_spice_score(reference, candidate, idf, beta):\n",
    "        reference_tokens = reference\n",
    "        candidate_tokens = candidate\n",
    "\n",
    "        reference_ngrams = [count_ngrams(reference_tokens, i) for i in range(1, beta + 1)]\n",
    "        candidate_ngrams = [count_ngrams(candidate_tokens, i) for i in range(1, beta + 1)]\n",
    "\n",
    "        precision_scores = []\n",
    "        recall_scores = []\n",
    "\n",
    "        for i in range(beta):\n",
    "            common_ngrams = set(candidate_ngrams[i]) & set(reference_ngrams[i])\n",
    "\n",
    "            precision = len(common_ngrams) / len(candidate_ngrams[i]) if len(candidate_ngrams[i]) > 0 else 0.0\n",
    "            recall = len(common_ngrams) / len(reference_ngrams[i]) if len(reference_ngrams[i]) > 0 else 0.0\n",
    "\n",
    "            precision_scores.append(precision)\n",
    "            recall_scores.append(recall)\n",
    "\n",
    "        precision_avg = np.mean(precision_scores)\n",
    "        recall_avg = np.mean(recall_scores)\n",
    "\n",
    "        spice_score = (precision_avg * recall_avg) / (precision_avg + recall_avg) if precision_avg + recall_avg > 0 else 0.0\n",
    "\n",
    "        if idf:\n",
    "            spice_score *= np.exp(np.sum([idf[token] for token in common_ngrams]) / len(candidate_tokens))\n",
    "\n",
    "        return spice_score\n",
    "\n",
    "    if idf is None:\n",
    "        idf = {}\n",
    "\n",
    "    spice_scores = []\n",
    "\n",
    "    for reference, candidate in zip(reference_list, candidate_list):\n",
    "        spice_score = compute_spice_score(reference, candidate, idf, beta)\n",
    "        spice_scores.append(spice_score)\n",
    "\n",
    "    return np.mean(spice_scores)\n",
    "def get_BLEU_score(cands, refs): #获取BLEU分数\n",
    "    multiple_refs = []\n",
    "    for idx in range(len(refs)):\n",
    "        multiple_refs.append(refs[(idx//1)*1 : (idx//1)*1+1])#每个候选文本对应cpi==1条参考文本\n",
    "    bleu4 = corpus_bleu(multiple_refs, cands, weights=(0.25,0.25,0.25,0.25))\n",
    "    return bleu4\n",
    "def get_CIDER_D_score(cands, refs): #获得CIDER-D分数\n",
    "    refs_ = [wvec_to_capls(model.vocab,ref) for ref in refs]\n",
    "    cands_ = [wvec_to_capls(model.vocab,cand) for cand in cands]\n",
    "    return cider_d(refs_, cands_)\n",
    "def get_SPICE_score(cands, refs): #获得SPICE分数\n",
    "    refs_ = [wvec_to_cap(model.vocab,ref) for ref in refs]\n",
    "    cands_ = [wvec_to_cap(model.vocab,cand) for cand in cands]\n",
    "    return spice(refs_, cands_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型定义\n",
    "### ARCTIC，一个典型的基于注意力的编解码模型\n",
    "\n",
    "模型架构: ARCTIC 是一个基于注意力的编码-解码模型，使用了图像编码器和注意力解码器。\n",
    "#### 编码器部分\n",
    "图像编码器使用了 ResNet-101 网络进行特征提取。\n",
    "\n",
    "我们使用了预训练模型的权重，所以后续的训练中实际上编码器不参与训练过程，其参数是冻结的\n",
    "\n",
    "\n",
    "\n",
    "我们并将其最后一个非全连接层作为网格表示提取层。\n",
    "#### 解码器部分\n",
    "解码器采用 GRU，利用注意力上下文向量和当前时刻的词嵌入来生成预测结果。该模型支持束搜索来生成更准确的描述。\n",
    "\n",
    "解码器实质是一个rnn，其是有一层加性注意力机制，它接受查询（Q）和键值对（K，V），计算注意力分数，最后输出上下文向量。\n",
    "\n",
    "- **加性注意力机制**：我们在AdditiveAttention 类上实现了加性注意力机制，用于在解码过程中关注图像中不同部分的信息。\n",
    "\n",
    "- **使用GRU**: rnn具体使用的是 GRU ：是一种门控循环单元（gated recurrent unit）的缩写，它是一种用于处理序列数据的循环神经网络（RNN）\n",
    "- \n",
    "    我个人认为选择GRU 而不是 LSTM 是因为相比LSTM，使用GRU能够达到相当的效果，并且相比之下更容易进行训练，能够很大程度上提高训练效率，可以很出现可观的效果。\n",
    "\n",
    "- **forward过程：** 在单步forward过程中，我们将上下文向量和当前时刻的词表示拼接，然后和我们的rnn贴合在一起\n",
    "\n",
    "    作为 GRU 的输入，进入全连接层对GRU 输出进行线性变换，得到单步forward；而实际生成句子的时候就是重复这个过程，选择概率最大的词作为下一个词，直到遇到结束符或者到达最大的长度\n",
    "\n",
    "    当然，这种推理方法不是最好的，实质上这是一种 贪心算法\n",
    "    我们知道，**贪心算法的缺点就是它无法保证全局最优**，我们要的是所有预测的的概率相乘最大。\n",
    "\n",
    "- **使用束搜索**：所以我们还实现了另一种方法，即使用束搜索来生成更准确的描述。\n",
    "\n",
    "    束搜索就是在每一步的时候，计算到这一步为止的预测y序列的概率最大的前k条，k叫集束宽。\n",
    "\n",
    "    当然束搜索的缺点也很明显，推理所需时间实际上是倍增的，但是效果上是有增益的。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ARCTIC_config = Namespace(\n",
    "        max_len = 93,\n",
    "        captions_per_image = 1,\n",
    "        batch_size = 32,\n",
    "        image_code_dim = 2048,\n",
    "        word_dim = 512,\n",
    "        hidden_size = 512,\n",
    "        attention_dim = 512,\n",
    "        num_layers = 1,\n",
    "        encoder_learning_rate = 0.0001,\n",
    "        decoder_learning_rate = 0.0005,\n",
    "        num_epochs = 10,\n",
    "        grad_clip = 5.0,\n",
    "        alpha_weight = 1.0,\n",
    "        evaluate_step = 900, # 每隔多少步在验证集上测试一次\n",
    "        checkpoint = None, # 如果不为None，则利用该变量路径的模型继续训练\n",
    "        best_checkpoint = 'model/ARCTIC/best_ARCTIC.ckpt', # 验证集上表现最优的模型的路径\n",
    "        last_checkpoint = 'model/ARCTIC/last_ARCTIC.ckpt', # 训练完成时的模型的路径\n",
    "        beam_k = 5 #束搜索的束宽\n",
    "    )\n",
    "class ImageEncoder(nn.Module):\n",
    "    def __init__(self, finetuned=True):\n",
    "        super(ImageEncoder, self).__init__()\n",
    "        model = torchvision.models.resnet101(weights=ResNet101_Weights.DEFAULT)\n",
    "        # ResNet-101网格表示提取器\n",
    "        self.grid_rep_extractor = nn.Sequential(*(list(model.children())[:-2])) #去掉最后两层 \n",
    "        for param in self.grid_rep_extractor.parameters(): #冻结参数--不参与训练\n",
    "            param.requires_grad = finetuned #是否微调\n",
    "    def forward(self, images):\n",
    "        out = self.grid_rep_extractor(images) \n",
    "        return out\n",
    "class AdditiveAttention(nn.Module): #加性注意力\n",
    "    def  __init__(self, query_dim, key_dim, attn_dim):\n",
    "        \"\"\"\n",
    "            query_dim: 查询Q的维度\n",
    "            key_dim: 键K的维度\n",
    "            attn_dim: 注意力函数隐藏层表示的维度\n",
    "        \"\"\"\n",
    "        \n",
    "        super(AdditiveAttention, self).__init__()\n",
    "        self.attn_w_1_q = nn.Linear(query_dim, attn_dim) #Q的线性变换\n",
    "        self.attn_w_1_k = nn.Linear(key_dim, attn_dim) #K的线性变换\n",
    "        self.attn_w_2 = nn.Linear(attn_dim, 1) #注意力函数隐藏层到输出层的线性变换\n",
    "        self.tanh = nn.Tanh() #激活函数\n",
    "        self.softmax = nn.Softmax(dim=1) #归一化函数\n",
    "\n",
    "    def forward(self, query, key_value):\n",
    "        \"\"\"\n",
    "        Q K V：Q和K算出相关性得分，作为V的权重，K=V\n",
    "        参数：\n",
    "            query: 查询 (batch_size, q_dim)\n",
    "            key_value: 键和值，(batch_size, n_kv, kv_dim)\n",
    "        \"\"\"\n",
    "        # （2）计算query和key的相关性，实现注意力评分函数\n",
    "        # -> (batch_size, 1, attn_dim)\n",
    "        queries = self.attn_w_1_q(query).unsqueeze(1) \n",
    "        # -> (batch_size, n_kv, attn_dim)\n",
    "        keys = self.attn_w_1_k(key_value) #\n",
    "        # -> (batch_size, n_kv)\n",
    "        attn = self.attn_w_2(self.tanh(queries+keys)).squeeze(2)  #注意力评分函数\n",
    "        # （3）归一化相关性分数\n",
    "        # -> (batch_size, n_kv)\n",
    "        attn = self.softmax(attn)  #归一化\n",
    "        # （4）计算输出\n",
    "        # (batch_size x 1 x n_kv)(batch_size x n_kv x kv_dim)\n",
    "        # -> (batch_size, 1, kv_dim)\n",
    "        output = torch.bmm(attn.unsqueeze(1), key_value).squeeze(1)\n",
    "        return output, attn\n",
    "class AttentionDecoder(nn.Module):\n",
    "    def __init__(self, image_code_dim, vocab_size, word_dim, attention_dim, hidden_size, num_layers, dropout=0.5):\n",
    "        super(AttentionDecoder, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, word_dim) #词嵌入 \n",
    "        self.attention = AdditiveAttention(hidden_size, image_code_dim, attention_dim) #注意力机制\n",
    "        self.init_state = nn.Linear(image_code_dim, num_layers*hidden_size) #初始化隐状态\n",
    "        self.rnn = nn.GRU(word_dim + image_code_dim, hidden_size, num_layers) #GRU 门控循环\n",
    "        self.dropout = nn.Dropout(p=dropout) #dropout\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size) #全连接层\n",
    "        # RNN默认已初始化\n",
    "        self.init_weights() #初始化权重\n",
    "        \n",
    "    def init_weights(self): #初始化权重\n",
    "        self.embed.weight.data.uniform_(-0.1, 0.1) #词嵌入\n",
    "        self.fc.bias.data.fill_(0) #全连接层\n",
    "        self.fc.weight.data.uniform_(-0.1, 0.1) #全连接层\n",
    "    \n",
    "    def init_hidden_state(self, image_code, captions, cap_lens):\n",
    "        \"\"\"\n",
    "        参数：\n",
    "            image_code：图像编码器输出的图像表示 \n",
    "                        (batch_size, image_code_dim, grid_height, grid_width)\n",
    "        \"\"\"\n",
    "        # 将图像网格表示转换为序列表示形式 \n",
    "        batch_size, image_code_dim = image_code.size(0), image_code.size(1)\n",
    "        # -> (batch_size, grid_height, grid_width, image_code_dim) \n",
    "        image_code = image_code.permute(0, 2, 3, 1)  \n",
    "        # -> (batch_size, grid_height * grid_width, image_code_dim)\n",
    "        image_code = image_code.view(batch_size, -1, image_code_dim)\n",
    "        # （1）按照caption的长短排序\n",
    "        sorted_cap_lens, sorted_cap_indices = torch.sort(cap_lens, 0, True)\n",
    "        captions = captions[sorted_cap_indices]\n",
    "        image_code = image_code[sorted_cap_indices]\n",
    "         #（2）初始化隐状态\n",
    "        hidden_state = self.init_state(image_code.mean(axis=1))\n",
    "        hidden_state = hidden_state.view(\n",
    "                            batch_size, \n",
    "                            self.rnn.num_layers, \n",
    "                            self.rnn.hidden_size).permute(1, 0, 2)\n",
    "        return image_code, captions, sorted_cap_lens, sorted_cap_indices, hidden_state\n",
    "\n",
    "    def forward_step(self, image_code, curr_cap_embed, hidden_state):\n",
    "        #（3.2）利用注意力机制获得上下文向量\n",
    "        # query：hidden_state[-1]，即最后一个隐藏层输出 (batch_size, hidden_size)\n",
    "        # context: (batch_size, hidden_size)\n",
    "        context, alpha = self.attention(hidden_state[-1], image_code)\n",
    "        #（3.3）以上下文向量和当前时刻词表示为输入，获得GRU输出\n",
    "        x = torch.cat((context, curr_cap_embed), dim=-1).unsqueeze(0)\n",
    "        # x: (1, real_batch_size, hidden_size+word_dim)\n",
    "        # out: (1, real_batch_size, hidden_size)\n",
    "        out, hidden_state = self.rnn(x, hidden_state)\n",
    "        #（3.4）获取该时刻的预测结果\n",
    "        # (real_batch_size, vocab_size)\n",
    "        preds = self.fc(self.dropout(out.squeeze(0)))\n",
    "        return preds, alpha, hidden_state\n",
    "        \n",
    "    def forward(self, image_code, captions, cap_lens):\n",
    "        \"\"\"\n",
    "        参数：\n",
    "            hidden_state: (num_layers, batch_size, hidden_size)\n",
    "            image_code:  (batch_size, feature_channel, feature_size)\n",
    "            captions: (batch_size, )\n",
    "        \"\"\"\n",
    "        # （1）将图文数据按照文本的实际长度从长到短排序\n",
    "        # （2）获得GRU的初始隐状态\n",
    "        image_code, captions, sorted_cap_lens, sorted_cap_indices, hidden_state \\\n",
    "            = self.init_hidden_state(image_code, captions, cap_lens)\n",
    "        batch_size = image_code.size(0)\n",
    "        # 输入序列长度减1，因为最后一个时刻不需要预测下一个词\n",
    "        lengths = sorted_cap_lens.cpu().numpy() - 1\n",
    "        # 初始化变量：模型的预测结果和注意力分数\n",
    "        predictions = torch.zeros(batch_size, lengths[0], self.fc.out_features).to(captions.device)\n",
    "        alphas = torch.zeros(batch_size, lengths[0], image_code.shape[1]).to(captions.device)\n",
    "        # 获取文本嵌入表示 cap_embeds: (batch_size, num_steps, word_dim)\n",
    "        cap_embeds = self.embed(captions)\n",
    "        # Teacher-Forcing模式\n",
    "        for step in range(lengths[0]):\n",
    "            #（3）解码\n",
    "            #（3.1）模拟pack_padded_sequence函数的原理，获取该时刻的非<pad>输入\n",
    "            real_batch_size = np.where(lengths>step)[0].shape[0]\n",
    "            preds, alpha, hidden_state = self.forward_step(\n",
    "                            image_code[:real_batch_size], \n",
    "                            cap_embeds[:real_batch_size, step, :],\n",
    "                            hidden_state[:, :real_batch_size, :].contiguous())            \n",
    "            # 记录结果\n",
    "            predictions[:real_batch_size, step, :] = preds\n",
    "            alphas[:real_batch_size, step, :] = alpha\n",
    "        return predictions, alphas, captions, lengths, sorted_cap_indices\n",
    "    \n",
    "class ARCTIC(nn.Module): #模型主体部分\n",
    "    def __init__(self, image_code_dim, vocab, word_dim, attention_dim, hidden_size, num_layers):\n",
    "        super(ARCTIC, self).__init__()\n",
    "        self.vocab = vocab\n",
    "        self.encoder = ImageEncoder()\n",
    "        self.decoder = AttentionDecoder(image_code_dim, len(vocab),\n",
    "                                        word_dim, attention_dim, hidden_size, num_layers)\n",
    "        print(\"test\")\n",
    "    def forward(self, images, captions, cap_lens):\n",
    "        image_code = self.encoder(images)\n",
    "        return self.decoder(image_code, captions, cap_lens)\n",
    "    def generate_by_beamsearch(self, images, beam_k, max_len): # beam_k束搜索\n",
    "        vocab_size = len(self.vocab)\n",
    "        image_codes = self.encoder(images)\n",
    "        texts = []\n",
    "        device = images.device\n",
    "        # 对每个图像样本执行束搜索\n",
    "        for image_code in image_codes:\n",
    "            # 将图像表示复制k份\n",
    "            image_code = image_code.unsqueeze(0).repeat(beam_k,1,1,1)\n",
    "            # 生成k个候选句子，初始时，仅包含开始符号<start>\n",
    "            cur_sents = torch.full((beam_k, 1), self.vocab['<start>'], dtype=torch.long).to(device)\n",
    "            cur_sent_embed = self.decoder.embed(cur_sents)[:,0,:]\n",
    "            sent_lens = torch.LongTensor([1]*beam_k).to(device)\n",
    "            # 获得GRU的初始隐状态\n",
    "            image_code, cur_sent_embed, _, _, hidden_state = \\\n",
    "                self.decoder.init_hidden_state(image_code, cur_sent_embed, sent_lens)\n",
    "            # 存储已生成完整的句子（以句子结束符<end>结尾的句子）\n",
    "            end_sents = []\n",
    "            # 存储已生成完整的句子的概率\n",
    "            end_probs = []\n",
    "            # 存储未完整生成的句子的概率\n",
    "            probs = torch.zeros(beam_k, 1).to(device)\n",
    "            k = beam_k\n",
    "            while True:\n",
    "                preds, _, hidden_state = self.decoder.forward_step(image_code[:k], cur_sent_embed, hidden_state.contiguous())\n",
    "                # -> (k, vocab_size)\n",
    "                preds = nn.functional.log_softmax(preds, dim=1)\n",
    "                # 对每个候选句子采样概率值最大的前k个单词生成k个新的候选句子，并计算概率\n",
    "                # -> (k, vocab_size)\n",
    "                probs = probs.repeat(1,preds.size(1)) + preds\n",
    "                if cur_sents.size(1) == 1:\n",
    "                    # 第一步时，所有句子都只包含开始标识符，因此，仅利用其中一个句子计算topk\n",
    "                    values, indices = probs[0].topk(k, 0, True, True)\n",
    "                else:\n",
    "                    # probs: (k, vocab_size) 是二维张量\n",
    "                    # topk函数直接应用于二维张量会按照指定维度取最大值，这里需要在全局取最大值\n",
    "                    # 因此，将probs转换为一维张量，再使用topk函数获取最大的k个值\n",
    "                    values, indices = probs.view(-1).topk(k, 0, True, True)\n",
    "                # 计算最大的k个值对应的句子索引和词索引\n",
    "                sent_indices = torch.div(indices, vocab_size, rounding_mode='trunc') \n",
    "                word_indices = indices % vocab_size \n",
    "                # 将词拼接在前一轮的句子后，获得此轮的句子\n",
    "                cur_sents = torch.cat([cur_sents[sent_indices], word_indices.unsqueeze(1)], dim=1)\n",
    "                # 查找此轮生成句子结束符<end>的句子\n",
    "                end_indices = [idx for idx, word in enumerate(word_indices) if word == self.vocab['<end>']]\n",
    "                if len(end_indices) > 0:\n",
    "                    end_probs.extend(values[end_indices])\n",
    "                    end_sents.extend(cur_sents[end_indices].tolist())\n",
    "                    # 如果所有的句子都包含结束符，则停止生成\n",
    "                    k -= len(end_indices)\n",
    "                    if k == 0:\n",
    "                        break\n",
    "                # 查找还需要继续生成词的句子\n",
    "                cur_indices = [idx for idx, word in enumerate(word_indices) \n",
    "                               if word != self.vocab['<end>']]\n",
    "                if len(cur_indices) > 0:\n",
    "                    cur_sent_indices = sent_indices[cur_indices]\n",
    "                    cur_word_indices = word_indices[cur_indices]\n",
    "                    # 仅保留还需要继续生成的句子、句子概率、隐状态、词嵌入\n",
    "                    cur_sents = cur_sents[cur_indices]\n",
    "                    probs = values[cur_indices].view(-1,1)\n",
    "                    hidden_state = hidden_state[:,cur_sent_indices,:]\n",
    "                    cur_sent_embed = self.decoder.embed(\n",
    "                        cur_word_indices.view(-1,1))[:,0,:]\n",
    "                # 句子太长，停止生成\n",
    "                if cur_sents.size(1) >= max_len:\n",
    "                    break\n",
    "            if len(end_sents) == 0:\n",
    "                # 如果没有包含结束符的句子，则选取第一个句子作为生成句子\n",
    "                gen_sent = cur_sents[0].tolist()\n",
    "            else: \n",
    "                # 否则选取包含结束符的句子中概率最大的句子\n",
    "                gen_sent = end_sents[end_probs.index(max(end_probs))]\n",
    "            texts.append(gen_sent)\n",
    "        return texts\n",
    "mode_arctic=\"../best_arctic.ckpt\"\n",
    "model=torch.load(mode_arctic)[\"model\"] #加载模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 视觉Transformer (ViT) + Transformer解码器\n",
    "\n",
    "### 网格/区域表示、Transformer编码器+Transformer解码器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### blip + 多模态构建 新数据集\n",
    "\n",
    "为了得到带有背景描述的数据集，我们利用训练的服饰图像描述模型和多模态大语言模型，为真实背景的服饰图像数据集增加服饰描述和背景描述，构建全新的服饰图像描述数据集。\n",
    "我的使用的新数据集是选用 DeepFasion开源的12w数据集，仅使用图片，选用其中背景较丰富的5000张图像区间\n",
    "\n",
    "下面我封装了一个模块，用以快速调用Blip 进行批量处理\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class blip_model():\n",
    "    def __init__(self) -> None:\n",
    "        self.processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "        self.model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\").to(\"cuda\")\n",
    "    def gen_res(self,img_path):\n",
    "        raw_image = Image.open(img_path).convert('RGB')\n",
    "\n",
    "        text = \"a people in front of \"\n",
    "        input_1 = self.processor(raw_image, text, return_tensors=\"pt\").to(\"cuda\")\n",
    "        out_1 = self.model.generate(**input_1,max_length=100)\n",
    "        res_1=self.processor.decode(out_1[0], skip_special_tokens=True)\n",
    "\n",
    "        input_2 = self.processor(raw_image, return_tensors=\"pt\").to(\"cuda\")\n",
    "        out_2 = self.model.generate(**input_2,max_length=100)\n",
    "        res_2=self.processor.decode(out_2[0], skip_special_tokens=True)\n",
    "        return res_1+\". \"+res_2\n",
    "def gen_json(img_path,n):\n",
    "    model=blip_model()\n",
    "    #img_path=\"D:/NNDL/data/deepfashion-multimodal/images\"\n",
    "    #获取该目录下所有文件，存入列表中\n",
    "    imgs=os.listdir(img_path)\n",
    "    res={}\n",
    "    start=31000\n",
    "\n",
    "    for img in range(start,len(imgs)):\n",
    "        img_k=imgs[img]\n",
    "        img_path_=img_path+\"/\"+img_k\n",
    "        res[img_k]=model.gen_res(img_path_)\n",
    "        if len(res)>=n:\n",
    "            break\n",
    "    #保存为json文件\n",
    "    with open('res.json', 'w') as f:\n",
    "        json.dump(res, f,indent=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 实验结果\n",
    "由于除了BLEU-4以外，其他指标的最大值其实和数据有关，所以为此我们先进行一个实验：得到如下指标的最大结果，然后将其作为一种相对的标准，结果如下：\n",
    "BLEU-MAX:1.0|CIDEr-D-MAX:0.0043918896512309125|SPICE-MAX:0.18703616844830037，在评估过程中将会同时输出实际值和相对值（被压缩到0-1之间），"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-MAX:1.0|CIDEr-D-MAX:0.0043918896512309125|SPICE-MAX:0.18703616844830037\n"
     ]
    }
   ],
   "source": [
    "max_cider=0.0043918896512309125\n",
    "max_spice=0.18703616844830037\n",
    "def evaluate_(data_loader) :#用来测试剩下两个指标的最大值\n",
    "    cands = []# 存储参考文本\n",
    "    refs = []# 需要过滤的词\n",
    "    filterd_words = set({model.vocab['<start>'], model.vocab['<end>'], model.vocab['<pad>']})\n",
    "    for i, (imgs, caps, caplens) in enumerate(data_loader):\n",
    "        cands.extend([filter_useless_words(cap, filterd_words) for cap in caps.tolist()])# 参考文本\n",
    "        refs.extend([filter_useless_words(cap, filterd_words) for cap in caps.tolist()]) #候选文本\n",
    "    bleu4_score = get_BLEU_score(cands, refs)\n",
    "    cider_d_score = get_CIDER_D_score(cands, refs)\n",
    "    spice_score= get_SPICE_score(cands, refs)\n",
    "    print(f\"BLEU-MAX:{bleu4_score}|CIDEr-D-MAX:{cider_d_score}|SPICE-MAX:{spice_score}\")\n",
    "    max_cider=cider_d_score\n",
    "    max_spice=spice_score\n",
    "evaluate_(test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里以 ARCTIC 的测评代码为例子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@@@实际值 BLEU:0.30466660274770024|CIDEr-D:0.0017447527516659645|SPICE:0.1336081641272871\n",
      "@@@相对值（0-1） BLEU:0.30466660274770024|CIDEr-D:0.3972669830575009|SPICE:0.7143439968629298\n"
     ]
    }
   ],
   "source": [
    "def filter_useless_words(sent, filterd_words):\n",
    "    # 去除句子中不参与BLEU值计算的符号\n",
    "    return [w for w in sent if w not in filterd_words]\n",
    "cider_d_score=0\n",
    "spice_score=0\n",
    "def evaluate(data_loader, model, config):\n",
    "    model.eval()\n",
    "    # 存储候选文本\n",
    "    cands = []\n",
    "    # 存储参考文本\n",
    "    refs = []\n",
    "    # 需要过滤的词\n",
    "    filterd_words = set({model.vocab['<start>'], model.vocab['<end>'], model.vocab['<pad>']})\n",
    "    cpi = config.captions_per_image\n",
    "    device = next(model.parameters()).device\n",
    "    for i, (imgs, caps, caplens) in enumerate(data_loader):\n",
    "        with torch.no_grad():\n",
    "            # 通过束搜索，生成候选文本\n",
    "            texts = model.generate_by_beamsearch(imgs.to(device), config.beam_k, config.max_len+2)\n",
    "            # 候选文本\n",
    "            cands.extend([filter_useless_words(text, filterd_words) for text in texts])\n",
    "            # 参考文本\n",
    "            refs.extend([filter_useless_words(cap, filterd_words) for cap in caps.tolist()])\n",
    "    \n",
    "    \n",
    "    bleu4_score = get_BLEU_score(cands, refs)\n",
    "    cider_d_score = get_CIDER_D_score(cands, refs)\n",
    "    spice_score= get_SPICE_score(cands, refs)\n",
    "    print(f\"@@@实际值 BLEU:{bleu4_score}|CIDEr-D:{cider_d_score}|SPICE:{spice_score}\")\n",
    "    print(f\"@@@相对值（0-1） BLEU:{bleu4_score}|CIDEr-D:{cider_d_score/max_cider}|SPICE:{spice_score/max_spice}\")\n",
    "    #model.train()\n",
    "evaluate(test_loader, model, ARCTIC_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同时我们以测试集的第一行数据来进行演示：以生成的描述文本和参考文本进行比较，直观评估ARCTIC的性能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@生成数据： [[154, 21, 47, 16, 31, 32, 39, 39, 52, 28, 10, 11, 12, 1, 39, 52, 16, 28, 7, 8, 9, 72, 13, 16, 84, 1, 49, 50, 28, 7, 8, 9, 10, 11, 12, 57, 16, 25, 59, 34, 35, 60, 155], [154, 21, 47, 16, 31, 32, 39, 39, 52, 28, 43, 12, 1, 39, 52, 16, 28, 7, 8, 9, 72, 13, 16, 84, 1, 49, 50, 28, 7, 8, 9, 43, 12, 57, 16, 25, 59, 34, 35, 60, 155], [154, 38, 40, 4, 5, 6, 7, 8, 9, 43, 12, 1, 13, 14, 15, 16, 81, 1, 18, 3, 16, 14, 5, 19, 1, 8, 16, 7, 9, 15, 4, 43, 12, 1, 26, 3, 16, 28, 7, 8, 9, 43, 12, 1, 26, 3, 16, 28, 7, 8, 9, 43, 12, 57, 16, 25, 59, 34, 35, 60, 155]]\n",
      "@生成的文本： This person is wearing a tank tank top with solid color patterns. The tank top is with cotton fabric and its neckline is round. The pants are with cotton fabric and solid color patterns. There is an accessory on her wrist.\n",
      "@实际的文本： This woman is wearing a tank tank shirt with graphic patterns and a three-point shorts. The tank shirt is with cotton fabric and its neckline is crew. The shorts are with cotton fabric and graphic patterns.\n"
     ]
    }
   ],
   "source": [
    "def batch_eva(data_loader, model, config): #这里使用实验数据的第一个batch来进行演示\n",
    "    model.eval()\n",
    "    for i, (imgs, caps, caplens) in  enumerate(test_loader):\n",
    "        cands = [] # 存储候选文本 \n",
    "        refs = [] # 存储参考文本\n",
    "        filterd_words = set({model.vocab['<start>'], model.vocab['<end>'], model.vocab['<pad>']}) #过滤词\n",
    "        cpi = config.captions_per_image\n",
    "        texts = model.generate_by_beamsearch(imgs.to(\"cuda\"), config.beam_k, config.max_len+2)\n",
    "        print(\"@生成数据：\",texts)\n",
    "        print(\"@生成的文本：\",wvec_to_cap(model.vocab,texts[0])) #抽出一个batch的第一个文本\n",
    "        print(\"@实际的文本：\",wvec_to_cap(model.vocab,caps[0].tolist()))\n",
    "        break\n",
    "batch_eva(test_loader, model, ARCTIC_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "之后的两个模型的forward形式同上， 我们整理了最大值的统计情况,如下表格\n",
    "\n",
    "\n",
    "| 模型名称 | BLEU-4 | CIDEr-D | SPICE |\n",
    "|---------|------|------|------|\n",
    "| ARCTIC   |  0.30466660274770024    |   (相对值：0.39726) 0.0017447527516659645  |   (相对值：0.71434)0.1336081641272871   |\n",
    "| VIT   |    **0.3074786561430062**  |   (相对值：0.93790) 0.004119164250591897   |   (相对值：0.70628)0.1321016861247424   |\n",
    "| SwinTrans   |   0.25770958160979623   |    (相对值：0.91071) 0.003999756354414652  |  (相对值：0.60681)0.11349623153205401    |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 实验结果分析\n",
    "\n",
    "根据实验结果，分析模型的优缺点\n",
    "- BLEU-4 分析下\n",
    "  - ARCTIC 模型 性能评价:\n",
    "    ARCTIC 模型在 BLEU-4 上表现良好，得分为（0.30466660274770024）。\n",
    "    语法和词汇的准确性得到了有效提升,而且经过实验贪心算法下的得分要低于束搜索算法下的得分，所以束搜索算法在生成文本时，能够生成更流畅的文本。\n",
    "  - 视觉Transformer (ViT) + Transformer解码器 性能评价:\n",
    "      ViT + Transformer 模型在 BLEU-4 上的得分为（0.3074786561430062）。模型在服饰描述任务中取得了良好的结果，生成文本在语和词汇方面表现出色。**在这个指标下 其语义流畅度最好**\n",
    "  - 网格/区域表示、Transformer编码器+Transformer解码器 性能评价:  \n",
    "      网格/区域表示 + Transformer 模型在 BLEU-4 上的得分为（0.25770958160979623）。\n",
    "      模型在服饰描述任务中呈现出略低的性能，表现缺乏一定语句流畅度\n",
    "- CIDEr-D 分析下\n",
    "  - ARCTIC 模型 性能评价:\n",
    "    ARCTIC 模型在 CIDEr-D 上取得了(相对值：0.39726) 0.0017447527516659645 \n",
    "    CIDEr-D 分数显示模型在文本多样性和丰富性方面有一定的成功，但是不如其他模型。\n",
    "  - 视觉Transformer (ViT) + Transformer解码器 性能评价:\n",
    "    ViT + Transformer 模型在 CIDEr-D 上的得分为 (相对值：0.93790) 0.004119164250591897。\n",
    "    模型在服饰描述任务中具有较高的文本多样性和丰富性，**是表现最好的模型**\n",
    "  - 网格/区域表示、Transformer编码器+Transformer解码器 性能评价:\n",
    "    网格/区域表示 + Transformer 模型在 CIDEr-D 上的得分为(相对值：0.91071) 0.003999756354414652。\n",
    "    模型对服饰描述的多样性和丰富性取得了一定的成功。可以看出虽然流畅度不如其他模型，但是多样性还是不错的。\n",
    "- SPICE 分析下\n",
    "  - ARCTIC 模型 性能评价:\n",
    "    ARCTIC 模型在 SPICE 上取得了 (相对值：0.71434)0.1336081641272871 。\n",
    "    SPICE 分数反映了模型生成文本与图像内容相关性的程度，在语义层面上有一个很好的效果\n",
    "  - 视觉Transformer (ViT) + Transformer解码器 性能评价:\n",
    "    ViT + Transformer 模型在 SPICE 上的得分为 (相对值：0.70628)0.1321016861247424 。\n",
    "    模型在服饰描述任务中表现出色，成功捕捉图像语义信息。\n",
    "  - 网格/区域表示、Transformer编码器+Transformer解码器 性能评价:\n",
    "    网格/区域表示 + Transformer 模型在 SPICE 上的得分为 (相对值：0.60681)0.11349623153205401。\n",
    "    模型在 SPICE 上的表现显示其在描述图像内容方面的良好性能，但是效果相对较低\n",
    "\n",
    "当然 ARCTIC、视觉Transformer (ViT) + Transformer解码器和网格/区域表示、Transformer编码器+Transformer解码器这三个模型有不同的优劣势\n",
    "\n",
    "- ARCTIC 模型：\n",
    "  - 优势：\n",
    "    结合了注意力机制和编解码模型，有助于捕捉输入图像和生成描述之间的语义关系。\n",
    "    注意力机制使得模型在生成描述时能够更加关注与服饰相关的区域，提高了描述的准确性。\n",
    "  - 劣势：\n",
    "    可能需要更多的计算资源和训练时间，因为结合了多个模型组件。\n",
    "    在处理大规模数据集时，训练和推理速度可能较慢。\n",
    "    推理过程中尝试了不同的生成方式，发现加入 beam search 策略效果最佳。\n",
    "\n",
    "\n",
    "- 视觉 Transformer (ViT) + Transformer 解码器：\n",
    "  - 优势：\n",
    "    ViT 模型将输入图像转换为序列数据，直接应用 Transformer 解码器进行描述生成。\n",
    "    Transformer 解码器在自然语言处理任务中表现出色，生成准确且流畅的描述。\n",
    "  - 劣势：\n",
    "    ViT 模型可能对输入图像的分辨率和细节要求较高，对于复杂的服饰图像可能需要更多的训练数据和计算资源。\n",
    "    在处理长序列数据时，Transformer 解码器可能面临较长的训练和推理时间。\n",
    "\n",
    "\n",
    "- 网格/区域表示、Transformer 编码器+Transformer 解码器：\n",
    "  - 优势：\n",
    "    网格/区域表示将图像划分为网格或区域，有助于捕捉局部特征。\n",
    "    Transformer 编码器和解码器在处理序列数据时具有较强的建模能力，能够生成准确的描述。\n",
    "  - 劣势：\n",
    "    网格/区域表示可能需要额外的预处理步骤来划分图像，并可能导致信息损失。\n",
    "    Transformer 编码器和解码器的训练和推理时间可能较长，特别是在处理大规模数据集时\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 存在的问题 \n",
    "\n",
    "- 模型训练部分\n",
    "  - **资源不足**：不同模型对计算资源的需求不同，,作为学生的算力水平非常有限, 要训练出以该模型架构下的最优模型是一件困难的问题\n",
    "  - **数据集限制**：以训练数据构建的词典其实是一个非常小的词典，其实限制了模型的上限，离开数据集，在真实数据下的推理可能还是会有不完备的地方\n",
    "  - **图像裁剪**：数据集进行批量处理时的正方形裁剪可能会导致原始数据丢失部分信息，尤其是头部和脚步，还有白边导致的背景错误。\n",
    " \n",
    "- 数据生成部分\n",
    "  - **Blip性能不足**：我们使用的较小的模型，性能不足以提取所有背景。\n",
    "    比如对于一个图像，经过blip生成的文本是：a people in front of a brick wall. a girl wearing a denim dress and white **shoes** ；但是原图没有鞋子的信息。在没有鞋子信息的情况下还是生成了鞋子信息\n",
    "  - **LLM性能不足**：文心一言相比ChatGPT，理解能力还是较弱\n",
    "    即使在我反复强调的情况下，LLM还是违背我对文本内容和长度的限制，生成如下长篇大论：The scene takes place in a room with a green mat on the floor. The background is a wall, which is painted in a neutral color and has no decorations or features. The room is dimly lit, with only a small amount of natural light coming from the window. There is a door leading out of the room, but it is closed. The man standing on the green mat is wearing a blue shorts and a white shirt, and he is facing the wall. He appears to be in a meditative or contemplative state, as he is standing still and not interacting with anything else in the room.\n",
    "    \n",
    "    但是，实际上我们需要的是相对较短的描述\n",
    "   - 真实多模态能力的缺失：由于现在的结构是先img2txt再txt2txt，对于LLM来说会损失很多图像信息，真正的多模态还是需要直接进行img、txt2txt，下面是一个badcase：img2txt模型将玻璃识别成了镜子，导致LLM出错。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 总结\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
